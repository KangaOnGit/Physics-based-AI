{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11725a5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T14:32:47.138167Z",
     "iopub.status.busy": "2025-06-03T14:32:47.137595Z",
     "iopub.status.idle": "2025-06-03T14:32:49.246425Z",
     "shell.execute_reply": "2025-06-03T14:32:49.245845Z"
    },
    "papermill": {
     "duration": 2.116044,
     "end_time": "2025-06-03T14:32:49.247726",
     "exception": false,
     "start_time": "2025-06-03T14:32:47.131682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import kaggle_evaluation.aimo_2_inference_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "421c95bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T14:32:49.256878Z",
     "iopub.status.busy": "2025-06-03T14:32:49.256589Z",
     "iopub.status.idle": "2025-06-03T14:36:14.008425Z",
     "shell.execute_reply": "2025-06-03T14:36:14.007692Z"
    },
    "papermill": {
     "duration": 204.757661,
     "end_time": "2025-06-03T14:36:14.009751",
     "exception": false,
     "start_time": "2025-06-03T14:32:49.252090",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.5)\r\n",
      "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from albumentations) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations) (1.15.2)\r\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations) (6.0.2)\r\n",
      "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.11.4)\r\n",
      "Requirement already satisfied: albucore==0.0.23 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.0.23)\r\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations) (4.11.0.86)\r\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations) (3.12.3)\r\n",
      "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations) (6.2.1)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->albumentations) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->albumentations) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->albumentations) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->albumentations) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->albumentations) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->albumentations) (2.4.1)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\r\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (4.13.2)\r\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24.4->albumentations) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24.4->albumentations) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.24.4->albumentations) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.24.4->albumentations) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.24.4->albumentations) (2024.2.0)\r\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fc920b55290>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/segmentation-models-pytorch/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fc920b3da10>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/segmentation-models-pytorch/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fc920da58d0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/segmentation-models-pytorch/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fc920b68d50>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/segmentation-models-pytorch/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fc920c47250>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/segmentation-models-pytorch/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement segmentation-models-pytorch (from versions: none)\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for segmentation-models-pytorch\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install albumentations\n",
    "!pip install segmentation-models-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b072ccd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T14:36:14.020068Z",
     "iopub.status.busy": "2025-06-03T14:36:14.019554Z",
     "iopub.status.idle": "2025-06-03T14:36:59.742734Z",
     "shell.execute_reply": "2025-06-03T14:36:59.742111Z"
    },
    "papermill": {
     "duration": 45.733652,
     "end_time": "2025-06-03T14:36:59.748040",
     "exception": false,
     "start_time": "2025-06-03T14:36:14.014388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/albumentations/check_version.py:107: UserWarning: Error fetching version info <urlopen error [Errno -3] Temporary failure in name resolution>\n",
      "  data = fetch_version_info()\n"
     ]
    }
   ],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import logging\n",
    "import warnings\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Deep learning (PyTorch)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.parametrizations import weight_norm\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# Computer vision\n",
    "import cv2\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Miscellaneous utilities\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "import einops\n",
    "from einops import rearrange\n",
    "from sklearn.model_selection import KFold\n",
    "from pathlib import Path\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e12f0a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T14:36:59.758136Z",
     "iopub.status.busy": "2025-06-03T14:36:59.757458Z",
     "iopub.status.idle": "2025-06-03T14:36:59.761411Z",
     "shell.execute_reply": "2025-06-03T14:36:59.760605Z"
    },
    "papermill": {
     "duration": 0.009753,
     "end_time": "2025-06-03T14:36:59.762338",
     "exception": false,
     "start_time": "2025-06-03T14:36:59.752585",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE = torch.device('cpu')\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 0.001\n",
    "IMAGE_SIZE = 224\n",
    "NUM_CLASSES = 3\n",
    "SEED = 42\n",
    "N_FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3fad321",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T14:36:59.771903Z",
     "iopub.status.busy": "2025-06-03T14:36:59.771432Z",
     "iopub.status.idle": "2025-06-03T14:36:59.774365Z",
     "shell.execute_reply": "2025-06-03T14:36:59.773845Z"
    },
    "papermill": {
     "duration": 0.008662,
     "end_time": "2025-06-03T14:36:59.775267",
     "exception": false,
     "start_time": "2025-06-03T14:36:59.766605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21076358",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T14:36:59.784686Z",
     "iopub.status.busy": "2025-06-03T14:36:59.784243Z",
     "iopub.status.idle": "2025-06-03T14:36:59.920547Z",
     "shell.execute_reply": "2025-06-03T14:36:59.919974Z"
    },
    "papermill": {
     "duration": 0.142022,
     "end_time": "2025-06-03T14:36:59.921591",
     "exception": false,
     "start_time": "2025-06-03T14:36:59.779569",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /kaggle/input/data-1\n"
     ]
    }
   ],
   "source": [
    "path = kagglehub.dataset_download(\"khanhpt1999/data-1\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fbe69ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T14:36:59.931590Z",
     "iopub.status.busy": "2025-06-03T14:36:59.931384Z",
     "iopub.status.idle": "2025-06-03T14:36:59.943462Z",
     "shell.execute_reply": "2025-06-03T14:36:59.942936Z"
    },
    "papermill": {
     "duration": 0.018184,
     "end_time": "2025-06-03T14:36:59.944388",
     "exception": false,
     "start_time": "2025-06-03T14:36:59.926204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dropout=0.1, img_size=64):  # Added img_size parameter\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.img_size = img_size  # Store image size\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3, bias=False)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # FIXED: Relative position embeddings for actual image size\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * img_size - 1) * (2 * img_size - 1), num_heads)\n",
    "        )\n",
    "        \n",
    "        # FIXED: Initialize relative position bias for actual image size\n",
    "        coords_h = torch.arange(img_size)\n",
    "        coords_w = torch.arange(img_size)\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing='ij'))\n",
    "        coords_flatten = torch.flatten(coords, 1)\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "        relative_coords[:, :, 0] += img_size - 1\n",
    "        relative_coords[:, :, 1] += img_size - 1\n",
    "        relative_coords[:, :, 0] *= 2 * img_size - 1\n",
    "        relative_position_index = relative_coords.sum(-1)\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        N = H * W\n",
    "        \n",
    "        # Reshape to sequence format [B, N, C]\n",
    "        x = x.reshape(B, C, N).transpose(1, 2)\n",
    "        \n",
    "        # Generate Q, K, V\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        \n",
    "        # Add relative position bias (only if image size matches)\n",
    "        if H == self.img_size and W == self.img_size:\n",
    "            relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].reshape(\n",
    "                H * W, H * W, -1)\n",
    "            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n",
    "            attn = attn + relative_position_bias.unsqueeze(0)\n",
    "        \n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, self.embed_dim)\n",
    "        x = self.proj(x)\n",
    "        \n",
    "        # Reshape back to spatial format [B, C, H, W]\n",
    "        x = x.transpose(1, 2).view(B, C, H, W)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class SelfAttentionConv(nn.Module):\n",
    "    \"\"\"Self-attention layer that can replace convolutions\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # If channels change, use 1x1 conv for projection\n",
    "        if in_channels != out_channels:\n",
    "            self.channel_proj = nn.Conv2d(in_channels, out_channels, 1, 1, 0)\n",
    "        else:\n",
    "            self.channel_proj = nn.Identity()\n",
    "            \n",
    "        # Self-attention components\n",
    "        self.attention = MultiHeadSelfAttention(out_channels, num_heads)\n",
    "        \n",
    "        # Learnable scale parameter for gradual integration\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Project channels if needed\n",
    "        identity = self.channel_proj(x)\n",
    "        \n",
    "        # Apply self-attention\n",
    "        attended = self.attention(identity)\n",
    "        \n",
    "        # Gradual integration with learnable scale\n",
    "        out = self.gamma * attended + identity\n",
    "        \n",
    "        # Handle stride if needed\n",
    "        if self.stride > 1:\n",
    "            out = F.avg_pool2d(out, kernel_size=self.stride, stride=self.stride)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3930434f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T14:36:59.954441Z",
     "iopub.status.busy": "2025-06-03T14:36:59.954245Z",
     "iopub.status.idle": "2025-06-03T14:37:00.002565Z",
     "shell.execute_reply": "2025-06-03T14:37:00.002018Z"
    },
    "papermill": {
     "duration": 0.054852,
     "end_time": "2025-06-03T14:37:00.003616",
     "exception": false,
     "start_time": "2025-06-03T14:36:59.948764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ElectricForceModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        imsize = 224,\n",
    "        v0 = 0,\n",
    "        layers = 3,\n",
    "        loops = 8,\n",
    "        debug = False,\n",
    "        beta = 0.8,\n",
    "        use_self_attention = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.register_buffer('v0', torch.tensor([v0], dtype=torch.float32))\n",
    "        self.register_buffer('beta', torch.tensor([beta], dtype=torch.float32))\n",
    "\n",
    "        self.adaptive_epsilon = nn.Parameter(torch.tensor([1.0]))\n",
    "        self.adaptive_mass = nn.Parameter(torch.tensor([1.0]))\n",
    "        self.adaptive_time = nn.Parameter(torch.tensor([0.1]))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.adaptive_epsilon.data.fill_(1.0)\n",
    "            self.adaptive_mass.data.fill_(1.0)\n",
    "            self.adaptive_time.data.fill_(0.1)\n",
    "\n",
    "        self.imsize = imsize\n",
    "        self.layers = layers\n",
    "        self.loops = loops\n",
    "        self.debug = debug\n",
    "        self.use_self_attention = use_self_attention\n",
    "\n",
    "        if self.use_self_attention:\n",
    "            self.input_attention = MultiHeadSelfAttention(3, num_heads=3, img_size=imsize)\n",
    "            self.attention_scale = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "        if self.use_self_attention:\n",
    "            self.mapping_to_vector_space = SelfAttentionConv(3, 2, num_heads=2)\n",
    "            self.mapping_to_ipt = SelfAttentionConv(2, 3, num_heads=3)\n",
    "        else:\n",
    "            self.mapping_to_vector_space = nn.Conv2d(3, 2, 1, 1, 0)\n",
    "            self.mapping_to_ipt = nn.Conv2d(2, 3, 1, 1, 0)\n",
    "            \n",
    "        self.space_norm = nn.Tanh()\n",
    "        \n",
    "\n",
    "    def compute_distance(self, ipt, chunk_size=512):\n",
    "        \n",
    "        \"\"\"\n",
    "        Compute the distance of each point with respect to every point in zeta\n",
    "\n",
    "        Input:\n",
    "            - ipt: Input Image | [B, 3, self.imsize, self.imsize]\n",
    "\n",
    "        Outpt:\n",
    "            - zeta: The coordinates map for each pixel in image (x, y) | [B, 2, self.imsize, self.imsize]\n",
    "            - x: x-coordinates of every point in zeta | [B, N]\n",
    "            - y: y-coordinates of every point in zeta | [B, N]\n",
    "            - inverse_sq_dist: 1/r^2, the inverse squared distance of each point with respect to every point in zeta | [B, N, N]\n",
    "            - inverse_dist: 1/r, the inverse distance of each point with respect to every point in zeta | [B, N, N]\n",
    "            \n",
    "        How the Function Works:\n",
    "            - Find zeta using a point-wise-convolutional-layer, the number of channel output will be the number of dimensions in vector space\n",
    "                            (2 -> (x, y) == Vector Space\n",
    "                             3 -> (x, y, z) == 3D Space\n",
    "                             ...\n",
    "                             ...\n",
    "                            )\n",
    "            - Find x and y by slicing zeta along the 0th and 1st of the 1st Dimension\n",
    "            - Calculate the Euclidean Distance between each point in zeta\n",
    "            - inverse_sq_dist is D**(-2) and inverse_dist is D**(-1)\n",
    "        \"\"\"\n",
    "                \n",
    "        zeta = self.mapping_to_vector_space(ipt)\n",
    "        zeta = self.space_norm(zeta) # [B, 2, self.imsize, self.imsize]\n",
    "        if self.debug:\n",
    "            print(f\"zeta Shape: {zeta.shape}\")\n",
    "\n",
    "        B, C, H, W = zeta.shape\n",
    "        N = H*W\n",
    "\n",
    "        pts = zeta.reshape(B, 2, N) # [B, 2, N]\n",
    "        if self.debug:\n",
    "            print(f\"pts Shape: {pts.shape}\")\n",
    "\n",
    "        x, y = pts[:, 0], pts[:, 1] # [B, N], [B, N]\n",
    "        if self.debug:\n",
    "            print(f\"x Shape: {x.shape}\")\n",
    "            print(f\"y Shape: {y.shape}\")\n",
    "\n",
    "        zflat = pts.permute(0, 2, 1) # [B, N, 2]\n",
    "        if self.debug:\n",
    "            print(f\"zflat Shape: {zflat.shape}\")\n",
    "\n",
    "        if N <= chunk_size:\n",
    "            D = torch.cdist(zflat, zflat, p=2) # [B, N, N]\n",
    "            D = torch.where(D==0, float('inf'), D) # [B, N, N]\n",
    "        else:\n",
    "            if self.debug:\n",
    "                print(f\"Using chunking with chunk_size={chunk_size} for N={N}\")\n",
    "        \n",
    "            D = torch.zeros(B, N, N, device=zflat.device, dtype=zflat.dtype) # [B, N, N]\n",
    "            \n",
    "            for i in range(0, N, chunk_size):\n",
    "                end_i = min(i + chunk_size, N)\n",
    "                chunk_points = zflat[:, i:end_i, :] # [B, chunk_size, 2]\n",
    "                distances_chunk = torch.cdist(chunk_points, zflat, p=2) # [B, chunk_size, N]\n",
    "\n",
    "                diag_mask = torch.zeros_like(distances_chunk, dtype=torch.bool)\n",
    "                for idx in range(end_i - i):\n",
    "                    diag_mask[:, idx, i + idx] = True\n",
    "                    \n",
    "                distances_chunk = torch.where(diag_mask, float('inf'), distances_chunk)\n",
    "                D[:, i:end_i, :] = distances_chunk # [B, chunk_size, N]\n",
    "\n",
    "        D = torch.where(D == 0, float('inf'), D)\n",
    "        if self.debug:\n",
    "            print(f\"D Shape: {D.shape}\") # [B, N, N]\n",
    "            \n",
    "        #inverse_sq_dist = D.pow(-2) # [B, N, N]\n",
    "        #inverse_dist = D.pow(-1) # [B, N, N]\n",
    "        #if self.debug:\n",
    "            #print(f\"inverse_sq_dist Shape: {inverse_sq_dist.shape}\") # [B, N, N]\n",
    "            #print(f\"inverse_dist Shape: {inverse_dist.shape}\") # [B, N, N]\n",
    "            \n",
    "        if self.debug:\n",
    "            print(\"-\"*59)\n",
    "        \n",
    "        return zeta, x, y, D #inverse_sq_dist, #inverse_dist\n",
    "\n",
    "    def compute_angle(self, x, y, chunk_size=512):\n",
    "        \n",
    "        \"\"\"\n",
    "        Compute angle of forces acted on each point with respect to the x-axis\n",
    "        Input: \n",
    "            - x: x-coordinates of every point in zeta | [B, N]\n",
    "            - y: y-coordinates of every point in zeta | [B, N]\n",
    "\n",
    "        Output:\n",
    "            - theta_rad: Radian of angle of forces with respect to the x-axis | [B, N, N]\n",
    "\n",
    "        How the Function Works:\n",
    "            - Applying Inverse Function of tan to find the angle | tan = dy/dx\n",
    "        \"\"\"\n",
    "        \n",
    "        B, N = x.shape\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Running compute_angle...\")\n",
    "            print(f\" \")\n",
    "            \n",
    "        if N <= chunk_size:\n",
    "            theta_rad = torch.atan2( # [B, N, N]\n",
    "                y.unsqueeze(2) - y.unsqueeze(1), # [B, N, 1] - [B, 1, N] -> [B, N, N]\n",
    "                x.unsqueeze(2) - x.unsqueeze(1), # [B, N, 1] - [B, 1, N] -> [B, N, N]\n",
    "            )\n",
    "        else:\n",
    "            if self.debug:\n",
    "                print(f\"Using chunking for angle computation with chunk_size={chunk_size} for N={N}\")\n",
    "            \n",
    "            theta_rad = torch.zeros(B, N, N, device=x.device, dtype=x.dtype) # [B, N, N]\n",
    "            \n",
    "            for i in range(0, N, chunk_size):\n",
    "                end_i = min(i + chunk_size, N)\n",
    "                \n",
    "                x_chunk = x[:, i:end_i] # [B, chunk_size]\n",
    "                y_chunk = y[:, i:end_i] # [B, chunk_size]\n",
    "                \n",
    "                dy = y_chunk.unsqueeze(2) - y.unsqueeze(1) # [B, chunk_size, 1] - [B, 1, N] -> [B, chunk_size, N]\n",
    "                dx = x_chunk.unsqueeze(2) - x.unsqueeze(1) # [B, chunk_size, 1] - [B, 1, N] -> [B, chunk_size, N]\n",
    "                \n",
    "                theta_chunk = torch.atan2(dy, dx) # [B, chunk_size, N]\n",
    "                theta_rad[:, i:end_i, :] = theta_chunk # [B, chunk_size, N]\n",
    "                \n",
    "        theta_deg = theta_rad * (180.0 / math.pi) # [B, N, N]\n",
    "        if self.debug:\n",
    "            print(f\"theta_rad Shape: {theta_rad.shape}\")\n",
    "            print(f\"theta_deg Shape: {theta_deg.shape}\")\n",
    "            \n",
    "        if self.debug:\n",
    "            print(\"-\"*59)\n",
    "        return theta_rad\n",
    "\n",
    "    def compute_electric_force(self, ipt, theta_rad, inverse_sq_dist, chunk_size = 512):\n",
    "        \n",
    "        \"\"\"\n",
    "        Compute the Net Electric Force acted on each point\n",
    "        \n",
    "        Input:\n",
    "            - ipt: Input Image | [B, 3, self.imsize, self.imsize]\n",
    "            - theta_rad: The angle of forces acted on each point with respect to the x-axis | [B, N, N]\n",
    "            - inverse_sq_dist: 1/r^2, the inverse squared distance of each point with respect to every point in zeta | [B, N, N]\n",
    "    \n",
    "        Output:\n",
    "            - Fx: Force acted on each point in the x-axis | [B, N]\n",
    "            - Fy: Force acted on each point in the y-axis | [B, N]\n",
    "    \n",
    "        How the Function Works:\n",
    "            - F_E = qE = qi (k_E sum(qj/r^2))\n",
    "                + Since all variables are vectors, we split them into x and y axis, respectively\n",
    "        \"\"\"\n",
    "        \n",
    "        k_E = 1.0 / (4.0 * math.pi *torch.abs(self.adaptive_epsilon))\n",
    "        \n",
    "        B, C, H, W = ipt.shape\n",
    "        N = H*W\n",
    "        \n",
    "        ipt_reshaped = ipt.reshape(B, N, C) # [B, N, C]\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Running compute_electric_force...\")\n",
    "            print(\" \")\n",
    "    \n",
    "        if N <= chunk_size:\n",
    "            # Small enough - use original approach\n",
    "            qi_qj = ipt_reshaped.unsqueeze(2) * ipt_reshaped.unsqueeze(1) # [B, N, 1, C] * [B, 1, N, C] -> [B, N, N, C]\n",
    "            qi_qj = torch.mean(qi_qj, dim = 3) # [B, N, N]\n",
    "            if self.debug:\n",
    "                print(f\"qi_qj Shape: {qi_qj.shape}\") # [B, N, N]\n",
    "    \n",
    "            Fx = k_E * qi_qj * inverse_sq_dist * torch.cos(theta_rad) # [B, N, N]\n",
    "            Fy = k_E * qi_qj * inverse_sq_dist * torch.sin(theta_rad) # [B, N, N]\n",
    "    \n",
    "            Fx = torch.sum(Fx, dim = 2) # [B, N]\n",
    "            Fy = torch.sum(Fy, dim = 2) # [B, N]\n",
    "        else:\n",
    "            # CHUNKED BROADCASTING - avoid creating large [B, N, N, C] tensor\n",
    "            if self.debug:\n",
    "                print(f\"Using chunked broadcasting for force computation with chunk_size={chunk_size} for N={N}\")\n",
    "            \n",
    "            Fx = torch.zeros(B, N, device=ipt.device, dtype=ipt.dtype) # [B, N]\n",
    "            Fy = torch.zeros(B, N, device=ipt.device, dtype=ipt.dtype) # [B, N]\n",
    "            \n",
    "            for i in range(0, N, chunk_size):\n",
    "                end_i = min(i + chunk_size, N)\n",
    "                \n",
    "                # Get chunk of qi values\n",
    "                qi_chunk = ipt_reshaped[:, i:end_i, :] # [B, chunk_size, C]\n",
    "                \n",
    "                # Initialize qi_qj chunk for this row chunk\n",
    "                qi_qj_chunk = torch.zeros(B, end_i - i, N, device=ipt.device, dtype=ipt.dtype) # [B, chunk_size, N]\n",
    "                \n",
    "                # CHUNKED COMPUTATION of qi_qj to avoid large broadcasting\n",
    "                for j in range(0, N, chunk_size):\n",
    "                    end_j = min(j + chunk_size, N)\n",
    "                    \n",
    "                    # Get chunk of qj values  \n",
    "                    qj_chunk = ipt_reshaped[:, j:end_j, :] # [B, chunk_size_j, C]\n",
    "                    \n",
    "                    # Small broadcasting: [B, chunk_i, 1, C] * [B, 1, chunk_j, C] -> [B, chunk_i, chunk_j, C]\n",
    "                    qi_expanded = qi_chunk.unsqueeze(2) # [B, chunk_i, 1, C]\n",
    "                    qj_expanded = qj_chunk.unsqueeze(1) # [B, 1, chunk_j, C]\n",
    "                    qi_qj_small = qi_expanded * qj_expanded # [B, chunk_i, chunk_j, C]\n",
    "                    qi_qj_small = torch.mean(qi_qj_small, dim=3) # [B, chunk_i, chunk_j]\n",
    "                    \n",
    "                    # Store in the chunk\n",
    "                    qi_qj_chunk[:, :, j:end_j] = qi_qj_small # [B, chunk_i, chunk_j]\n",
    "                    \n",
    "                    # Cleanup small tensors\n",
    "                    del qi_expanded, qj_expanded, qi_qj_small\n",
    "                \n",
    "                # Get corresponding chunks for force computation\n",
    "                inverse_sq_dist_chunk = inverse_sq_dist[:, i:end_i, :] # [B, chunk_size, N]\n",
    "                theta_rad_chunk = theta_rad[:, i:end_i, :] # [B, chunk_size, N]\n",
    "                \n",
    "                # Compute forces for this chunk\n",
    "                Fx_chunk = k_E * qi_qj_chunk * inverse_sq_dist_chunk * torch.cos(theta_rad_chunk) # [B, chunk_size, N]\n",
    "                Fy_chunk = k_E * qi_qj_chunk * inverse_sq_dist_chunk * torch.sin(theta_rad_chunk) # [B, chunk_size, N]\n",
    "                \n",
    "                # Sum across interactions\n",
    "                Fx[:, i:end_i] = torch.sum(Fx_chunk, dim=2) # [B, chunk_size]\n",
    "                Fy[:, i:end_i] = torch.sum(Fy_chunk, dim=2) # [B, chunk_size]\n",
    "                \n",
    "                # Cleanup chunk tensors\n",
    "                del qi_chunk, qi_qj_chunk, inverse_sq_dist_chunk, theta_rad_chunk, Fx_chunk, Fy_chunk\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Fx Shape: {Fx.shape}\") # [B, N]\n",
    "            print(f\"Fy Shape: {Fy.shape}\") # [B, N]\n",
    "            print(\"-\"*59)\n",
    "            \n",
    "        return Fx, Fy\n",
    "\n",
    "    def compute_vector_translation(self, Fx, Fy, zeta):\n",
    "        \"\"\"\n",
    "        Move each point in the vector space corresponding to forces acted on it.\n",
    "\n",
    "        Input:\n",
    "            - Fx: Force acted on each point in the x-axis | [B, N]\n",
    "            - Fy: Force acted on each point in the y-axis | [B, N]\n",
    "            - zeta: Coordinates of each point (x, y) | [B, 2, self.imsize, self.imsize]\n",
    "\n",
    "        Output:\n",
    "            - new_zeta: Fully updated Coordinates of each point (x, y) | [B, 2, self.imsize, self.imsize]\n",
    "\n",
    "        How the Function Works:\n",
    "            - Displacement = Velocity x time + Initial Position -> D =  vt + x0\n",
    "            - v = Acceleration x time + Initial Velocity = at + v0\n",
    "            - Acceleration = Force/Mass = F/m\n",
    "                + Since all variables are vectors, we split them into x and y axis, respectively\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Starting compute_vector_translation...\")\n",
    "            print(\" \")\n",
    "            \n",
    "        ax = Fx / torch.abs(self.adaptive_mass) # [B, N]\n",
    "        ay = Fy / torch.abs(self.adaptive_mass) # [B, N]\n",
    "        if self.debug:\n",
    "            print(f\"ax Shape: {ax.shape}\")\n",
    "            print(f\"ay Shape: {ay.shape}\")\n",
    "            \n",
    "        B, N = Fx.shape\n",
    "        v0 = self.v0.expand(B, N) # [B, N]\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"v0 Shape: {v0.shape}\")\n",
    "            \n",
    "        time_step = torch.abs(self.adaptive_time)\n",
    "        vx = v0 + ax * time_step # [B, N]\n",
    "        vy = v0 + ay * time_step # [B, N]\n",
    "        vx = vx.reshape(B, self.imsize, self.imsize) # [B, self.imsize, self.imsize]\n",
    "        vy = vy.reshape(B, self.imsize, self.imsize) # [B, self.imsize, self.imsize]\n",
    "        if self.debug:\n",
    "            print(f\"vx Shape: {vx.shape}\")\n",
    "            print(f\"vy Shape: {vy.shape}\")\n",
    "            \n",
    "        if self.debug:\n",
    "            print(f\"zeta Shape: {zeta.shape}\")\n",
    "            print(f\"zeta[:, 0] Shape: {zeta[:, 0].shape}\")\n",
    "            print(f\"zeta[:, 1] Shape: {zeta[:, 1].shape}\")\n",
    "        new_zeta = zeta.clone()\n",
    "        new_zeta[:, 0] = zeta[:, 0] + vx # [B, 2, self.imsize, self.imsize]\n",
    "        new_zeta[:, 1] = zeta[:, 1] + vy # [B, 2, self.imsize, self.imsize]\n",
    "\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Updated zeta Shape: {new_zeta.shape}\")\n",
    "            print(f\"-\"*59)\n",
    "            \n",
    "        return new_zeta\n",
    "\n",
    "    def update_input(self, ipt, zeta):\n",
    "        \n",
    "        \"\"\"\n",
    "        Update Input by Unmapping zeta\n",
    "\n",
    "        Input:\n",
    "            - ipt: Input Image | [B, 3, self.imsize, self.imsize]\n",
    "            - zeta: Coordinates of each point (x, y) | [B, 2, self.imsize, self.imsize]\n",
    "\n",
    "        Output:\n",
    "            - new_ipt: New Input Image | [B, 3, self.imsize, self.imsize]\n",
    "\n",
    "        How the Function Works:\n",
    "            - You got the new zeta Coordinates\n",
    "            - You unmap it using a point-wise-convolutional-layer\n",
    "            - New Input will be the summation of the unmapped zeta and the Input Image\n",
    "        \"\"\"\n",
    "        if self.debug:\n",
    "            print(f\"Running update_input...\")\n",
    "            \n",
    "        map_to_ipt = self.mapping_to_ipt(zeta) # [B, 3, self.imsize, self.imsize]\n",
    "        new_ipt = map_to_ipt*self.beta + ipt*(1-self.beta) # [B, 3, self.imsize, self.imsize]\n",
    "        return new_ipt\n",
    "\n",
    "    def compute_energy_of_system(self, ipt, inverse_dist, chunk_size = 512):\n",
    "    \n",
    "        \"\"\"\n",
    "        Compute total Energy of the System\n",
    "    \n",
    "        Input:\n",
    "            - ipt: Final Input Image | [B, 3, self.imsize, self.imsize]\n",
    "            - inverse_dist: 1/r, the inverse distance of each point with respect to every point in zeta | [B, N, N]\n",
    "        \n",
    "        Output:\n",
    "            - V: Total Energy of Final Input Image | [B]\n",
    "    \n",
    "        How the Function Works:\n",
    "            - V = sum(qi * Vj) = qi (k_E sum(qj/r^2))\n",
    "        \"\"\"\n",
    "        \n",
    "        k_E = 1.0 / (4.0 * math.pi * torch.abs(self.adaptive_epsilon))\n",
    "        \n",
    "        B, C, H, W = ipt.shape\n",
    "        N = H*W\n",
    "        \n",
    "        ipt_reshaped = ipt.reshape(B, N, C) # [B, N, C]\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Running compute_energy_of_system...\")\n",
    "            print(\" \")\n",
    "    \n",
    "        if N <= chunk_size:\n",
    "            # Small enough - use original approach\n",
    "            qi_qj = ipt_reshaped.unsqueeze(2) * ipt_reshaped.unsqueeze(1) # [B, N, 1, C] * [B, 1, N, C] -> [B, N, N, C]\n",
    "            qi_qj = torch.mean(qi_qj, dim = 3) # [B, N, N]\n",
    "            if self.debug:\n",
    "                print(f\"qi_qj Shape: {qi_qj.shape}\") # [B, N, N]\n",
    "    \n",
    "            V = k_E * qi_qj * inverse_dist # [B, N, N]\n",
    "            V = torch.sum(V, dim = 2) # [B, N]\n",
    "            V = torch.sum(V, dim = 1) # [B]\n",
    "        else:\n",
    "            # CHUNKED BROADCASTING - avoid creating large [B, N, N, C] tensor\n",
    "            if self.debug:\n",
    "                print(f\"Using chunked broadcasting for energy computation with chunk_size={chunk_size} for N={N}\")\n",
    "            \n",
    "            V = torch.zeros(B, device=ipt.device, dtype=ipt.dtype) # [B]\n",
    "            \n",
    "            for i in range(0, N, chunk_size):\n",
    "                end_i = min(i + chunk_size, N)\n",
    "                \n",
    "                # Get chunk of qi values\n",
    "                qi_chunk = ipt_reshaped[:, i:end_i, :] # [B, chunk_size, C]\n",
    "                \n",
    "                # Initialize qi_qj chunk for this row chunk\n",
    "                qi_qj_chunk = torch.zeros(B, end_i - i, N, device=ipt.device, dtype=ipt.dtype) # [B, chunk_size, N]\n",
    "                \n",
    "                # CHUNKED COMPUTATION of qi_qj to avoid large broadcasting\n",
    "                for j in range(0, N, chunk_size):\n",
    "                    end_j = min(j + chunk_size, N)\n",
    "                    \n",
    "                    # Get chunk of qj values\n",
    "                    qj_chunk = ipt_reshaped[:, j:end_j, :] # [B, chunk_size_j, C]\n",
    "                    \n",
    "                    # Small broadcasting: [B, chunk_i, 1, C] * [B, 1, chunk_j, C] -> [B, chunk_i, chunk_j, C]\n",
    "                    qi_expanded = qi_chunk.unsqueeze(2) # [B, chunk_i, 1, C]\n",
    "                    qj_expanded = qj_chunk.unsqueeze(1) # [B, 1, chunk_j, C]\n",
    "                    qi_qj_small = qi_expanded * qj_expanded # [B, chunk_i, chunk_j, C]\n",
    "                    qi_qj_small = torch.mean(qi_qj_small, dim=3) # [B, chunk_i, chunk_j]\n",
    "                    \n",
    "                    # Store in the chunk\n",
    "                    qi_qj_chunk[:, :, j:end_j] = qi_qj_small # [B, chunk_i, chunk_j]\n",
    "                    \n",
    "                    # Cleanup small tensors\n",
    "                    del qi_expanded, qj_expanded, qi_qj_small\n",
    "                \n",
    "                # Get corresponding distance chunk\n",
    "                inverse_dist_chunk = inverse_dist[:, i:end_i, :] # [B, chunk_size, N]\n",
    "                \n",
    "                # Compute energy for this chunk\n",
    "                V_chunk = k_E * qi_qj_chunk * inverse_dist_chunk # [B, chunk_size, N]\n",
    "                V_chunk = torch.sum(V_chunk, dim=2) # [B, chunk_size]\n",
    "                V_chunk = torch.sum(V_chunk, dim=1) # [B]\n",
    "                \n",
    "                # Accumulate energy\n",
    "                V += V_chunk # [B]\n",
    "                \n",
    "                # Cleanup chunk tensors\n",
    "                del qi_chunk, qi_qj_chunk, inverse_dist_chunk, V_chunk\n",
    "                \n",
    "        if self.debug:\n",
    "            print(f\"V Shape: {V.shape}\") # [B]\n",
    "            print(\"-\"*59)\n",
    "            \n",
    "        return V\n",
    "\n",
    "    def aggregate_to_one_function(self, ipt, chunk_size):\n",
    "    \n",
    "        k_E = 1.0 / (4.0 * math.pi * torch.abs(self.adaptive_epsilon))\n",
    "        epsilon_dist_sq = 1e-12 # For squared distances to avoid division by zero\n",
    "    \n",
    "        if self.debug:\n",
    "            print(f\"Running aggregate_to_one_function (ULTRA EFFICIENT)\")\n",
    "            print(f\"  ipt Shape: {ipt.shape}\")\n",
    "    \n",
    "        # 1. Compute zeta (spatial coordinates)\n",
    "        # These are computed once and reused. Their memory is relatively small.\n",
    "        zeta_raw = self.mapping_to_vector_space(ipt)\n",
    "        zeta = self.space_norm(zeta_raw)  # [B, 2, H, W]\n",
    "        B, C_zeta, H, W = zeta.shape\n",
    "        N = H * W\n",
    "    \n",
    "        if self.debug: print(f\"  zeta Shape: {zeta.shape}\")\n",
    "    \n",
    "        # Global accumulators for total forces and system potential\n",
    "        Fx_total = torch.zeros(B, N, device=ipt.device, dtype=ipt.dtype)\n",
    "        Fy_total = torch.zeros(B, N, device=ipt.device, dtype=ipt.dtype)\n",
    "        V_total_system = torch.zeros(B, device=ipt.device, dtype=ipt.dtype) # Scalar potential for the whole system\n",
    "    \n",
    "        # Flattened coordinates and charges (properties from ipt)\n",
    "        # These are [B, N, Dims]\n",
    "        pts_zeta_coords_all = zeta.reshape(B, 2, N)\n",
    "        x_coords_all = pts_zeta_coords_all[:, 0, :] # [B, N]\n",
    "        y_coords_all = pts_zeta_coords_all[:, 1, :] # [B, N]\n",
    "        zflat_for_cdist_all = pts_zeta_coords_all.permute(0, 2, 1) # [B, N, 2] (for torch.cdist)\n",
    "    \n",
    "        B_ipt, C_ipt, H_ipt, W_ipt = ipt.shape\n",
    "        ipt_flat_charges_all = ipt.reshape(B_ipt, C_ipt, N).permute(0, 2, 1) # [B, N, C_ipt]\n",
    "    \n",
    "        # Outer loop: Iterate over TARGET chunks (points experiencing the force)\n",
    "        for i_start in range(0, N, chunk_size):\n",
    "            i_end = min(i_start + chunk_size, N)\n",
    "            current_i_chunk_size = i_end - i_start\n",
    "    \n",
    "            if self.debug: print(f\"  Processing TARGET chunk: points {i_start} to {i_end-1}\")\n",
    "    \n",
    "            # Data for current TARGET chunk_i (small tensors)\n",
    "            target_coords_i_cdist = zflat_for_cdist_all[:, i_start:i_end, :]     # [B, current_i_chunk_size, 2]\n",
    "            target_x_i = x_coords_all[:, i_start:i_end]                         # [B, current_i_chunk_size]\n",
    "            target_y_i = y_coords_all[:, i_start:i_end]                         # [B, current_i_chunk_size]\n",
    "            target_charges_qi = ipt_flat_charges_all[:, i_start:i_end, :]       # [B, current_i_chunk_size, C_ipt]\n",
    "    \n",
    "            # Accumulators for forces/potential ON this i-th target chunk\n",
    "            Fx_on_i_accum = torch.zeros(B, current_i_chunk_size, device=ipt.device, dtype=ipt.dtype)\n",
    "            Fy_on_i_accum = torch.zeros(B, current_i_chunk_size, device=ipt.device, dtype=ipt.dtype)\n",
    "            V_on_i_accum = torch.zeros(B, current_i_chunk_size, device=ipt.device, dtype=ipt.dtype) # Potential energy for each point in i-chunk\n",
    "    \n",
    "            # Inner loop: Iterate over SOURCE chunks (points exerting the force)\n",
    "            for j_start in range(0, N, chunk_size):\n",
    "                j_end = min(j_start + chunk_size, N)\n",
    "                current_j_chunk_size = j_end - j_start\n",
    "    \n",
    "                # Data for current SOURCE chunk_j (small tensors)\n",
    "                source_coords_j_cdist = zflat_for_cdist_all[:, j_start:j_end, :]   # [B, current_j_chunk_size, 2]\n",
    "                source_x_j = x_coords_all[:, j_start:j_end]                       # [B, current_j_chunk_size]\n",
    "                source_y_j = y_coords_all[:, j_start:j_end]                       # [B, current_j_chunk_size]\n",
    "                source_charges_qj = ipt_flat_charges_all[:, j_start:j_end, :]     # [B, current_j_chunk_size, C_ipt]\n",
    "    \n",
    "                # --- Compute interactions between current chunk_i and current chunk_j ---\n",
    "                # All tensors created here are [B, current_i_chunk_size, current_j_chunk_size]\n",
    "    \n",
    "                # A. Distances (r_ij)\n",
    "                # distances_ij: [B, current_i_chunk_size, current_j_chunk_size]\n",
    "                distances_ij = torch.cdist(target_coords_i_cdist, source_coords_j_cdist, p=2)\n",
    "    \n",
    "                # B. Angles (theta_ij of vector from j to i)\n",
    "                # dx_ji = x_i - x_j ; dy_ji = y_i - y_j\n",
    "                dx_ji_sub = target_x_i.unsqueeze(2) - source_x_j.unsqueeze(1)\n",
    "                dy_ji_sub = target_y_i.unsqueeze(2) - source_y_j.unsqueeze(1)\n",
    "                theta_ji_sub = torch.atan2(dy_ji_sub, dx_ji_sub) # Angle of vector R_ji (from source j to target i)\n",
    "    \n",
    "                # C. Charge interaction term (qi_qj_effective)\n",
    "                qi_expanded = target_charges_qi.unsqueeze(2)             # [B, size_i, 1, C_ipt]\n",
    "                qj_expanded = source_charges_qj.unsqueeze(1)             # [B, 1, size_j, C_ipt]\n",
    "                qi_qj_channels = qi_expanded * qj_expanded               # [B, size_i, size_j, C_ipt]\n",
    "                qi_qj_scalar_sub = torch.mean(qi_qj_channels, dim=3)   # [B, size_i, size_j]\n",
    "    \n",
    "                # D. Handle self-interaction if target and source chunks overlap\n",
    "                if i_start == j_start: # Only diagonal sub-chunks can have self-interaction\n",
    "                    diag_sub_mask = torch.eye(current_i_chunk_size, current_j_chunk_size,\n",
    "                                              device=ipt.device, dtype=torch.bool)\n",
    "                    # Ensure broadcasting if one chunk is smaller at the edge\n",
    "                    min_dim_self = min(current_i_chunk_size, current_j_chunk_size)\n",
    "                    diag_sub_mask_final = diag_sub_mask[:min_dim_self, :min_dim_self]\n",
    "                    \n",
    "                    distances_ij[:, :min_dim_self, :min_dim_self] = torch.where(\n",
    "                        diag_sub_mask_final,\n",
    "                        float('inf'),\n",
    "                        distances_ij[:, :min_dim_self, :min_dim_self]\n",
    "                    )\n",
    "    \n",
    "    \n",
    "                # E. Potential Energy V_ij = k_E * (qi_qj)_ij / r_ij\n",
    "                inv_dist_ij = torch.pow(distances_ij, -1) # Handles inf correctly -> 0.0\n",
    "                V_ij_sub_values = k_E * qi_qj_scalar_sub * inv_dist_ij\n",
    "    \n",
    "                # F. Force Components F_ij = (k_E * (qi_qj)_ij / r_ij^2) * unit_vector_ji\n",
    "                # F_radial_signed = k_E * qi_qj_scalar_sub * (1/r_ij^2)\n",
    "                # (1/r_ij^2) can be inv_dist_ij.pow(2) or 1.0 / (distances_ij.pow(2) + epsilon_dist_sq)\n",
    "                inv_sq_dist_ij = 1.0 / (distances_ij.pow(2)) # Add epsilon for stability if r can be 0 for non-self\n",
    "                inv_sq_dist_ij = torch.where(torch.isinf(distances_ij), torch.zeros_like(inv_sq_dist_ij), inv_sq_dist_ij) # Ensure inf dist -> 0 force\n",
    "    \n",
    "                F_radial_signed_ij = k_E * qi_qj_scalar_sub * inv_sq_dist_ij\n",
    "    \n",
    "                Fx_ij_sub = F_radial_signed_ij * torch.cos(theta_ji_sub)\n",
    "                Fy_ij_sub = F_radial_signed_ij * torch.sin(theta_ji_sub)\n",
    "    \n",
    "                # G. Accumulate forces and potential for the current target_i_chunk\n",
    "                Fx_on_i_accum += torch.sum(Fx_ij_sub, dim=2) # Sum over current source_j_chunk dimension\n",
    "                Fy_on_i_accum += torch.sum(Fy_ij_sub, dim=2)\n",
    "                V_on_i_accum += torch.sum(V_ij_sub_values, dim=2) # Sum potential contributions for each point in i_chunk\n",
    "    \n",
    "                # H. Explicitly delete intermediate sub-chunk tensors\n",
    "                del distances_ij, dx_ji_sub, dy_ji_sub, theta_ji_sub, qi_expanded, qj_expanded\n",
    "                del qi_qj_channels, qi_qj_scalar_sub, inv_dist_ij, V_ij_sub_values\n",
    "                del inv_sq_dist_ij, F_radial_signed_ij, Fx_ij_sub, Fy_ij_sub\n",
    "                if torch.cuda.is_available(): torch.cuda.empty_cache() # Aggressive cache clearing\n",
    "    \n",
    "            # Store results for the fully processed i-th target chunk\n",
    "            Fx_total[:, i_start:i_end] = Fx_on_i_accum\n",
    "            Fy_total[:, i_start:i_end] = Fy_on_i_accum\n",
    "            V_total_system += torch.sum(V_on_i_accum, dim=1) # Sum potential over points in i-chunk, then add to system total\n",
    "    \n",
    "            # Explicitly delete i-chunk accumulators\n",
    "            del target_coords_i_cdist, target_x_i, target_y_i, target_charges_qi\n",
    "            del Fx_on_i_accum, Fy_on_i_accum, V_on_i_accum\n",
    "            if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "    \n",
    "        if self.debug:\n",
    "            print(f\"  aggregate_to_one_function (ULTRA EFFICIENT) finished.\")\n",
    "            if Fx_total.numel() > 0: print(f\"  Fx_total max abs: {Fx_total.abs().max().item()}\")\n",
    "            if Fy_total.numel() > 0: print(f\"  Fy_total max abs: {Fy_total.abs().max().item()}\")\n",
    "            if V_total_system.numel() > 0: print(f\"  V_total_system: {V_total_system.item()}\")\n",
    "            print(\"-\" * 59)\n",
    "    \n",
    "        return Fx_total, Fy_total, V_total_system, zeta # Make sure zeta is returned\n",
    "\n",
    "            \n",
    "            \n",
    "    def forward(self, ipt, chunk_size=16):\n",
    "        assert ipt.shape[2] and ipt.shape[3] == self.imsize, \"Input Shape's Height and Width must match parameter imsize\"\n",
    "        for layer in range(self.layers):\n",
    "            \n",
    "            if self.use_self_attention:\n",
    "                attended_ipt = self.input_attention(ipt)\n",
    "                ipt = self.attention_scale * attended_ipt + ipt\n",
    "            if self.debug:\n",
    "                print(f\"After self-attention ipt Shape: {ipt.shape}\")\n",
    "                \n",
    "            if self.debug:\n",
    "                print(f\"Running Layer {layer + 1}/{self.layers}\")\n",
    "                print(f\"-\"*59)\n",
    "            for loop in range(self.loops):\n",
    "                if self.debug:\n",
    "                    print(f\"Starting {loop + 1}/{self.loops} for compute_electric_force...\")\n",
    "                    print(\" \")\n",
    "                #zeta, x, y, D = self.compute_distance(ipt, chunk_size)\n",
    "                #theta_rad = self.compute_angle(x, y, chunk_size)\n",
    "                #Fx, Fy = self.compute_electric_force(ipt, theta_rad, D.pow(-2), chunk_size)\n",
    "                Fx, Fy, V, zeta = self.aggregate_to_one_function(ipt, chunk_size)\n",
    "                zeta = self.compute_vector_translation(Fx, Fy, zeta)\n",
    "    \n",
    "            zeta = self.space_norm(zeta)\n",
    "            ipt = self.update_input(ipt, zeta)\n",
    "            \n",
    "        ipt = self.space_norm(ipt)\n",
    "        Fx, Fy, V, zeta = self.aggregate_to_one_function(ipt, chunk_size)\n",
    "        V = torch.abs(V)\n",
    "        \n",
    "        return ipt, zeta, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b1bd862",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T14:37:00.013550Z",
     "iopub.status.busy": "2025-06-03T14:37:00.013064Z",
     "iopub.status.idle": "2025-06-03T14:37:00.018553Z",
     "shell.execute_reply": "2025-06-03T14:37:00.017747Z"
    },
    "papermill": {
     "duration": 0.011283,
     "end_time": "2025-06-03T14:37:00.019399",
     "exception": false,
     "start_time": "2025-06-03T14:37:00.008116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SegmentationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transforms=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # c nh v mask\n",
    "        image = np.array(Image.open(self.image_paths[idx]).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(self.mask_paths[idx]).convert(\"L\"))\n",
    "        \n",
    "        mask = np.where(mask < 85, 0, np.where(mask < 170, 1, 2))\n",
    "        \n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "            \n",
    "        mask = mask.long() if isinstance(mask, torch.Tensor) else torch.from_numpy(mask).long()\n",
    "        #print(mask.shape)\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef9f54fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T14:37:00.029484Z",
     "iopub.status.busy": "2025-06-03T14:37:00.029269Z",
     "iopub.status.idle": "2025-06-03T14:37:00.037395Z",
     "shell.execute_reply": "2025-06-03T14:37:00.036889Z"
    },
    "papermill": {
     "duration": 0.013802,
     "end_time": "2025-06-03T14:37:00.038169",
     "exception": false,
     "start_time": "2025-06-03T14:37:00.024367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_multi_iou(outputs, masks, num_classes=3):\n",
    "    probs = torch.softmax(outputs, dim=1)\n",
    "    preds = torch.argmax(probs, dim=1)\n",
    "    iou_per_class = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_cls = (preds == cls)\n",
    "        target_cls = (masks == cls)\n",
    "        intersection = (pred_cls & target_cls).sum().float()\n",
    "        union = (pred_cls | target_cls).sum().float()\n",
    "        iou = (intersection / (union + 1e-8)).item()\n",
    "        iou_per_class.append(iou)\n",
    "    return np.mean(iou_per_class)\n",
    "\n",
    "def compute_dice_score(outputs, masks, num_classes=3):\n",
    "    probs = torch.softmax(outputs, dim=1)\n",
    "    preds = torch.argmax(probs, dim=1)\n",
    "    dice_per_class = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_cls = (preds == cls)\n",
    "        target_cls = (masks == cls)\n",
    "        intersection = (pred_cls & target_cls).sum().float()\n",
    "        dice = (2. * intersection / (pred_cls.sum() + target_cls.sum() + 1e-8)).item()\n",
    "        dice_per_class.append(dice)\n",
    "    return np.mean(dice_per_class)\n",
    "\n",
    "def compute_precision(outputs, masks, num_classes=3):\n",
    "    probs = torch.softmax(outputs, dim=1)\n",
    "    preds = torch.argmax(probs, dim=1)\n",
    "    precision_per_class = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_cls = (preds == cls)\n",
    "        target_cls = (masks == cls)\n",
    "        true_positive = (pred_cls & target_cls).sum().float()\n",
    "        predicted_positive = pred_cls.sum().float()\n",
    "        precision = (true_positive / (predicted_positive + 1e-8)).item()\n",
    "        precision_per_class.append(precision)\n",
    "    return np.mean(precision_per_class)\n",
    "\n",
    "def compute_accuracy(outputs, masks):\n",
    "    probs = torch.softmax(outputs, dim=1)\n",
    "    preds = torch.argmax(probs, dim=1)\n",
    "    correct = (preds == masks).sum().float()\n",
    "    total = torch.numel(masks)\n",
    "    return (correct / total).item()\n",
    "\n",
    "\n",
    "def post_process(mask, kernel_size=3):\n",
    "    import cv2\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    \n",
    "    cleaned = cv2.morphologyEx(mask.numpy().astype(np.uint8), \n",
    "                              cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    cleaned = cv2.morphologyEx(cleaned, \n",
    "                              cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    return torch.from_numpy(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "560a7a80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T14:37:00.047613Z",
     "iopub.status.busy": "2025-06-03T14:37:00.047159Z",
     "iopub.status.idle": "2025-06-03T14:37:00.051586Z",
     "shell.execute_reply": "2025-06-03T14:37:00.051094Z"
    },
    "papermill": {
     "duration": 0.009947,
     "end_time": "2025-06-03T14:37:00.052406",
     "exception": false,
     "start_time": "2025-06-03T14:37:00.042459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Mixup:\n",
    "    def __init__(self, alpha=0.4, p=0.5):\n",
    "        self.alpha = alpha\n",
    "        self.p = p\n",
    "        \n",
    "    def __call__(self, images, masks):\n",
    "        if np.random.rand() < self.p:\n",
    "            batch_size = len(images) if isinstance(images, list) else 1\n",
    "            if batch_size > 1:\n",
    "                indices = np.random.permutation(batch_size)\n",
    "                img2, mask2 = images[indices], masks[indices]\n",
    "                \n",
    "                lam = np.random.beta(self.alpha, self.alpha)\n",
    "\n",
    "                mixed_img = lam * images + (1 - lam) * img2\n",
    "                mixed_mask = lam * masks + (1 - lam) * mask2\n",
    "                \n",
    "                return mixed_img, mixed_mask\n",
    "        return images, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d356ed67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T14:37:00.061911Z",
     "iopub.status.busy": "2025-06-03T14:37:00.061471Z",
     "iopub.status.idle": "2025-06-03T14:37:00.065692Z",
     "shell.execute_reply": "2025-06-03T14:37:00.065218Z"
    },
    "papermill": {
     "duration": 0.009796,
     "end_time": "2025-06-03T14:37:00.066570",
     "exception": false,
     "start_time": "2025-06-03T14:37:00.056774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "def get_valid_transform():\n",
    "    return A.Compose([\n",
    "        A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "        ToTensorV2(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de870b3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T14:37:00.075932Z",
     "iopub.status.busy": "2025-06-03T14:37:00.075507Z",
     "iopub.status.idle": "2025-06-03T14:37:00.082196Z",
     "shell.execute_reply": "2025-06-03T14:37:00.081719Z"
    },
    "papermill": {
     "duration": 0.012189,
     "end_time": "2025-06-03T14:37:00.082980",
     "exception": false,
     "start_time": "2025-06-03T14:37:00.070791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_predictions(model, dataloader, device, num_samples=4):\n",
    "    model.eval()\n",
    "    images, masks, preds = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img, mask in dataloader:\n",
    "            img = img.to(device)\n",
    "            output = model(img)\n",
    "            pred = torch.argmax(torch.softmax(output, dim=1), dim=1)\n",
    "            \n",
    "            for i in range(min(img.shape[0], num_samples - len(images))):\n",
    "                if len(images) < num_samples:\n",
    "                    images.append(img[i].cpu())\n",
    "                    masks.append(mask[i].cpu())\n",
    "                    preds.append(pred[i].cpu())\n",
    "            \n",
    "            if len(images) >= num_samples:\n",
    "                break\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, num_samples * 5))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        img = images[i].permute(1, 2, 0).numpy()\n",
    "        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        axes[i, 0].imshow(img)\n",
    "        axes[i, 0].set_title('Original Image')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(masks[i], cmap='jet')\n",
    "        axes[i, 1].set_title('Ground Truth')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        axes[i, 2].imshow(preds[i], cmap='jet')\n",
    "        axes[i, 2].set_title('Prediction')\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7234884",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T14:37:00.092424Z",
     "iopub.status.busy": "2025-06-03T14:37:00.091985Z",
     "iopub.status.idle": "2025-06-03T14:37:00.098651Z",
     "shell.execute_reply": "2025-06-03T14:37:00.098155Z"
    },
    "papermill": {
     "duration": 0.012297,
     "end_time": "2025-06-03T14:37:00.099562",
     "exception": false,
     "start_time": "2025-06-03T14:37:00.087265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion, device, tau):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc='Training')\n",
    "    for images, masks in pbar:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs, zeta, V = model(images)\n",
    "        loss = criterion(outputs, masks)*(1-tau) + V*tau\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return train_loss / len(dataloader)\n",
    "\n",
    "# Validation function\n",
    "def validate(model, dataloader, criterion, device, tau):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    iou_scores, dsc_scores, precision_scores, acc_scores = [], [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(dataloader, desc='Validation'):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            outputs, zeta, V = model(images)\n",
    "            #outputs = convert_model_output_to_segmentation(outputs)\n",
    "            loss = criterion(outputs, masks)*(1-tau) + V*tau\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            iou_scores.append(compute_multi_iou(outputs, masks))\n",
    "            dsc_scores.append(compute_dice_score(outputs, masks))\n",
    "            precision_scores.append(compute_precision(outputs, masks))\n",
    "            acc_scores.append(compute_accuracy(outputs, masks))\n",
    "    \n",
    "    mean_iou = np.mean(iou_scores)\n",
    "    mean_dsc = np.mean(dsc_scores)\n",
    "    mean_precision = np.mean(precision_scores)\n",
    "    mean_acc = np.mean(acc_scores)\n",
    "    \n",
    "    return val_loss / len(dataloader), mean_iou, mean_dsc, mean_precision, mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "634ae280",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T14:37:00.108963Z",
     "iopub.status.busy": "2025-06-03T14:37:00.108599Z",
     "iopub.status.idle": "2025-06-03T14:37:00.114679Z",
     "shell.execute_reply": "2025-06-03T14:37:00.114212Z"
    },
    "papermill": {
     "duration": 0.011648,
     "end_time": "2025-06-03T14:37:00.115569",
     "exception": false,
     "start_time": "2025-06-03T14:37:00.103921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs, device, fold=None):\n",
    "    best_metrics = {'mean_iou': 0.0, 'mean_dsc': 0.0}\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'iou': [], 'dsc': [], 'precision': [], 'accuracy': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, tau = 0.2)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        \n",
    "        val_loss, mean_iou, mean_dsc, mean_precision, mean_acc = validate(model, val_loader, criterion, device, tau = 0.2)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['iou'].append(mean_iou)\n",
    "        history['dsc'].append(mean_dsc)\n",
    "        history['precision'].append(mean_precision)\n",
    "        history['accuracy'].append(mean_acc)\n",
    "        \n",
    "        scheduler.step(mean_dsc)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"Accuracy: {mean_acc:.4f}, Mean IoU: {mean_iou:.4f}, Mean DSC/F1: {mean_dsc:.4f}, Mean Precision: {mean_precision:.4f}\")\n",
    "        \n",
    "        if mean_dsc > best_metrics['mean_dsc']:\n",
    "            best_metrics['mean_dsc'] = mean_dsc\n",
    "            best_metrics['mean_iou'] = mean_iou\n",
    "            best_metrics['mean_precision'] = mean_precision\n",
    "            best_metrics['mean_acc'] = mean_acc\n",
    "            \n",
    "            model_name = f\"best_model_fold{fold}.pth\" if fold is not None else \"best_model.pth\"\n",
    "            torch.save(model.state_dict(), model_name)\n",
    "            print(f\"Saved best model with DSC: {mean_dsc:.4f}\")\n",
    "        \n",
    "        print(f\"Best DSC: {best_metrics['mean_dsc']:.4f}, Best IoU: {best_metrics['mean_iou']:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return model, history, best_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcb09f97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T14:37:00.124915Z",
     "iopub.status.busy": "2025-06-03T14:37:00.124727Z",
     "iopub.status.idle": "2025-06-03T14:37:00.137602Z",
     "shell.execute_reply": "2025-06-03T14:37:00.137118Z"
    },
    "papermill": {
     "duration": 0.018691,
     "end_time": "2025-06-03T14:37:00.138503",
     "exception": false,
     "start_time": "2025-06-03T14:37:00.119812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_optimal_k(all_images, all_masks, k_values=[3, 5, 7, 10], num_epochs=10, tau = 0.2):\n",
    "\n",
    "    k_scores = []\n",
    "    k_variances = []\n",
    "    train_times = []\n",
    "    \n",
    "    for k in k_values:\n",
    "        fold_scores = []\n",
    "        \n",
    "        kf = KFold(n_splits=k, shuffle=True, random_state=SEED)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(all_images)):\n",
    "            print(f\"  Fold {fold+1}/{k}\")\n",
    "            \n",
    "            train_img = [all_images[i] for i in train_idx]\n",
    "            train_mask = [all_masks[i] for i in train_idx]\n",
    "            val_img = [all_images[i] for i in val_idx]\n",
    "            val_mask = [all_masks[i] for i in val_idx]\n",
    "            \n",
    "            # To dataset\n",
    "            train_dataset = SegmentationDataset(train_img, train_mask, get_train_transform())\n",
    "            val_dataset = SegmentationDataset(val_img, val_mask, get_valid_transform())\n",
    "            \n",
    "            # To dataloader\n",
    "            train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "            \n",
    "            # Khi to model mi cho mi fold\n",
    "            model = ElectricForceModel().to(DEVICE)\n",
    "            \n",
    "            # Setup optimizer v loss\n",
    "            optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3)\n",
    "            \n",
    "            best_metrics = {'mean_dsc': 0.0}\n",
    "            \n",
    "            for epoch in range(num_epochs):\n",
    "                model.train()\n",
    "                for images, masks in train_loader:\n",
    "                    images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs, zeta, V = model(images)\n",
    "                    #outputs = convert_model_output_to_segmentation(outputs)\n",
    "                    loss = criterion(outputs, masks)*(1-tau) + V*tau\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                model.eval()\n",
    "                dsc_scores = []\n",
    "                with torch.no_grad():\n",
    "                    for images, masks in val_loader:\n",
    "                        images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
    "                        outputs, zeta, V = model(images)\n",
    "                        #outputs = conconvert_model_output_to_segmentation(outputs)\n",
    "                        dsc_scores.append(compute_dice_score(outputs, masks))\n",
    "                \n",
    "                mean_dsc = np.mean(dsc_scores)\n",
    "                scheduler.step(mean_dsc)\n",
    "                \n",
    "                if mean_dsc > best_metrics['mean_dsc']:\n",
    "                    best_metrics['mean_dsc'] = mean_dsc\n",
    "            \n",
    "            # Lu Dice score ca fold ny\n",
    "            fold_scores.append(best_metrics['mean_dsc'])\n",
    "            print(f\"    Dice score: {best_metrics['mean_dsc']:.4f}\")\n",
    "        \n",
    "        train_time = time.time() - start_time\n",
    "        train_times.append(train_time)\n",
    "        \n",
    "        mean_score = np.mean(fold_scores)\n",
    "        score_variance = np.var(fold_scores)\n",
    "        \n",
    "        k_scores.append(mean_score)\n",
    "        k_variances.append(score_variance)\n",
    "        \n",
    "        print(f\"K={k}, im trung bnh: {mean_score:.4f}, Phng sai: {score_variance:.6f}\")\n",
    "        print(f\"Thi gian training: {train_time:.2f} giy\")\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    ax1.plot(k_values, k_scores, 'o-', linewidth=2, color='blue')\n",
    "    ax1.set_xlabel('S lng Fold (K)')\n",
    "    ax1.set_ylabel('im DSC trung bnh')\n",
    "    ax1.set_title('Hiu sut theo gi tr K')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # V phng sai\n",
    "    ax2.plot(k_values, k_variances, 'o-', linewidth=2, color='orange')\n",
    "    ax2.set_xlabel('S lng Fold (K)')\n",
    "    ax2.set_ylabel('Phng sai')\n",
    "    ax2.set_title('Phng sai theo gi tr K')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # V thi gian training\n",
    "    ax3.plot(k_values, train_times, 'o-', linewidth=2, color='green')\n",
    "    ax3.set_xlabel('S lng Fold (K)')\n",
    "    ax3.set_ylabel('Thi gian training (giy)')\n",
    "    ax3.set_title('Thi gian training theo gi tr K')\n",
    "    ax3.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('k_fold_elbow_analysis.png')\n",
    "    plt.show()\n",
    "    \n",
    "    optimal_k = find_elbow_point(k_values, k_scores, k_variances)\n",
    "\n",
    "    \n",
    "    return optimal_k\n",
    "\n",
    "def find_elbow_point(k_values, k_scores, k_variances):\n",
    "    \"\"\"\n",
    "    Xc nh im elbow da trn phn trm ci thin v phng sai\n",
    "    \"\"\"\n",
    "    \n",
    "    improvements = []\n",
    "    for i in range(1, len(k_values)):\n",
    "        improvement = (k_scores[i] - k_scores[i-1]) / k_scores[i-1] * 100\n",
    "        improvements.append(improvement)\n",
    "    \n",
    "    for i in range(len(improvements)):\n",
    "        if improvements[i] < 1.0:\n",
    "            return k_values[i+1]\n",
    "    \n",
    "    var_index = np.argmin(k_variances)\n",
    "    return k_values[var_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5d500d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T14:37:00.147862Z",
     "iopub.status.busy": "2025-06-03T14:37:00.147672Z"
    },
    "papermill": {
     "duration": 7246.332832,
     "end_time": "2025-06-03T16:37:46.475597",
     "exception": false,
     "start_time": "2025-06-03T14:37:00.142765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Total images: 300\n",
      "Total masks: 300\n",
      "  Fold 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/albumentations/core/validation.py:87: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    }
   ],
   "source": [
    "def run_training():\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    \n",
    "    root_dir = Path('/kaggle/input/data-1/content')\n",
    "    all_images = sorted(list((root_dir / 'images').glob('*.bmp')))\n",
    "    all_masks = sorted(list((root_dir / 'masks').glob('*.png')))\n",
    "    \n",
    "    all_images = sorted(all_images, key=lambda x: x.stem)\n",
    "    all_masks = sorted(all_masks, key=lambda x: x.stem)\n",
    "    \n",
    "    print(f\"Total images: {len(all_images)}\")\n",
    "    print(f\"Total masks: {len(all_masks)}\")\n",
    "    \n",
    "    optimal_k = find_optimal_k(\n",
    "        all_images, all_masks, \n",
    "        k_values=[3, 5, 7, 10], \n",
    "        num_epochs=10\n",
    "    )\n",
    "    \n",
    "    kf = KFold(n_splits=optimal_k, shuffle=True, random_state=SEED)\n",
    "    fold_metrics = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(all_images)):\n",
    "        print(f\"Training Fold {fold+1}/{optimal_k}\")\n",
    "\n",
    "        train_img = [all_images[i] for i in train_idx]\n",
    "        train_mask = [all_masks[i] for i in train_idx]\n",
    "        val_img = [all_images[i] for i in val_idx]\n",
    "        val_mask = [all_masks[i] for i in val_idx]\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = SegmentationDataset(train_img, train_mask, get_train_transform())\n",
    "        val_dataset = SegmentationDataset(val_img, val_mask, get_valid_transform())\n",
    "        \n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "        \n",
    "        # Initialize model\n",
    "        model = ElectricForceModel().to(DEVICE)\n",
    "        \n",
    "        # Setup optimizer and loss\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=5, factor=0.5)\n",
    "        \n",
    "        # Train model\n",
    "        model, history, best_metrics = train_model(\n",
    "            model, \n",
    "            train_loader, \n",
    "            val_loader,\n",
    "            optimizer,\n",
    "            criterion,\n",
    "            scheduler,\n",
    "            NUM_EPOCHS,\n",
    "            DEVICE,\n",
    "            fold=fold\n",
    "        )\n",
    "        \n",
    "        # Save metrics\n",
    "        fold_metrics.append(best_metrics)\n",
    "        \n",
    "        # Visualize predictions\n",
    "        visualize_predictions(model, val_loader, DEVICE)\n",
    "        \n",
    "        # Plot learning curves\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(history['train_loss'], label='Train Loss')\n",
    "        plt.plot(history['val_loss'], label='Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.title('Loss Curves')\n",
    "        \n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(history['iou'], label='IoU')\n",
    "        plt.plot(history['dsc'], label='DSC/F1')\n",
    "        plt.legend()\n",
    "        plt.title('IoU and DSC Curves')\n",
    "        \n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(history['precision'], label='Precision')\n",
    "        plt.legend()\n",
    "        plt.title('Precision Curve')\n",
    "        \n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(history['accuracy'], label='Accuracy')\n",
    "        plt.legend()\n",
    "        plt.title('Accuracy Curve')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'learning_curves_fold{fold}.png')\n",
    "        plt.show()\n",
    "    \n",
    "    # Print overall metrics\n",
    "    print(\"\\nAverage metrics across all folds:\")\n",
    "    avg_iou = np.mean([metrics['mean_iou'] for metrics in fold_metrics])\n",
    "    avg_dsc = np.mean([metrics['mean_dsc'] for metrics in fold_metrics])\n",
    "    avg_precision = np.mean([metrics['mean_precision'] for metrics in fold_metrics])\n",
    "    avg_acc = np.mean([metrics['mean_acc'] for metrics in fold_metrics])\n",
    "    \n",
    "    print(f\"Avg IoU: {avg_iou:.4f}\")\n",
    "    print(f\"Avg DSC/F1: {avg_dsc:.4f}\")\n",
    "    print(f\"Avg Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Avg Accuracy: {avg_acc:.4f}\")\n",
    "\n",
    "# Run the training\n",
    "if __name__ == \"__main__\":\n",
    "    run_training()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 11376393,
     "sourceId": 86023,
     "sourceType": "competition"
    },
    {
     "datasetId": 7306477,
     "sourceId": 11643754,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7504.339689,
   "end_time": "2025-06-03T16:37:47.482232",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-03T14:32:43.142543",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
