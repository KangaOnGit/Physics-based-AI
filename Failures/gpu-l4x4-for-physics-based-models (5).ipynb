{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86023,"databundleVersionId":11376393,"sourceType":"competition"},{"sourceId":11643754,"sourceType":"datasetVersion","datasetId":7306477},{"sourceId":11681844,"sourceType":"datasetVersion","datasetId":7331802}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport polars as pl\nimport kaggle_evaluation.aimo_2_inference_server","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T10:46:29.999239Z","iopub.execute_input":"2025-06-04T10:46:29.999843Z","iopub.status.idle":"2025-06-04T10:46:31.275813Z","shell.execute_reply.started":"2025-06-04T10:46:29.999820Z","shell.execute_reply":"2025-06-04T10:46:31.275269Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"!pip install albumentations\n!pip install segmentation-models-pytorch","metadata":{"execution":{"iopub.status.busy":"2025-06-04T09:59:42.466296Z","iopub.execute_input":"2025-06-04T09:59:42.466570Z","iopub.status.idle":"2025-06-04T10:03:07.525025Z","shell.execute_reply.started":"2025-06-04T09:59:42.466554Z","shell.execute_reply":"2025-06-04T10:03:07.524377Z"},"scrolled":true}},{"cell_type":"code","source":"# Standard library\nimport os\nimport gc\nimport time\nimport math\nimport random\nimport logging\nimport warnings\nlogging.basicConfig(level=logging.ERROR)\n\n# Scientific computing\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\n\n# Deep learning (PyTorch)\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.parametrizations import weight_norm\nfrom sklearn.metrics import pairwise_distances\n\n# Computer vision\nimport cv2\nimport torchvision\nfrom PIL import Image\nfrom torchvision import transforms\n\n# Miscellaneous utilities\nfrom tqdm import tqdm\nimport timm\nimport einops\nfrom einops import rearrange\nfrom sklearn.model_selection import KFold\nfrom pathlib import Path\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport kagglehub","metadata":{"execution":{"iopub.status.busy":"2025-06-04T10:46:31.276757Z","iopub.execute_input":"2025-06-04T10:46:31.277051Z","iopub.status.idle":"2025-06-04T10:47:16.996775Z","shell.execute_reply.started":"2025-06-04T10:46:31.277034Z","shell.execute_reply":"2025-06-04T10:47:16.996222Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/albumentations/check_version.py:107: UserWarning: Error fetching version info <urlopen error [Errno -3] Temporary failure in name resolution>\n  data = fetch_version_info()\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"#DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nDEVICE = torch.device('cpu')\nNUM_EPOCHS = 5\nBATCH_SIZE = 1\nLEARNING_RATE = 0.001\nIMAGE_SIZE = 128\nNUM_CLASSES = 3\nSEED = 42\nN_FOLDS = 5","metadata":{"execution":{"iopub.status.busy":"2025-06-04T10:47:16.997358Z","iopub.execute_input":"2025-06-04T10:47:16.997683Z","iopub.status.idle":"2025-06-04T10:47:17.000862Z","shell.execute_reply.started":"2025-06-04T10:47:16.997667Z","shell.execute_reply":"2025-06-04T10:47:17.000370Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T10:47:17.002124Z","iopub.execute_input":"2025-06-04T10:47:17.002298Z","iopub.status.idle":"2025-06-04T10:47:17.014135Z","shell.execute_reply.started":"2025-06-04T10:47:17.002284Z","shell.execute_reply":"2025-06-04T10:47:17.013630Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"path = kagglehub.dataset_download(\"khanhpt1999/data-1\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"execution":{"iopub.status.busy":"2025-06-04T10:47:17.014607Z","iopub.execute_input":"2025-06-04T10:47:17.014761Z","iopub.status.idle":"2025-06-04T10:47:17.153748Z","shell.execute_reply.started":"2025-06-04T10:47:17.014748Z","shell.execute_reply":"2025-06-04T10:47:17.153269Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/data-1\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"img = Image.open(\"/kaggle/input/test-images/image_2025-05-05_142003010.png\").convert(\"RGB\")\nprint(\"PIL image size:\", img.size, \"mode:\", img.mode)\n\ntransform = transforms.Compose([\n    transforms.Resize((128, 128)),                 \n    transforms.ToTensor(),                \n    transforms.Normalize( \n        mean=[0.5,0.5,0.5],\n        std=[0.5,0.5,0.5]\n    ),\n])\n\ninpt: torch.Tensor = transform(img)\nprint(\"Tensor shape:\", inpt.shape, \"dtype:\", inpt.dtype, \"range:\", inpt.min(),\"→\",inpt.max())","metadata":{"execution":{"iopub.status.busy":"2025-06-04T10:47:17.154315Z","iopub.execute_input":"2025-06-04T10:47:17.154535Z","iopub.status.idle":"2025-06-04T10:47:17.270074Z","shell.execute_reply.started":"2025-06-04T10:47:17.154521Z","shell.execute_reply":"2025-06-04T10:47:17.269522Z"}}},{"cell_type":"markdown","source":"inpt = inpt.unsqueeze(0)\nprint(f\"Shape of Input: {inpt.shape}\")","metadata":{"execution":{"iopub.status.busy":"2025-06-04T10:47:17.270600Z","iopub.execute_input":"2025-06-04T10:47:17.270768Z","iopub.status.idle":"2025-06-04T10:47:17.273944Z","shell.execute_reply.started":"2025-06-04T10:47:17.270755Z","shell.execute_reply":"2025-06-04T10:47:17.273447Z"}}},{"cell_type":"code","source":"class MultiHeadSelfAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads=8, dropout=0.1, img_size=64):  # Added img_size parameter\n        super().__init__()\n        assert embed_dim % num_heads == 0\n        \n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        self.img_size = img_size  # Store image size\n        \n        # Linear projections for Q, K, V\n        self.qkv = nn.Linear(embed_dim, embed_dim * 3, bias=False)\n        self.proj = nn.Linear(embed_dim, embed_dim)\n        self.dropout = nn.Dropout(dropout)\n        \n        # FIXED: Relative position embeddings for actual image size\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros((2 * img_size - 1) * (2 * img_size - 1), num_heads)\n        )\n        \n        # FIXED: Initialize relative position bias for actual image size\n        coords_h = torch.arange(img_size)\n        coords_w = torch.arange(img_size)\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing='ij'))\n        coords_flatten = torch.flatten(coords, 1)\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n        relative_coords[:, :, 0] += img_size - 1\n        relative_coords[:, :, 1] += img_size - 1\n        relative_coords[:, :, 0] *= 2 * img_size - 1\n        relative_position_index = relative_coords.sum(-1)\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n        \n    def forward(self, x):\n        B, C, H, W = x.shape\n        N = H * W\n        \n        # Reshape to sequence format [B, N, C]\n        x = x.reshape(B, C, N).transpose(1, 2)\n        \n        # Generate Q, K, V\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        \n        # Scaled dot-product attention\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        \n        # Add relative position bias (only if image size matches)\n        if H == self.img_size and W == self.img_size:\n            relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].reshape(\n                H * W, H * W, -1)\n            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n            attn = attn + relative_position_bias.unsqueeze(0)\n        \n        attn = F.softmax(attn, dim=-1)\n        attn = self.dropout(attn)\n        \n        # Apply attention to values\n        x = (attn @ v).transpose(1, 2).reshape(B, N, self.embed_dim)\n        x = self.proj(x)\n        \n        # Reshape back to spatial format [B, C, H, W]\n        x = x.transpose(1, 2).view(B, C, H, W)\n        \n        return x\n\nclass SelfAttentionConv(nn.Module):\n    \"\"\"Self-attention layer that can replace convolutions\"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, num_heads=8):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.num_heads = num_heads\n        \n        # If channels change, use 1x1 conv for projection\n        if in_channels != out_channels:\n            self.channel_proj = nn.Conv2d(in_channels, out_channels, 1, 1, 0)\n        else:\n            self.channel_proj = nn.Identity()\n            \n        # Self-attention components\n        self.attention = MultiHeadSelfAttention(out_channels, num_heads)\n        \n        # Learnable scale parameter for gradual integration\n        self.gamma = nn.Parameter(torch.zeros(1))\n        \n    def forward(self, x):\n        # Project channels if needed\n        identity = self.channel_proj(x)\n        \n        # Apply self-attention\n        attended = self.attention(identity)\n        \n        # Gradual integration with learnable scale\n        out = self.gamma * attended + identity\n        \n        # Handle stride if needed\n        if self.stride > 1:\n            out = F.avg_pool2d(out, kernel_size=self.stride, stride=self.stride)\n            \n        return out","metadata":{"execution":{"iopub.status.busy":"2025-06-04T10:47:17.274508Z","iopub.execute_input":"2025-06-04T10:47:17.274938Z","iopub.status.idle":"2025-06-04T10:47:17.287871Z","shell.execute_reply.started":"2025-06-04T10:47:17.274918Z","shell.execute_reply":"2025-06-04T10:47:17.287400Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class ElectricForceModel(nn.Module):\n\n    def __init__(\n        self,\n        imsize = 128,\n        v0 = 0,\n        layers = 3,\n        loops = 3,\n        debug = False,\n        beta = 0.8,\n        use_self_attention = False\n    ):\n        super().__init__()\n\n        self.register_buffer('v0', torch.tensor([v0], dtype=torch.float32))\n        self.register_buffer('beta', torch.tensor([beta], dtype=torch.float32))\n\n        self.adaptive_epsilon = nn.Parameter(torch.tensor([1.0]))\n        self.adaptive_mass = nn.Parameter(torch.tensor([1.0]))\n        self.adaptive_time = nn.Parameter(torch.tensor([0.1]))\n        \n        with torch.no_grad():\n            self.adaptive_epsilon.data.fill_(1.0)\n            self.adaptive_mass.data.fill_(1.0)\n            self.adaptive_time.data.fill_(0.1)\n\n        self.imsize = imsize\n        self.layers = layers\n        self.loops = loops\n        self.debug = debug\n        self.use_self_attention = use_self_attention\n\n        if self.use_self_attention:\n            self.input_attention = MultiHeadSelfAttention(3, num_heads=3, img_size=imsize)\n            self.attention_scale = nn.Parameter(torch.zeros(1))\n        \n        if self.use_self_attention:\n            self.mapping_to_vector_space = SelfAttentionConv(3, 2, num_heads=2)\n            self.mapping_to_ipt = SelfAttentionConv(2, 3, num_heads=3)\n        else:\n            self.mapping_to_vector_space = nn.Conv2d(3, 2, 1, 1, 0)\n            self.mapping_to_ipt = nn.Conv2d(2, 3, 1, 1, 0)\n            \n        self.space_norm = nn.Tanh()\n        \n\n    def compute_distance(self, ipt, chunk_size=512):\n        \n        \"\"\"\n        Compute the distance of each point with respect to every point in zeta\n\n        Input:\n            - ipt: Input Image | [B, 3, self.imsize, self.imsize]\n\n        Outpt:\n            - zeta: The coordinates map for each pixel in image (x, y) | [B, 2, self.imsize, self.imsize]\n            - x: x-coordinates of every point in zeta | [B, N]\n            - y: y-coordinates of every point in zeta | [B, N]\n            - inverse_sq_dist: 1/r^2, the inverse squared distance of each point with respect to every point in zeta | [B, N, N]\n            - inverse_dist: 1/r, the inverse distance of each point with respect to every point in zeta | [B, N, N]\n            \n        How the Function Works:\n            - Find zeta using a point-wise-convolutional-layer, the number of channel output will be the number of dimensions in vector space\n                            (2 -> (x, y) == Vector Space\n                             3 -> (x, y, z) == 3D Space\n                             ...\n                             ...\n                            )\n            - Find x and y by slicing zeta along the 0th and 1st of the 1st Dimension\n            - Calculate the Euclidean Distance between each point in zeta\n            - inverse_sq_dist is D**(-2) and inverse_dist is D**(-1)\n        \"\"\"\n                \n        zeta = self.mapping_to_vector_space(ipt)\n        zeta = self.space_norm(zeta) # [B, 2, self.imsize, self.imsize]\n        if self.debug:\n            print(f\"zeta Shape: {zeta.shape}\")\n\n        B, C, H, W = zeta.shape\n        N = H*W\n\n        pts = zeta.reshape(B, 2, N) # [B, 2, N]\n        if self.debug:\n            print(f\"pts Shape: {pts.shape}\")\n\n        x, y = pts[:, 0], pts[:, 1] # [B, N], [B, N]\n        if self.debug:\n            print(f\"x Shape: {x.shape}\")\n            print(f\"y Shape: {y.shape}\")\n\n        zflat = pts.permute(0, 2, 1) # [B, N, 2]\n        if self.debug:\n            print(f\"zflat Shape: {zflat.shape}\")\n\n        if N <= chunk_size:\n            D = torch.cdist(zflat, zflat, p=2) # [B, N, N]\n            D = torch.where(D==0, float('inf'), D) # [B, N, N]\n        else:\n            if self.debug:\n                print(f\"Using chunking with chunk_size={chunk_size} for N={N}\")\n        \n            D = torch.zeros(B, N, N, device=zflat.device, dtype=zflat.dtype) # [B, N, N]\n            \n            for i in range(0, N, chunk_size):\n                end_i = min(i + chunk_size, N)\n                chunk_points = zflat[:, i:end_i, :] # [B, chunk_size, 2]\n                distances_chunk = torch.cdist(chunk_points, zflat, p=2) # [B, chunk_size, N]\n\n                diag_mask = torch.zeros_like(distances_chunk, dtype=torch.bool)\n                for idx in range(end_i - i):\n                    diag_mask[:, idx, i + idx] = True\n                    \n                distances_chunk = torch.where(diag_mask, float('inf'), distances_chunk)\n                D[:, i:end_i, :] = distances_chunk # [B, chunk_size, N]\n\n        D = torch.where(D == 0, float('inf'), D)\n        if self.debug:\n            print(f\"D Shape: {D.shape}\") # [B, N, N]\n            \n        #inverse_sq_dist = D.pow(-2) # [B, N, N]\n        #inverse_dist = D.pow(-1) # [B, N, N]\n        #if self.debug:\n            #print(f\"inverse_sq_dist Shape: {inverse_sq_dist.shape}\") # [B, N, N]\n            #print(f\"inverse_dist Shape: {inverse_dist.shape}\") # [B, N, N]\n            \n        if self.debug:\n            print(\"-\"*59)\n        \n        return zeta, x, y, D #inverse_sq_dist, #inverse_dist\n\n    def compute_angle(self, x, y, chunk_size=512):\n        \n        \"\"\"\n        Compute angle of forces acted on each point with respect to the x-axis\n        Input: \n            - x: x-coordinates of every point in zeta | [B, N]\n            - y: y-coordinates of every point in zeta | [B, N]\n\n        Output:\n            - theta_rad: Radian of angle of forces with respect to the x-axis | [B, N, N]\n\n        How the Function Works:\n            - Applying Inverse Function of tan to find the angle | tan = dy/dx\n        \"\"\"\n        \n        B, N = x.shape\n        \n        if self.debug:\n            print(f\"Running compute_angle...\")\n            print(f\" \")\n            \n        if N <= chunk_size:\n            theta_rad = torch.atan2( # [B, N, N]\n                y.unsqueeze(2) - y.unsqueeze(1), # [B, N, 1] - [B, 1, N] -> [B, N, N]\n                x.unsqueeze(2) - x.unsqueeze(1), # [B, N, 1] - [B, 1, N] -> [B, N, N]\n            )\n        else:\n            if self.debug:\n                print(f\"Using chunking for angle computation with chunk_size={chunk_size} for N={N}\")\n            \n            theta_rad = torch.zeros(B, N, N, device=x.device, dtype=x.dtype) # [B, N, N]\n            \n            for i in range(0, N, chunk_size):\n                end_i = min(i + chunk_size, N)\n                \n                x_chunk = x[:, i:end_i] # [B, chunk_size]\n                y_chunk = y[:, i:end_i] # [B, chunk_size]\n                \n                dy = y_chunk.unsqueeze(2) - y.unsqueeze(1) # [B, chunk_size, 1] - [B, 1, N] -> [B, chunk_size, N]\n                dx = x_chunk.unsqueeze(2) - x.unsqueeze(1) # [B, chunk_size, 1] - [B, 1, N] -> [B, chunk_size, N]\n                \n                theta_chunk = torch.atan2(dy, dx) # [B, chunk_size, N]\n                theta_rad[:, i:end_i, :] = theta_chunk # [B, chunk_size, N]\n                \n        theta_deg = theta_rad * (180.0 / math.pi) # [B, N, N]\n        if self.debug:\n            print(f\"theta_rad Shape: {theta_rad.shape}\")\n            print(f\"theta_deg Shape: {theta_deg.shape}\")\n            \n        if self.debug:\n            print(\"-\"*59)\n        return theta_rad\n\n    def compute_electric_force(self, ipt, theta_rad, inverse_sq_dist, chunk_size = 512):\n        \n        \"\"\"\n        Compute the Net Electric Force acted on each point\n        \n        Input:\n            - ipt: Input Image | [B, 3, self.imsize, self.imsize]\n            - theta_rad: The angle of forces acted on each point with respect to the x-axis | [B, N, N]\n            - inverse_sq_dist: 1/r^2, the inverse squared distance of each point with respect to every point in zeta | [B, N, N]\n    \n        Output:\n            - Fx: Force acted on each point in the x-axis | [B, N]\n            - Fy: Force acted on each point in the y-axis | [B, N]\n    \n        How the Function Works:\n            - F_E = qE = qi (k_E sum(qj/r^2))\n                + Since all variables are vectors, we split them into x and y axis, respectively\n        \"\"\"\n        \n        k_E = 1.0 / (4.0 * math.pi *torch.abs(self.adaptive_epsilon))\n        \n        B, C, H, W = ipt.shape\n        N = H*W\n        \n        ipt_reshaped = ipt.reshape(B, N, C) # [B, N, C]\n        \n        if self.debug:\n            print(f\"Running compute_electric_force...\")\n            print(\" \")\n    \n        if N <= chunk_size:\n            # Small enough - use original approach\n            qi_qj = ipt_reshaped.unsqueeze(2) * ipt_reshaped.unsqueeze(1) # [B, N, 1, C] * [B, 1, N, C] -> [B, N, N, C]\n            qi_qj = torch.mean(qi_qj, dim = 3) # [B, N, N]\n            if self.debug:\n                print(f\"qi_qj Shape: {qi_qj.shape}\") # [B, N, N]\n    \n            Fx = k_E * qi_qj * inverse_sq_dist * torch.cos(theta_rad) # [B, N, N]\n            Fy = k_E * qi_qj * inverse_sq_dist * torch.sin(theta_rad) # [B, N, N]\n    \n            Fx = torch.sum(Fx, dim = 2) # [B, N]\n            Fy = torch.sum(Fy, dim = 2) # [B, N]\n        else:\n            # CHUNKED BROADCASTING - avoid creating large [B, N, N, C] tensor\n            if self.debug:\n                print(f\"Using chunked broadcasting for force computation with chunk_size={chunk_size} for N={N}\")\n            \n            Fx = torch.zeros(B, N, device=ipt.device, dtype=ipt.dtype) # [B, N]\n            Fy = torch.zeros(B, N, device=ipt.device, dtype=ipt.dtype) # [B, N]\n            \n            for i in range(0, N, chunk_size):\n                end_i = min(i + chunk_size, N)\n                \n                # Get chunk of qi values\n                qi_chunk = ipt_reshaped[:, i:end_i, :] # [B, chunk_size, C]\n                \n                # Initialize qi_qj chunk for this row chunk\n                qi_qj_chunk = torch.zeros(B, end_i - i, N, device=ipt.device, dtype=ipt.dtype) # [B, chunk_size, N]\n                \n                # CHUNKED COMPUTATION of qi_qj to avoid large broadcasting\n                for j in range(0, N, chunk_size):\n                    end_j = min(j + chunk_size, N)\n                    \n                    # Get chunk of qj values  \n                    qj_chunk = ipt_reshaped[:, j:end_j, :] # [B, chunk_size_j, C]\n                    \n                    # Small broadcasting: [B, chunk_i, 1, C] * [B, 1, chunk_j, C] -> [B, chunk_i, chunk_j, C]\n                    qi_expanded = qi_chunk.unsqueeze(2) # [B, chunk_i, 1, C]\n                    qj_expanded = qj_chunk.unsqueeze(1) # [B, 1, chunk_j, C]\n                    qi_qj_small = qi_expanded * qj_expanded # [B, chunk_i, chunk_j, C]\n                    qi_qj_small = torch.mean(qi_qj_small, dim=3) # [B, chunk_i, chunk_j]\n                    \n                    # Store in the chunk\n                    qi_qj_chunk[:, :, j:end_j] = qi_qj_small # [B, chunk_i, chunk_j]\n                    \n                    # Cleanup small tensors\n                    del qi_expanded, qj_expanded, qi_qj_small\n                \n                # Get corresponding chunks for force computation\n                inverse_sq_dist_chunk = inverse_sq_dist[:, i:end_i, :] # [B, chunk_size, N]\n                theta_rad_chunk = theta_rad[:, i:end_i, :] # [B, chunk_size, N]\n                \n                # Compute forces for this chunk\n                Fx_chunk = k_E * qi_qj_chunk * inverse_sq_dist_chunk * torch.cos(theta_rad_chunk) # [B, chunk_size, N]\n                Fy_chunk = k_E * qi_qj_chunk * inverse_sq_dist_chunk * torch.sin(theta_rad_chunk) # [B, chunk_size, N]\n                \n                # Sum across interactions\n                Fx[:, i:end_i] = torch.sum(Fx_chunk, dim=2) # [B, chunk_size]\n                Fy[:, i:end_i] = torch.sum(Fy_chunk, dim=2) # [B, chunk_size]\n                \n                # Cleanup chunk tensors\n                del qi_chunk, qi_qj_chunk, inverse_sq_dist_chunk, theta_rad_chunk, Fx_chunk, Fy_chunk\n        \n        if self.debug:\n            print(f\"Fx Shape: {Fx.shape}\") # [B, N]\n            print(f\"Fy Shape: {Fy.shape}\") # [B, N]\n            print(\"-\"*59)\n            \n        return Fx, Fy\n\n    def compute_vector_translation(self, Fx, Fy, zeta):\n        \"\"\"\n        Move each point in the vector space corresponding to forces acted on it.\n\n        Input:\n            - Fx: Force acted on each point in the x-axis | [B, N]\n            - Fy: Force acted on each point in the y-axis | [B, N]\n            - zeta: Coordinates of each point (x, y) | [B, 2, self.imsize, self.imsize]\n\n        Output:\n            - new_zeta: Fully updated Coordinates of each point (x, y) | [B, 2, self.imsize, self.imsize]\n\n        How the Function Works:\n            - Displacement = Velocity x time + Initial Position -> D =  vt + x0\n            - v = Acceleration x time + Initial Velocity = at + v0\n            - Acceleration = Force/Mass = F/m\n                + Since all variables are vectors, we split them into x and y axis, respectively\n        \"\"\"\n        \n        if self.debug:\n            print(f\"Starting compute_vector_translation...\")\n            print(\" \")\n            \n        ax = Fx / torch.abs(self.adaptive_mass) # [B, N]\n        ay = Fy / torch.abs(self.adaptive_mass) # [B, N]\n        if self.debug:\n            print(f\"ax Shape: {ax.shape}\")\n            print(f\"ay Shape: {ay.shape}\")\n            \n        B, N = Fx.shape\n        v0 = self.v0.expand(B, N) # [B, N]\n        \n        if self.debug:\n            print(f\"v0 Shape: {v0.shape}\")\n            \n        time_step = torch.abs(self.adaptive_time)\n        vx = v0 + ax * time_step # [B, N]\n        vy = v0 + ay * time_step # [B, N]\n        vx = vx.reshape(B, self.imsize, self.imsize) # [B, self.imsize, self.imsize]\n        vy = vy.reshape(B, self.imsize, self.imsize) # [B, self.imsize, self.imsize]\n        if self.debug:\n            print(f\"vx Shape: {vx.shape}\")\n            print(f\"vy Shape: {vy.shape}\")\n            \n        if self.debug:\n            print(f\"zeta Shape: {zeta.shape}\")\n            print(f\"zeta[:, 0] Shape: {zeta[:, 0].shape}\")\n            print(f\"zeta[:, 1] Shape: {zeta[:, 1].shape}\")\n        new_zeta = zeta.clone()\n        new_zeta[:, 0] = zeta[:, 0] + vx # [B, 2, self.imsize, self.imsize]\n        new_zeta[:, 1] = zeta[:, 1] + vy # [B, 2, self.imsize, self.imsize]\n\n        \n        if self.debug:\n            print(f\"Updated zeta Shape: {new_zeta.shape}\")\n            print(f\"-\"*59)\n            \n        return new_zeta\n\n    def update_input(self, ipt, zeta):\n        \n        \"\"\"\n        Update Input by Unmapping zeta\n\n        Input:\n            - ipt: Input Image | [B, 3, self.imsize, self.imsize]\n            - zeta: Coordinates of each point (x, y) | [B, 2, self.imsize, self.imsize]\n\n        Output:\n            - new_ipt: New Input Image | [B, 3, self.imsize, self.imsize]\n\n        How the Function Works:\n            - You got the new zeta Coordinates\n            - You unmap it using a point-wise-convolutional-layer\n            - New Input will be the summation of the unmapped zeta and the Input Image\n        \"\"\"\n        if self.debug:\n            print(f\"Running update_input...\")\n            \n        map_to_ipt = self.mapping_to_ipt(zeta) # [B, 3, self.imsize, self.imsize]\n        new_ipt = map_to_ipt*self.beta + (1/self.loops)*ipt*(1-self.beta) # [B, 3, self.imsize, self.imsize]\n        return new_ipt\n\n    def compute_energy_of_system(self, ipt, inverse_dist, chunk_size = 512):\n    \n        \"\"\"\n        Compute total Energy of the System\n    \n        Input:\n            - ipt: Final Input Image | [B, 3, self.imsize, self.imsize]\n            - inverse_dist: 1/r, the inverse distance of each point with respect to every point in zeta | [B, N, N]\n        \n        Output:\n            - V: Total Energy of Final Input Image | [B]\n    \n        How the Function Works:\n            - V = sum(qi * Vj) = qi (k_E sum(qj/r^2))\n        \"\"\"\n        \n        k_E = 1.0 / (4.0 * math.pi * torch.abs(self.adaptive_epsilon))\n        \n        B, C, H, W = ipt.shape\n        N = H*W\n        \n        ipt_reshaped = ipt.reshape(B, N, C) # [B, N, C]\n        \n        if self.debug:\n            print(f\"Running compute_energy_of_system...\")\n            print(\" \")\n    \n        if N <= chunk_size:\n            # Small enough - use original approach\n            qi_qj = ipt_reshaped.unsqueeze(2) * ipt_reshaped.unsqueeze(1) # [B, N, 1, C] * [B, 1, N, C] -> [B, N, N, C]\n            qi_qj = torch.mean(qi_qj, dim = 3) # [B, N, N]\n            if self.debug:\n                print(f\"qi_qj Shape: {qi_qj.shape}\") # [B, N, N]\n    \n            V = k_E * qi_qj * inverse_dist # [B, N, N]\n            V = torch.sum(V, dim = 2) # [B, N]\n            V = torch.sum(V, dim = 1) # [B]\n        else:\n            # CHUNKED BROADCASTING - avoid creating large [B, N, N, C] tensor\n            if self.debug:\n                print(f\"Using chunked broadcasting for energy computation with chunk_size={chunk_size} for N={N}\")\n            \n            V = torch.zeros(B, device=ipt.device, dtype=ipt.dtype) # [B]\n            \n            for i in range(0, N, chunk_size):\n                end_i = min(i + chunk_size, N)\n                \n                # Get chunk of qi values\n                qi_chunk = ipt_reshaped[:, i:end_i, :] # [B, chunk_size, C]\n                \n                # Initialize qi_qj chunk for this row chunk\n                qi_qj_chunk = torch.zeros(B, end_i - i, N, device=ipt.device, dtype=ipt.dtype) # [B, chunk_size, N]\n                \n                # CHUNKED COMPUTATION of qi_qj to avoid large broadcasting\n                for j in range(0, N, chunk_size):\n                    end_j = min(j + chunk_size, N)\n                    \n                    # Get chunk of qj values\n                    qj_chunk = ipt_reshaped[:, j:end_j, :] # [B, chunk_size_j, C]\n                    \n                    # Small broadcasting: [B, chunk_i, 1, C] * [B, 1, chunk_j, C] -> [B, chunk_i, chunk_j, C]\n                    qi_expanded = qi_chunk.unsqueeze(2) # [B, chunk_i, 1, C]\n                    qj_expanded = qj_chunk.unsqueeze(1) # [B, 1, chunk_j, C]\n                    qi_qj_small = qi_expanded * qj_expanded # [B, chunk_i, chunk_j, C]\n                    qi_qj_small = torch.mean(qi_qj_small, dim=3) # [B, chunk_i, chunk_j]\n                    \n                    # Store in the chunk\n                    qi_qj_chunk[:, :, j:end_j] = qi_qj_small # [B, chunk_i, chunk_j]\n                    \n                    # Cleanup small tensors\n                    del qi_expanded, qj_expanded, qi_qj_small\n                \n                # Get corresponding distance chunk\n                inverse_dist_chunk = inverse_dist[:, i:end_i, :] # [B, chunk_size, N]\n                \n                # Compute energy for this chunk\n                V_chunk = k_E * qi_qj_chunk * inverse_dist_chunk # [B, chunk_size, N]\n                V_chunk = torch.sum(V_chunk, dim=2) # [B, chunk_size]\n                V_chunk = torch.sum(V_chunk, dim=1) # [B]\n                \n                # Accumulate energy\n                V += V_chunk # [B]\n                \n                # Cleanup chunk tensors\n                del qi_chunk, qi_qj_chunk, inverse_dist_chunk, V_chunk\n                \n        if self.debug:\n            print(f\"V Shape: {V.shape}\") # [B]\n            print(\"-\"*59)\n            \n        return V\n\n    def aggregate_to_one_function(self, ipt, chunk_size):\n    \n        k_E = 1.0 / (4.0 * math.pi * torch.abs(self.adaptive_epsilon))\n        epsilon_dist_sq = 1e-12 # For squared distances to avoid division by zero\n    \n        if self.debug:\n            print(f\"Running aggregate_to_one_function (ULTRA EFFICIENT)\")\n            print(f\"  ipt Shape: {ipt.shape}\")\n    \n        # 1. Compute zeta (spatial coordinates)\n        # These are computed once and reused. Their memory is relatively small.\n        zeta_raw = self.mapping_to_vector_space(ipt)\n        zeta = self.space_norm(zeta_raw)  # [B, 2, H, W]\n        B, C_zeta, H, W = zeta.shape\n        N = H * W\n    \n        if self.debug: print(f\"  zeta Shape: {zeta.shape}\")\n    \n        # Global accumulators for total forces and system potential\n        Fx_total = torch.zeros(B, N, device=ipt.device, dtype=ipt.dtype)\n        Fy_total = torch.zeros(B, N, device=ipt.device, dtype=ipt.dtype)\n        #V_total_system = torch.zeros(B, device=ipt.device, dtype=ipt.dtype) # Scalar potential for the whole system\n    \n        # Flattened coordinates and charges (properties from ipt)\n        # These are [B, N, Dims]\n        pts_zeta_coords_all = zeta.reshape(B, 2, N)\n        x_coords_all = pts_zeta_coords_all[:, 0, :] # [B, N]\n        y_coords_all = pts_zeta_coords_all[:, 1, :] # [B, N]\n        zflat_for_cdist_all = pts_zeta_coords_all.permute(0, 2, 1) # [B, N, 2] (for torch.cdist)\n    \n        B_ipt, C_ipt, H_ipt, W_ipt = ipt.shape\n        ipt_flat_charges_all = ipt.reshape(B_ipt, C_ipt, N).permute(0, 2, 1) # [B, N, C_ipt]\n    \n        # Outer loop: Iterate over TARGET chunks (points experiencing the force)\n        for i_start in range(0, N, chunk_size):\n            i_end = min(i_start + chunk_size, N)\n            current_i_chunk_size = i_end - i_start\n    \n            if self.debug: print(f\"  Processing TARGET chunk: points {i_start} to {i_end-1}\")\n    \n            # Data for current TARGET chunk_i (small tensors)\n            target_coords_i_cdist = zflat_for_cdist_all[:, i_start:i_end, :]     # [B, current_i_chunk_size, 2]\n            target_x_i = x_coords_all[:, i_start:i_end]                         # [B, current_i_chunk_size]\n            target_y_i = y_coords_all[:, i_start:i_end]                         # [B, current_i_chunk_size]\n            target_charges_qi = ipt_flat_charges_all[:, i_start:i_end, :]       # [B, current_i_chunk_size, C_ipt]\n    \n            # Accumulators for forces/potential ON this i-th target chunk\n            Fx_on_i_accum = torch.zeros(B, current_i_chunk_size, device=ipt.device, dtype=ipt.dtype)\n            Fy_on_i_accum = torch.zeros(B, current_i_chunk_size, device=ipt.device, dtype=ipt.dtype)\n            #V_on_i_accum = torch.zeros(B, current_i_chunk_size, device=ipt.device, dtype=ipt.dtype) # Potential energy for each point in i-chunk\n    \n            # Inner loop: Iterate over SOURCE chunks (points exerting the force)\n            for j_start in range(0, N, chunk_size):\n                j_end = min(j_start + chunk_size, N)\n                current_j_chunk_size = j_end - j_start\n    \n                # Data for current SOURCE chunk_j (small tensors)\n                source_coords_j_cdist = zflat_for_cdist_all[:, j_start:j_end, :]   # [B, current_j_chunk_size, 2]\n                source_x_j = x_coords_all[:, j_start:j_end]                       # [B, current_j_chunk_size]\n                source_y_j = y_coords_all[:, j_start:j_end]                       # [B, current_j_chunk_size]\n                source_charges_qj = ipt_flat_charges_all[:, j_start:j_end, :]     # [B, current_j_chunk_size, C_ipt]\n    \n                # --- Compute interactions between current chunk_i and current chunk_j ---\n                # All tensors created here are [B, current_i_chunk_size, current_j_chunk_size]\n    \n                # A. Distances (r_ij)\n                # distances_ij: [B, current_i_chunk_size, current_j_chunk_size]\n                distances_ij = torch.cdist(target_coords_i_cdist, source_coords_j_cdist, p=2)\n    \n                # B. Angles (theta_ij of vector from j to i)\n                # dx_ji = x_i - x_j ; dy_ji = y_i - y_j\n                dx_ji_sub = target_x_i.unsqueeze(2) - source_x_j.unsqueeze(1)\n                dy_ji_sub = target_y_i.unsqueeze(2) - source_y_j.unsqueeze(1)\n                theta_ji_sub = torch.atan2(dy_ji_sub, dx_ji_sub) # Angle of vector R_ji (from source j to target i)\n    \n                # C. Charge interaction term (qi_qj_effective)\n                qi_expanded = target_charges_qi.unsqueeze(2)             # [B, size_i, 1, C_ipt]\n                qj_expanded = source_charges_qj.unsqueeze(1)             # [B, 1, size_j, C_ipt]\n                qi_qj_channels = qi_expanded * qj_expanded               # [B, size_i, size_j, C_ipt]\n                qi_qj_scalar_sub = torch.mean(qi_qj_channels, dim=3)   # [B, size_i, size_j]\n    \n                # D. Handle self-interaction if target and source chunks overlap\n                if i_start == j_start: # Only diagonal sub-chunks can have self-interaction\n                    diag_sub_mask = torch.eye(current_i_chunk_size, current_j_chunk_size,\n                                              device=ipt.device, dtype=torch.bool)\n                    # Ensure broadcasting if one chunk is smaller at the edge\n                    min_dim_self = min(current_i_chunk_size, current_j_chunk_size)\n                    diag_sub_mask_final = diag_sub_mask[:min_dim_self, :min_dim_self]\n\n                    distances_ij_corrected = distances_ij.clone()\n                    distances_ij_corrected[:, :min_dim_self, :min_dim_self] = torch.where(\n                        diag_sub_mask_final,\n                        float('inf'),\n                        distances_ij_corrected[:, :min_dim_self, :min_dim_self]\n                    )\n                    distances_ij = distances_ij_corrected\n    \n    \n                # E. Potential Energy V_ij = k_E * (qi_qj)_ij / r_ij\n                \n                #inv_dist_ij = torch.pow(distances_ij, -1) # Handles inf correctly -> 0.0\n                #V_ij_sub_values = k_E * qi_qj_scalar_sub * inv_dist_ij\n    \n                # F. Force Components F_ij = (k_E * (qi_qj)_ij / r_ij^2) * unit_vector_ji\n                # F_radial_signed = k_E * qi_qj_scalar_sub * (1/r_ij^2)\n                # (1/r_ij^2) can be inv_dist_ij.pow(2) or 1.0 / (distances_ij.pow(2) + epsilon_dist_sq)\n                distances_ij_inf = torch.where(distances_ij == 0, float('inf'), distances_ij)\n                inv_sq_dist_ij = torch.pow(distances_ij_inf, -2) # Add epsilon for stability if r can be 0 for non-self\n                #inv_sq_dist_ij = torch.where(torch.isinf(distances_ij), torch.zeros_like(inv_sq_dist_ij), inv_sq_dist_ij) # Ensure inf dist -> 0 force\n    \n                F_radial_signed_ij = k_E * qi_qj_scalar_sub * inv_sq_dist_ij\n    \n                Fx_ij_sub = F_radial_signed_ij * torch.cos(theta_ji_sub)\n                Fy_ij_sub = F_radial_signed_ij * torch.sin(theta_ji_sub)\n    \n                # G. Accumulate forces and potential for the current target_i_chunk\n                normalization_factor = torch.pow(torch.tensor([N], dtype=torch.float32, device=ipt.device), 1/2)\n                Fx_on_i_accum = torch.sum(Fx_ij_sub, dim=2) / normalization_factor\n                Fy_on_i_accum = torch.sum(Fy_ij_sub, dim=2) / normalization_factor\n\n                Fx_on_i_accum = self.space_norm(Fx_on_i_accum)\n                Fy_on_i_accum = self.space_norm(Fy_on_i_accum)\n                #V_on_i_accum += torch.sum(V_ij_sub_values, dim=2) # Sum potential contributions for each point in i_chunk\n    \n                # H. Explicitly delete intermediate sub-chunk tensors\n                del distances_ij, dx_ji_sub, dy_ji_sub, theta_ji_sub, qi_expanded, qj_expanded\n                del qi_qj_channels, qi_qj_scalar_sub, #inv_dist_ij, V_ij_sub_values\n                del inv_sq_dist_ij, F_radial_signed_ij, Fx_ij_sub, Fy_ij_sub\n                if torch.cuda.is_available(): torch.cuda.empty_cache() # Aggressive cache clearing\n    \n            # Store results for the fully processed i-th target chunk\n            Fx_total[:, i_start:i_end] = Fx_on_i_accum\n            Fy_total[:, i_start:i_end] = Fy_on_i_accum\n            #V_total_system += torch.sum(V_on_i_accum, dim=1) # Sum potential over points in i-chunk, then add to system total\n    \n            # Explicitly delete i-chunk accumulators\n            del target_coords_i_cdist, target_x_i, target_y_i, target_charges_qi\n            del Fx_on_i_accum, Fy_on_i_accum, #V_on_i_accum\n            if torch.cuda.is_available(): torch.cuda.empty_cache()\n    \n        if self.debug:\n            print(f\"  aggregate_to_one_function (ULTRA EFFICIENT) finished.\")\n            print(f\"  Fx_total: {Fx_total}\")\n            print(f\"  Fy_total: {Fy_total}\")\n            #if V_total_system.numel() > 0: print(f\"  V_total_system: {V_total_system.item()}\")\n            print(\"-\" * 59)\n    \n        return Fx_total, Fy_total, zeta, #V_total_system # Make sure zeta is returned\n\n    def forward(self, ipt, chunk_size=1024):\n        assert ipt.shape[2] and ipt.shape[3] == self.imsize, \"Input Shape's Height and Width must match parameter imsize\"\n        for layer in range(self.layers):\n            \n            if self.use_self_attention:\n                attended_ipt = self.input_attention(ipt)\n                ipt = self.attention_scale * attended_ipt + ipt\n            if self.debug:\n                print(f\"After self-attention ipt Shape: {ipt.shape}\")\n                \n            if self.debug:\n                print(f\"Running Layer {layer + 1}/{self.layers}\")\n                print(f\"-\"*59)\n            for loop in range(self.loops):\n                if self.debug:\n                    print(f\"Starting {loop + 1}/{self.loops} for compute_electric_force...\")\n                    print(\" \")\n                #zeta, x, y, D = self.compute_distance(ipt, chunk_size)\n                #theta_rad = self.compute_angle(x, y, chunk_size)\n                #Fx, Fy = self.compute_electric_force(ipt, theta_rad, D.pow(-2), chunk_size)\n                Fx, Fy, zeta = self.aggregate_to_one_function(ipt, chunk_size)\n                zeta = self.compute_vector_translation(Fx, Fy, zeta)\n    \n            zeta = self.space_norm(zeta)\n            ipt = self.update_input(ipt, zeta)\n            \n        ipt = self.space_norm(ipt)\n        Fx, Fy, zeta = self.aggregate_to_one_function(ipt, chunk_size)\n        #V = torch.abs(V)\n        \n        return ipt, zeta, #V","metadata":{"execution":{"iopub.status.busy":"2025-06-04T10:47:17.288548Z","iopub.execute_input":"2025-06-04T10:47:17.288714Z","iopub.status.idle":"2025-06-04T10:47:17.335695Z","shell.execute_reply.started":"2025-06-04T10:47:17.288702Z","shell.execute_reply":"2025-06-04T10:47:17.335217Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"model = ElectricForceModel(debug = True)\nmodel = model.to(DEVICE)\n\nout, zeta = model(inpt.to(DEVICE))","metadata":{"execution":{"iopub.status.busy":"2025-06-04T10:47:17.337388Z","iopub.execute_input":"2025-06-04T10:47:17.337564Z","iopub.status.idle":"2025-06-04T10:47:28.479125Z","shell.execute_reply.started":"2025-06-04T10:47:17.337551Z","shell.execute_reply":"2025-06-04T10:47:28.478520Z"}}},{"cell_type":"code","source":"class SegmentationDataset(torch.utils.data.Dataset):\n    def __init__(self, image_paths, mask_paths, transforms=None):\n        self.image_paths = image_paths\n        self.mask_paths = mask_paths\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        # Đọc ảnh và mask\n        image = np.array(Image.open(self.image_paths[idx]).convert(\"RGB\"))\n        mask = np.array(Image.open(self.mask_paths[idx]).convert(\"L\"))\n        \n        mask = np.where(mask < 85, 0, np.where(mask < 170, 1, 2))\n        \n        if self.transforms:\n            augmented = self.transforms(image=image, mask=mask)\n            image = augmented['image']\n            mask = augmented['mask']\n            \n        mask = mask.long() if isinstance(mask, torch.Tensor) else torch.from_numpy(mask).long()\n        #print(mask.shape)\n        return image, mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T10:47:28.480478Z","iopub.execute_input":"2025-06-04T10:47:28.480681Z","iopub.status.idle":"2025-06-04T10:47:28.486003Z","shell.execute_reply.started":"2025-06-04T10:47:28.480667Z","shell.execute_reply":"2025-06-04T10:47:28.485492Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def compute_multi_iou(outputs, masks, num_classes=3):\n    probs = torch.softmax(outputs, dim=1)\n    preds = torch.argmax(probs, dim=1)\n    iou_per_class = []\n    for cls in range(num_classes):\n        pred_cls = (preds == cls)\n        target_cls = (masks == cls)\n        intersection = (pred_cls & target_cls).sum().float()\n        union = (pred_cls | target_cls).sum().float()\n        iou = (intersection / (union + 1e-8)).item()\n        iou_per_class.append(iou)\n    return np.mean(iou_per_class)\n\ndef compute_dice_score(outputs, masks, num_classes=3):\n    probs = torch.softmax(outputs, dim=1)\n    preds = torch.argmax(probs, dim=1)\n    dice_per_class = []\n    for cls in range(num_classes):\n        pred_cls = (preds == cls)\n        target_cls = (masks == cls)\n        intersection = (pred_cls & target_cls).sum().float()\n        dice = (2. * intersection / (pred_cls.sum() + target_cls.sum() + 1e-8)).item()\n        dice_per_class.append(dice)\n    return np.mean(dice_per_class)\n\ndef compute_precision(outputs, masks, num_classes=3):\n    probs = torch.softmax(outputs, dim=1)\n    preds = torch.argmax(probs, dim=1)\n    precision_per_class = []\n    for cls in range(num_classes):\n        pred_cls = (preds == cls)\n        target_cls = (masks == cls)\n        true_positive = (pred_cls & target_cls).sum().float()\n        predicted_positive = pred_cls.sum().float()\n        precision = (true_positive / (predicted_positive + 1e-8)).item()\n        precision_per_class.append(precision)\n    return np.mean(precision_per_class)\n\ndef compute_accuracy(outputs, masks):\n    probs = torch.softmax(outputs, dim=1)\n    preds = torch.argmax(probs, dim=1)\n    correct = (preds == masks).sum().float()\n    total = torch.numel(masks)\n    return (correct / total).item()\n\n\ndef post_process(mask, kernel_size=3):\n    import cv2\n    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n    \n    cleaned = cv2.morphologyEx(mask.numpy().astype(np.uint8), \n                              cv2.MORPH_OPEN, kernel)\n\n    cleaned = cv2.morphologyEx(cleaned, \n                              cv2.MORPH_CLOSE, kernel)\n    \n    return torch.from_numpy(cleaned)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T10:47:28.486616Z","iopub.execute_input":"2025-06-04T10:47:28.486800Z","iopub.status.idle":"2025-06-04T10:47:28.500816Z","shell.execute_reply.started":"2025-06-04T10:47:28.486786Z","shell.execute_reply":"2025-06-04T10:47:28.500330Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class Mixup:\n    def __init__(self, alpha=0.4, p=0.5):\n        self.alpha = alpha\n        self.p = p\n        \n    def __call__(self, images, masks):\n        if np.random.rand() < self.p:\n            batch_size = len(images) if isinstance(images, list) else 1\n            if batch_size > 1:\n                indices = np.random.permutation(batch_size)\n                img2, mask2 = images[indices], masks[indices]\n                \n                lam = np.random.beta(self.alpha, self.alpha)\n\n                mixed_img = lam * images + (1 - lam) * img2\n                mixed_mask = lam * masks + (1 - lam) * mask2\n                \n                return mixed_img, mixed_mask\n        return images, masks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T10:47:28.501351Z","iopub.execute_input":"2025-06-04T10:47:28.501515Z","iopub.status.idle":"2025-06-04T10:47:28.512821Z","shell.execute_reply.started":"2025-06-04T10:47:28.501502Z","shell.execute_reply":"2025-06-04T10:47:28.512338Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def get_train_transform():\n    return A.Compose([\n        A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.RandomRotate90(p=0.5),\n        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=15, p=0.5),\n        A.RandomBrightnessContrast(p=0.5),\n        A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n        ToTensorV2(),\n    ])\n\ndef get_valid_transform():\n    return A.Compose([\n        A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n        A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n        ToTensorV2(),\n    ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T10:47:28.513355Z","iopub.execute_input":"2025-06-04T10:47:28.513518Z","iopub.status.idle":"2025-06-04T10:47:28.522408Z","shell.execute_reply.started":"2025-06-04T10:47:28.513507Z","shell.execute_reply":"2025-06-04T10:47:28.521954Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def visualize_predictions(model, dataloader, device, num_samples=4):\n    model.eval()\n    images, masks, preds = [], [], []\n    \n    with torch.no_grad():\n        for img, mask in dataloader:\n            img = img.to(device)\n            output = model(img)\n            pred = torch.argmax(torch.softmax(output, dim=1), dim=1)\n            \n            for i in range(min(img.shape[0], num_samples - len(images))):\n                if len(images) < num_samples:\n                    images.append(img[i].cpu())\n                    masks.append(mask[i].cpu())\n                    preds.append(pred[i].cpu())\n            \n            if len(images) >= num_samples:\n                break\n    \n    fig, axes = plt.subplots(num_samples, 3, figsize=(15, num_samples * 5))\n    \n    for i in range(num_samples):\n        img = images[i].permute(1, 2, 0).numpy()\n        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n        img = np.clip(img, 0, 1)\n        \n        axes[i, 0].imshow(img)\n        axes[i, 0].set_title('Original Image')\n        axes[i, 0].axis('off')\n        \n        axes[i, 1].imshow(masks[i], cmap='jet')\n        axes[i, 1].set_title('Ground Truth')\n        axes[i, 1].axis('off')\n        \n        axes[i, 2].imshow(preds[i], cmap='jet')\n        axes[i, 2].set_title('Prediction')\n        axes[i, 2].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    return fig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T10:47:28.522983Z","iopub.execute_input":"2025-06-04T10:47:28.523159Z","iopub.status.idle":"2025-06-04T10:47:28.535905Z","shell.execute_reply.started":"2025-06-04T10:47:28.523147Z","shell.execute_reply":"2025-06-04T10:47:28.535455Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def train_one_epoch(model, dataloader, optimizer, criterion, device, tau):\n    model.train()\n    train_loss = 0.0\n    \n    pbar = tqdm(dataloader, desc='Training')\n    for images, masks in pbar:\n        images = images.to(device)\n        masks = masks.to(device)\n        \n        optimizer.zero_grad()\n        outputs, zeta= model(images)\n        loss = criterion(outputs, masks)#*(1-tau) + V*tau\n        \n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item()\n        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n    \n    return train_loss / len(dataloader)\n\n# Validation function\ndef validate(model, dataloader, criterion, device, tau):\n    model.eval()\n    val_loss = 0.0\n    iou_scores, dsc_scores, precision_scores, acc_scores = [], [], [], []\n    \n    with torch.no_grad():\n        for images, masks in tqdm(dataloader, desc='Validation'):\n            images = images.to(device)\n            masks = masks.to(device)\n            \n            outputs, zeta= model(images)\n            #outputs = convert_model_output_to_segmentation(outputs)\n            loss = criterion(outputs, masks)#*(1-tau) + V*tau\n            \n            val_loss += loss.item()\n            \n            iou_scores.append(compute_multi_iou(outputs, masks))\n            dsc_scores.append(compute_dice_score(outputs, masks))\n            precision_scores.append(compute_precision(outputs, masks))\n            acc_scores.append(compute_accuracy(outputs, masks))\n    \n    mean_iou = np.mean(iou_scores)\n    mean_dsc = np.mean(dsc_scores)\n    mean_precision = np.mean(precision_scores)\n    mean_acc = np.mean(acc_scores)\n    \n    return val_loss / len(dataloader), mean_iou, mean_dsc, mean_precision, mean_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T10:47:28.536387Z","iopub.execute_input":"2025-06-04T10:47:28.536539Z","iopub.status.idle":"2025-06-04T10:47:28.543282Z","shell.execute_reply.started":"2025-06-04T10:47:28.536528Z","shell.execute_reply":"2025-06-04T10:47:28.542826Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs, device, fold=None):\n    best_metrics = {'mean_iou': 0.0, 'mean_dsc': 0.0}\n    history = {\n        'train_loss': [], 'val_loss': [],\n        'iou': [], 'dsc': [], 'precision': [], 'accuracy': []\n    }\n    \n    for epoch in range(num_epochs):\n        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, tau = 0.2)\n        history['train_loss'].append(train_loss)\n        \n        val_loss, mean_iou, mean_dsc, mean_precision, mean_acc = validate(model, val_loader, criterion, device, tau = 0.2)\n        history['val_loss'].append(val_loss)\n        history['iou'].append(mean_iou)\n        history['dsc'].append(mean_dsc)\n        history['precision'].append(mean_precision)\n        history['accuracy'].append(mean_acc)\n        \n        scheduler.step(mean_dsc)\n        \n        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n        print(f\"Accuracy: {mean_acc:.4f}, Mean IoU: {mean_iou:.4f}, Mean DSC/F1: {mean_dsc:.4f}, Mean Precision: {mean_precision:.4f}\")\n        \n        if mean_dsc > best_metrics['mean_dsc']:\n            best_metrics['mean_dsc'] = mean_dsc\n            best_metrics['mean_iou'] = mean_iou\n            best_metrics['mean_precision'] = mean_precision\n            best_metrics['mean_acc'] = mean_acc\n            \n            model_name = f\"best_model_fold{fold}.pth\" if fold is not None else \"best_model.pth\"\n            torch.save(model.state_dict(), model_name)\n            print(f\"Saved best model with DSC: {mean_dsc:.4f}\")\n        \n        print(f\"Best DSC: {best_metrics['mean_dsc']:.4f}, Best IoU: {best_metrics['mean_iou']:.4f}\")\n        print(\"-\" * 50)\n    \n    return model, history, best_metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T10:47:28.543806Z","iopub.execute_input":"2025-06-04T10:47:28.543978Z","iopub.status.idle":"2025-06-04T10:47:28.556858Z","shell.execute_reply.started":"2025-06-04T10:47:28.543966Z","shell.execute_reply":"2025-06-04T10:47:28.556411Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def find_optimal_k(all_images, all_masks, k_values=[3, 5, 7, 10], num_epochs=10, tau = 0.2):\n\n    k_scores = []\n    k_variances = []\n    train_times = []\n    \n    for k in k_values:\n        fold_scores = []\n        \n        kf = KFold(n_splits=k, shuffle=True, random_state=SEED)\n        start_time = time.time()\n        \n        for fold, (train_idx, val_idx) in enumerate(kf.split(all_images)):\n            print(f\"  Fold {fold+1}/{k}\")\n            \n            train_img = [all_images[i] for i in train_idx]\n            train_mask = [all_masks[i] for i in train_idx]\n            val_img = [all_images[i] for i in val_idx]\n            val_mask = [all_masks[i] for i in val_idx]\n            \n            # Tạo dataset\n            train_dataset = SegmentationDataset(train_img, train_mask, get_train_transform())\n            val_dataset = SegmentationDataset(val_img, val_mask, get_valid_transform())\n            \n            # Tạo dataloader\n            train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n            \n            # Khởi tạo model mới cho mỗi fold\n            model = ElectricForceModel().to(DEVICE)\n            \n            # Setup optimizer và loss\n            optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n            criterion = nn.CrossEntropyLoss()\n            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3)\n            \n            best_metrics = {'mean_dsc': 0.0}\n            \n            for epoch in range(num_epochs):\n                model.train()\n                for images, masks in train_loader:\n                    images, masks = images.to(DEVICE), masks.to(DEVICE)\n                    optimizer.zero_grad()\n                    outputs, zeta= model(images)\n                    loss = criterion(outputs, masks)#*(1-tau) + V*tau\n                    loss.backward()\n                    optimizer.step()\n                \n                model.eval()\n                dsc_scores = []\n                with torch.no_grad():\n                    for images, masks in val_loader:\n                        images, masks = images.to(DEVICE), masks.to(DEVICE)\n                        outputs, zeta= model(images)\n                        #outputs = conconvert_model_output_to_segmentation(outputs)\n                        dsc_scores.append(compute_dice_score(outputs, masks))\n                \n                mean_dsc = np.mean(dsc_scores)\n                scheduler.step(mean_dsc)\n                \n                if mean_dsc > best_metrics['mean_dsc']:\n                    best_metrics['mean_dsc'] = mean_dsc\n            \n            # Lưu Dice score của fold này\n            fold_scores.append(best_metrics['mean_dsc'])\n            print(f\"    Dice score: {best_metrics['mean_dsc']:.4f}\")\n        \n        train_time = time.time() - start_time\n        train_times.append(train_time)\n        \n        mean_score = np.mean(fold_scores)\n        score_variance = np.var(fold_scores)\n        \n        k_scores.append(mean_score)\n        k_variances.append(score_variance)\n        \n        print(f\"K={k}, Điểm trung bình: {mean_score:.4f}, Phương sai: {score_variance:.6f}\")\n        print(f\"Thời gian training: {train_time:.2f} giây\")\n    \n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n    \n    ax1.plot(k_values, k_scores, 'o-', linewidth=2, color='blue')\n    ax1.set_xlabel('Số lượng Fold (K)')\n    ax1.set_ylabel('Điểm DSC trung bình')\n    ax1.set_title('Hiệu suất theo giá trị K')\n    ax1.grid(True)\n    \n    # Vẽ phương sai\n    ax2.plot(k_values, k_variances, 'o-', linewidth=2, color='orange')\n    ax2.set_xlabel('Số lượng Fold (K)')\n    ax2.set_ylabel('Phương sai')\n    ax2.set_title('Phương sai theo giá trị K')\n    ax2.grid(True)\n    \n    # Vẽ thời gian training\n    ax3.plot(k_values, train_times, 'o-', linewidth=2, color='green')\n    ax3.set_xlabel('Số lượng Fold (K)')\n    ax3.set_ylabel('Thời gian training (giây)')\n    ax3.set_title('Thời gian training theo giá trị K')\n    ax3.grid(True)\n    \n    plt.tight_layout()\n    plt.savefig('k_fold_elbow_analysis.png')\n    plt.show()\n    \n    optimal_k = find_elbow_point(k_values, k_scores, k_variances)\n\n    \n    return optimal_k\n\ndef find_elbow_point(k_values, k_scores, k_variances):\n    \"\"\"\n    Xác định điểm elbow dựa trên phần trăm cải thiện và phương sai\n    \"\"\"\n    \n    improvements = []\n    for i in range(1, len(k_values)):\n        improvement = (k_scores[i] - k_scores[i-1]) / k_scores[i-1] * 100\n        improvements.append(improvement)\n    \n    for i in range(len(improvements)):\n        if improvements[i] < 1.0:\n            return k_values[i+1]\n    \n    var_index = np.argmin(k_variances)\n    return k_values[var_index]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T10:47:28.557436Z","iopub.execute_input":"2025-06-04T10:47:28.557596Z","iopub.status.idle":"2025-06-04T10:47:28.571478Z","shell.execute_reply.started":"2025-06-04T10:47:28.557584Z","shell.execute_reply":"2025-06-04T10:47:28.571021Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def run_training():\n    print(f\"Using device: {DEVICE}\")\n    \n    root_dir = Path('/kaggle/input/data-1/content')\n    all_images = sorted(list((root_dir / 'images').glob('*.bmp')))\n    all_masks = sorted(list((root_dir / 'masks').glob('*.png')))\n    \n    all_images = sorted(all_images, key=lambda x: x.stem)\n    all_masks = sorted(all_masks, key=lambda x: x.stem)\n    \n    print(f\"Total images: {len(all_images)}\")\n    print(f\"Total masks: {len(all_masks)}\")\n    \n    optimal_k = find_optimal_k(\n        all_images, all_masks, \n        k_values=[3, 5, 7, 10], \n        num_epochs=10\n    )\n    \n    kf = KFold(n_splits=optimal_k, shuffle=True, random_state=SEED)\n    fold_metrics = []\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(all_images)):\n        print(f\"Training Fold {fold+1}/{optimal_k}\")\n\n        train_img = [all_images[i] for i in train_idx]\n        train_mask = [all_masks[i] for i in train_idx]\n        val_img = [all_images[i] for i in val_idx]\n        val_mask = [all_masks[i] for i in val_idx]\n        \n        # Create datasets\n        train_dataset = SegmentationDataset(train_img, train_mask, get_train_transform())\n        val_dataset = SegmentationDataset(val_img, val_mask, get_valid_transform())\n        \n        # Create dataloaders\n        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n        \n        # Initialize model\n        model = ElectricForceModel().to(DEVICE)\n        \n        # Setup optimizer and loss\n        optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n        criterion = nn.CrossEntropyLoss()\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=5, factor=0.5)\n        \n        # Train model\n        model, history, best_metrics = train_model(\n            model, \n            train_loader, \n            val_loader,\n            optimizer,\n            criterion,\n            scheduler,\n            NUM_EPOCHS,\n            DEVICE,\n            fold=fold\n        )\n        \n        # Save metrics\n        fold_metrics.append(best_metrics)\n        \n        # Visualize predictions\n        visualize_predictions(model, val_loader, DEVICE)\n        \n        # Plot learning curves\n        plt.figure(figsize=(12, 8))\n        \n        plt.subplot(2, 2, 1)\n        plt.plot(history['train_loss'], label='Train Loss')\n        plt.plot(history['val_loss'], label='Validation Loss')\n        plt.legend()\n        plt.title('Loss Curves')\n        \n        plt.subplot(2, 2, 2)\n        plt.plot(history['iou'], label='IoU')\n        plt.plot(history['dsc'], label='DSC/F1')\n        plt.legend()\n        plt.title('IoU and DSC Curves')\n        \n        plt.subplot(2, 2, 3)\n        plt.plot(history['precision'], label='Precision')\n        plt.legend()\n        plt.title('Precision Curve')\n        \n        plt.subplot(2, 2, 4)\n        plt.plot(history['accuracy'], label='Accuracy')\n        plt.legend()\n        plt.title('Accuracy Curve')\n        \n        plt.tight_layout()\n        plt.savefig(f'learning_curves_fold{fold}.png')\n        plt.show()\n    \n    # Print overall metrics\n    print(\"\\nAverage metrics across all folds:\")\n    avg_iou = np.mean([metrics['mean_iou'] for metrics in fold_metrics])\n    avg_dsc = np.mean([metrics['mean_dsc'] for metrics in fold_metrics])\n    avg_precision = np.mean([metrics['mean_precision'] for metrics in fold_metrics])\n    avg_acc = np.mean([metrics['mean_acc'] for metrics in fold_metrics])\n    \n    print(f\"Avg IoU: {avg_iou:.4f}\")\n    print(f\"Avg DSC/F1: {avg_dsc:.4f}\")\n    print(f\"Avg Precision: {avg_precision:.4f}\")\n    print(f\"Avg Accuracy: {avg_acc:.4f}\")\n\n# Run the training\nif __name__ == \"__main__\":\n    run_training()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T10:47:28.572034Z","iopub.execute_input":"2025-06-04T10:47:28.572199Z","iopub.status.idle":"2025-06-04T10:47:28.765771Z","shell.execute_reply.started":"2025-06-04T10:47:28.572187Z","shell.execute_reply":"2025-06-04T10:47:28.765104Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\nTotal images: 300\nTotal masks: 300\n  Fold 1/3\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/albumentations/core/validation.py:87: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n  original_init(self, **validated_kwargs)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_99/2197131619.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;31m# Run the training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_99/2197131619.py\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total masks: {len(all_masks)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     optimal_k = find_optimal_k(\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mall_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_masks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mk_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_99/3796853250.py\u001b[0m in \u001b[0;36mfind_optimal_k\u001b[0;34m(all_images, all_masks, k_values, num_epochs, tau)\u001b[0m\n\u001b[1;32m     42\u001b[0m                     \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m                     \u001b[0;31m#outputs = convert_model_output_to_segmentation(outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#*(1-tau) + V*tau\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_99/664248594.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, ipt, chunk_size)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mipt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mipt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mipt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Input Shape's Height and Width must match parameter imsize\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: Input Shape's Height and Width must match parameter imsize"],"ename":"AssertionError","evalue":"Input Shape's Height and Width must match parameter imsize","output_type":"error"}],"execution_count":19}]}