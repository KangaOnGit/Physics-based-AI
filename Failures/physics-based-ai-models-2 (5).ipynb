{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11643754,"sourceType":"datasetVersion","datasetId":7306477},{"sourceId":11681844,"sourceType":"datasetVersion","datasetId":7331802}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Imports**","metadata":{}},{"cell_type":"markdown","source":"!pip install albumentations==1.3.0\n!pip install segmentation-models-pytorch==0.3.0","metadata":{"execution":{"iopub.status.busy":"2025-05-25T13:08:53.095197Z","iopub.execute_input":"2025-05-25T13:08:53.095801Z","iopub.status.idle":"2025-05-25T13:10:14.184904Z","shell.execute_reply.started":"2025-05-25T13:08:53.095767Z","shell.execute_reply":"2025-05-25T13:10:14.183758Z"},"scrolled":true}},{"cell_type":"code","source":"# Standard library\nimport os\nimport gc\nimport time\nimport math\nimport random\nimport logging\nimport warnings\nlogging.basicConfig(level=logging.ERROR)\n\n# Scientific computing\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\n\n# Deep learning (PyTorch)\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.parametrizations import weight_norm\nfrom sklearn.metrics import pairwise_distances\n\n# Computer vision\nimport cv2\nimport torchvision\nfrom PIL import Image\nfrom torchvision import transforms\n\n# Miscellaneous utilities\nfrom tqdm import tqdm\nimport timm\nimport einops\nfrom einops import rearrange\nfrom sklearn.model_selection import KFold\nfrom pathlib import Path\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport kagglehub","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T13:15:08.688498Z","iopub.execute_input":"2025-05-25T13:15:08.689176Z","iopub.status.idle":"2025-05-25T13:15:20.966952Z","shell.execute_reply.started":"2025-05-25T13:15:08.689135Z","shell.execute_reply":"2025-05-25T13:15:20.966181Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.7' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nNUM_EPOCHS = 10\nBATCH_SIZE = 1\nLEARNING_RATE = 1e-4\nIMAGE_SIZE = 256\nNUM_CLASSES = 3\nSEED = 42\nN_FOLDS = 5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T13:15:20.968191Z","iopub.execute_input":"2025-05-25T13:15:20.968553Z","iopub.status.idle":"2025-05-25T13:15:21.046034Z","shell.execute_reply.started":"2025-05-25T13:15:20.968535Z","shell.execute_reply":"2025-05-25T13:15:21.045232Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"path = kagglehub.dataset_download(\"khanhpt1999/data-1\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T13:15:21.046886Z","iopub.execute_input":"2025-05-25T13:15:21.047166Z","iopub.status.idle":"2025-05-25T13:15:21.368730Z","shell.execute_reply.started":"2025-05-25T13:15:21.047143Z","shell.execute_reply":"2025-05-25T13:15:21.368227Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/data-1\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# **Model**","metadata":{}},{"cell_type":"code","source":"img = Image.open(\"/kaggle/input/test-images/image_2025-05-05_142003010.png\").convert(\"RGB\")\nprint(\"PIL image size:\", img.size, \"mode:\", img.mode)\n\ntransform = transforms.Compose([\n    transforms.Resize((128, 128)),                 \n    transforms.ToTensor(),                \n    transforms.Normalize( \n        mean=[0.5,0.5,0.5],\n        std=[0.5,0.5,0.5]\n    ),\n])\n\ninpt: torch.Tensor = transform(img)\nprint(\"Tensor shape:\", inpt.shape, \"dtype:\", inpt.dtype, \"range:\", inpt.min(),\"→\",inpt.max())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T13:15:21.370353Z","iopub.execute_input":"2025-05-25T13:15:21.370597Z","iopub.status.idle":"2025-05-25T13:15:21.471461Z","shell.execute_reply.started":"2025-05-25T13:15:21.370562Z","shell.execute_reply":"2025-05-25T13:15:21.470843Z"}},"outputs":[{"name":"stdout","text":"PIL image size: (1080, 608) mode: RGB\nTensor shape: torch.Size([3, 128, 128]) dtype: torch.float32 range: tensor(-1.) → tensor(0.9922)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"inpt = inpt.unsqueeze(0)\nprint(f\"Shape of Input: {inpt.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T13:15:21.472326Z","iopub.execute_input":"2025-05-25T13:15:21.472593Z","iopub.status.idle":"2025-05-25T13:15:21.476636Z","shell.execute_reply.started":"2025-05-25T13:15:21.472569Z","shell.execute_reply":"2025-05-25T13:15:21.475927Z"}},"outputs":[{"name":"stdout","text":"Shape of Input: torch.Size([1, 3, 128, 128])\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"class MultiHeadSelfAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads=8, dropout=0.1):\n        super().__init__()\n        assert embed_dim % num_heads == 0\n        \n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        \n        # Linear projections for Q, K, V\n        self.qkv = nn.Linear(embed_dim, embed_dim * 3, bias=False)\n        self.proj = nn.Linear(embed_dim, embed_dim)\n        self.dropout = nn.Dropout(dropout)\n        \n        # Relative position embeddings for 2D images\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros((2 * 32 - 1) * (2 * 32 - 1), num_heads)\n        )\n        \n        # Initialize relative position bias\n        coords_h = torch.arange(32)\n        coords_w = torch.arange(32)\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing='ij'))\n        coords_flatten = torch.flatten(coords, 1)\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n        relative_coords[:, :, 0] += 32 - 1\n        relative_coords[:, :, 1] += 32 - 1\n        relative_coords[:, :, 0] *= 2 * 32 - 1\n        relative_position_index = relative_coords.sum(-1)\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n        \n    def forward(self, x):\n        B, C, H, W = x.shape\n        N = H * W\n        \n        # Reshape to sequence format [B, N, C]\n        x = x.view(B, C, N).transpose(1, 2)\n        \n        # Generate Q, K, V\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        \n        # Scaled dot-product attention\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        \n        # Add relative position bias\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n            H * W, H * W, -1)\n        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n        attn = attn + relative_position_bias.unsqueeze(0)\n        \n        attn = F.softmax(attn, dim=-1)\n        attn = self.dropout(attn)\n        \n        # Apply attention to values\n        x = (attn @ v).transpose(1, 2).reshape(B, N, self.embed_dim)\n        x = self.proj(x)\n        \n        # Reshape back to spatial format [B, C, H, W]\n        x = x.transpose(1, 2).view(B, C, H, W)\n        \n        return x\n\nclass SelfAttentionConv(nn.Module):\n    \"\"\"Self-attention layer that can replace convolutions\"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, num_heads=8):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.num_heads = num_heads\n        \n        # If channels change, use 1x1 conv for projection\n        if in_channels != out_channels:\n            self.channel_proj = nn.Conv2d(in_channels, out_channels, 1, 1, 0)\n        else:\n            self.channel_proj = nn.Identity()\n            \n        # Self-attention components\n        self.attention = MultiHeadSelfAttention(out_channels, num_heads)\n        \n        # Learnable scale parameter for gradual integration\n        self.gamma = nn.Parameter(torch.zeros(1))\n        \n    def forward(self, x):\n        # Project channels if needed\n        identity = self.channel_proj(x)\n        \n        # Apply self-attention\n        attended = self.attention(identity)\n        \n        # Gradual integration with learnable scale\n        out = self.gamma * attended + identity\n        \n        # Handle stride if needed\n        if self.stride > 1:\n            out = F.avg_pool2d(out, kernel_size=self.stride, stride=self.stride)\n            \n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T13:15:21.477437Z","iopub.execute_input":"2025-05-25T13:15:21.477677Z","iopub.status.idle":"2025-05-25T13:15:21.490719Z","shell.execute_reply.started":"2025-05-25T13:15:21.477655Z","shell.execute_reply":"2025-05-25T13:15:21.489999Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class ElectricForceModel(nn.Module):\n\n    def __init__(\n        self,\n        imsize = 128,\n        v0 = 0,\n        layers = 3,\n        loops = 4,\n        debug = False,\n        beta = 1,\n        use_self_attention = False\n    ):\n        super().__init__()\n\n        self.register_buffer('v0', torch.tensor([v0], dtype=torch.float32))\n        self.register_buffer('beta', torch.tensor([beta], dtype=torch.float32))\n\n        self.adaptive_epsilon = nn.Parameter(torch.tensor([1.0]))\n        self.adaptive_mass = nn.Parameter(torch.tensor([1.0]))\n        self.adaptive_time = nn.Parameter(torch.tensor([0.1]))\n        \n        with torch.no_grad():\n            self.adaptive_epsilon.data.fill_(1.0)\n            self.adaptive_mass.data.fill_(1.0)\n            self.adaptive_time.data.fill_(0.1)\n\n        self.imsize = imsize\n        self.layers = layers\n        self.loops = loops\n        self.debug = debug\n        self.use_self_attention = use_self_attention\n\n        if self.use_self_attention:\n            self.input_attention = MultiHeadSelfAttention(3, num_heads=3)\n            self.attention_scale = nn.Parameter(torch.zeros(1))\n        \n        if self.use_self_attention:\n            self.mapping_to_vector_space = SelfAttentionConv(3, 2, num_heads=2)\n            self.mapping_to_ipt = SelfAttentionConv(2, 3, num_heads=3)\n        else:\n            self.mapping_to_vector_space = nn.Conv2d(3, 2, 1, 1, 0)\n            self.mapping_to_ipt = nn.Conv2d(2, 3, 1, 1, 0)\n            \n        self.space_norm = nn.Tanh()\n        \n\n    def compute_distance(self, ipt, chunk_size=512):\n        \n        \"\"\"\n        Compute the distance of each point with respect to every point in zeta\n\n        Input:\n            - ipt: Input Image | [B, 3, self.imsize, self.imsize]\n\n        Outpt:\n            - zeta: The coordinates map for each pixel in image (x, y) | [B, 2, self.imsize, self.imsize]\n            - x: x-coordinates of every point in zeta | [B, N]\n            - y: y-coordinates of every point in zeta | [B, N]\n            - inverse_sq_dist: 1/r^2, the inverse squared distance of each point with respect to every point in zeta | [B, N, N]\n            - inverse_dist: 1/r, the inverse distance of each point with respect to every point in zeta | [B, N, N]\n            \n        How the Function Works:\n            - Find zeta using a point-wise-convolutional-layer, the number of channel output will be the number of dimensions in vector space\n                            (2 -> (x, y) == Vector Space\n                             3 -> (x, y, z) == 3D Space\n                             ...\n                             ...\n                            )\n            - Find x and y by slicing zeta along the 0th and 1st of the 1st Dimension\n            - Calculate the Euclidean Distance between each point in zeta\n            - inverse_sq_dist is D**(-2) and inverse_dist is D**(-1)\n        \"\"\"\n        \n        if self.debug:\n            print(f\"Running compute_distance...\")\n            print(f\" \")\n            print(f\"ipt Shape: {ipt.shape}\") # [B, 3, self.imsize, self.imsize]\n        \n        if self.use_self_attention:\n            attended_ipt = self.input_attention(ipt)\n            ipt = self.attention_scale * attended_ipt + ipt\n            if self.debug:\n                print(f\"After self-attention ipt Shape: {ipt.shape}\")\n                \n        zeta = self.mapping_to_vector_space(ipt)\n        zeta = self.space_norm(zeta) # [B, 2, self.imsize, self.imsize]\n        if self.debug:\n            print(f\"zeta Shape: {zeta.shape}\")\n\n        B, C, H, W = zeta.shape\n        N = H*W\n\n        pts = zeta.reshape(B, 2, N) # [B, 2, N]\n        if self.debug:\n            print(f\"pts Shape: {pts.shape}\")\n\n        x, y = pts[:, 0], pts[:, 1] # [B, N], [B, N]\n        if self.debug:\n            print(f\"x Shape: {x.shape}\")\n            print(f\"y Shape: {y.shape}\")\n\n        zflat = pts.permute(0, 2, 1) # [B, N, 2]\n        if self.debug:\n            print(f\"zflat Shape: {zflat.shape}\")\n\n        if N <= chunk_size:\n            D = torch.cdist(zflat, zflat, p=2) # [B, N, N]\n            D = torch.where(D==0, float('inf'), D) # [B, N, N]\n        else:\n            if self.debug:\n                print(f\"Using chunking with chunk_size={chunk_size} for N={N}\")\n        \n            D = torch.zeros(B, N, N, device=zflat.device, dtype=zflat.dtype) # [B, N, N]\n            \n            for i in range(0, N, chunk_size):\n                end_i = min(i + chunk_size, N)\n                chunk_points = zflat[:, i:end_i, :] # [B, chunk_size, 2]\n                distances_chunk = torch.cdist(chunk_points, zflat, p=2) # [B, chunk_size, N]\n                \n                for idx in range(end_i - i):\n                    distances_chunk[:, idx, i + idx] = float('inf')\n            \n                D[:, i:end_i, :] = distances_chunk # [B, chunk_size, N]\n    \n        if self.debug:\n            print(f\"D Shape: {D.shape}\") # [B, N, N]\n            \n        inverse_sq_dist = D.pow(-2) # [B, N, N]\n        inverse_dist = D.pow(-1) # [B, N, N]\n        if self.debug:\n            print(f\"inverse_sq_dist Shape: {inverse_sq_dist.shape}\") # [B, N, N]\n            print(f\"inverse_dist Shape: {inverse_dist.shape}\") # [B, N, N]\n            \n        if self.debug:\n            print(\"-\"*59)\n        \n        return zeta, x, y, inverse_sq_dist, inverse_dist\n\n    def compute_angle(self, x, y, chunk_size=512):\n        \n        \"\"\"\n        Compute angle of forces acted on each point with respect to the x-axis\n        Input: \n            - x: x-coordinates of every point in zeta | [B, N]\n            - y: y-coordinates of every point in zeta | [B, N]\n\n        Output:\n            - theta_rad: Radian of angle of forces with respect to the x-axis | [B, N, N]\n\n        How the Function Works:\n            - Applying Inverse Function of tan to find the angle | tan = dy/dx\n        \"\"\"\n        \n        B, N = x.shape\n        \n        if N <= chunk_size:\n            theta_rad = torch.atan2( # [B, N, N]\n                y.unsqueeze(2) - y.unsqueeze(1), # [B, N, 1] - [B, 1, N] -> [B, N, N]\n                x.unsqueeze(2) - x.unsqueeze(1), # [B, N, 1] - [B, 1, N] -> [B, N, N]\n            )\n        else:\n            if self.debug:\n                print(f\"Using chunking for angle computation with chunk_size={chunk_size} for N={N}\")\n            \n            theta_rad = torch.zeros(B, N, N, device=x.device, dtype=x.dtype) # [B, N, N]\n            \n            for i in range(0, N, chunk_size):\n                end_i = min(i + chunk_size, N)\n                \n                x_chunk = x[:, i:end_i] # [B, chunk_size]\n                y_chunk = y[:, i:end_i] # [B, chunk_size]\n                \n                dy = y_chunk.unsqueeze(2) - y.unsqueeze(1) # [B, chunk_size, 1] - [B, 1, N] -> [B, chunk_size, N]\n                dx = x_chunk.unsqueeze(2) - x.unsqueeze(1) # [B, chunk_size, 1] - [B, 1, N] -> [B, chunk_size, N]\n                \n                theta_chunk = torch.atan2(dy, dx) # [B, chunk_size, N]\n                theta_rad[:, i:end_i, :] = theta_chunk # [B, chunk_size, N]\n                \n        theta_deg = theta_rad * (180.0 / math.pi) # [B, N, N]\n        if self.debug:\n            print(f\"Running compute_angle...\")\n            print(f\" \")\n            print(f\"theta_rad Shape: {theta_rad.shape}\")\n            print(f\"theta_deg Shape: {theta_deg.shape}\")\n            \n        if self.debug:\n            print(\"-\"*59)\n        return theta_rad\n\n    def compute_electric_force(self, ipt, theta_rad, inverse_sq_dist, chunk_size = 512):\n        \n        \"\"\"\n        Compute the Net Electric Force acted on each point.\n\n        Input:\n            - ipt: Input Image | [B, 3, self.imsize, self.imsize]\n            - theta_rad: The angle of forces acted on each point with respect to the x-axis | [B, N, N]\n            - inverse_sq_dist: 1/r^2, the inverse squared distance of each point with respect to every point in zeta | [B, N, N]\n\n        Output:\n            - Fx: Force acted on each point in the x-axis | [B, N]\n            - Fy: Force acted on each point in the y-axis | [B, N]\n\n        How the Function Works:\n            - F_E = qE = qi (k_E sum(qj/r^2))\n                + Since all variables are vectors, we split them into x and y axis, respectively\n        \"\"\"\n        \n        k_E = 1.0 / (4.0 * math.pi *torch.abs(self.adaptive_epsilon))\n        \n        B, C, H, W = ipt.shape\n        N = H*W\n        \n        ipt_reshaped = ipt.reshape(B, N, C) # [B, N, C]\n        \n        qi_qj = ipt_reshaped.unsqueeze(2) * ipt_reshaped.unsqueeze(1) # [B, N, 1, C] * [B, 1, N, C] -> [B, N, N, C]\n        qi_qj = torch.mean(qi_qj, dim = 3) # [B, N, N]\n        if self.debug:\n            print(f\"Running compute_electric_force...\")\n            print(\" \")\n            print(f\"qi_qj Shape: {qi_qj.shape}\")\n\n        if N <= chunk_size:\n            Fx = k_E * qi_qj * inverse_sq_dist * torch.cos(theta_rad) # [B, N, N]\n            Fy = k_E * qi_qj * inverse_sq_dist * torch.sin(theta_rad) # [B, N, N]\n    \n            Fx = torch.sum(Fx, dim = 2) # [B, N]\n            Fy = torch.sum(Fy, dim = 2) # [B, N]\n        else:\n            if self.debug:\n                print(f\"Using chunking for force computation with chunk_size={chunk_size} for N={N}\")\n            \n            Fx = torch.zeros(B, N, device=ipt.device, dtype=ipt.dtype) # [B, N]\n            Fy = torch.zeros(B, N, device=ipt.device, dtype=ipt.dtype) # [B, N]\n            \n            for i in range(0, N, chunk_size):\n                end_i = min(i + chunk_size, N)\n                \n                qi_qj_chunk = qi_qj[:, i:end_i, :] # [B, chunk_size, N]\n                inverse_sq_dist_chunk = inverse_sq_dist[:, i:end_i, :] # [B, chunk_size, N]\n                theta_rad_chunk = theta_rad[:, i:end_i, :] # [B, chunk_size, N]\n                \n                Fx_chunk = k_E * qi_qj_chunk * inverse_sq_dist_chunk * torch.cos(theta_rad_chunk) # [B, chunk_size, N]\n                Fy_chunk = k_E * qi_qj_chunk * inverse_sq_dist_chunk * torch.sin(theta_rad_chunk) # [B, chunk_size, N]\n                \n                Fx[:, i:end_i] = torch.sum(Fx_chunk, dim=2) # [B, chunk_size]\n                Fy[:, i:end_i] = torch.sum(Fy_chunk, dim=2) # [B, chunk_size]\n        \n        if self.debug:\n            print(f\"Fx Shape: {Fx.shape}\")\n            print(f\"Fy Shape: {Fy.shape}\")\n            print(\"-\"*59)\n            \n        return Fx, Fy\n\n    def compute_vector_translation(self, Fx, Fy, zeta):\n        \"\"\"\n        Move each point in the vector space corresponding to forces acted on it.\n\n        Input:\n            - Fx: Force acted on each point in the x-axis | [B, N]\n            - Fy: Force acted on each point in the y-axis | [B, N]\n            - zeta: Coordinates of each point (x, y) | [B, 2, self.imsize, self.imsize]\n\n        Output:\n            - new_zeta: Fully updated Coordinates of each point (x, y) | [B, 2, self.imsize, self.imsize]\n\n        How the Function Works:\n            - Displacement = Velocity x time + Initial Position -> D =  vt + x0\n            - v = Acceleration x time + Initial Velocity = at + v0\n            - Acceleration = Force/Mass = F/m\n                + Since all variables are vectors, we split them into x and y axis, respectively\n        \"\"\"\n        \n        if self.debug:\n            print(f\"Starting compute_vector_translation...\")\n            print(\" \")\n            \n        ax = Fx / torch.abs(self.adaptive_mass) # [B, N]\n        ay = Fy / torch.abs(self.adaptive_mass) # [B, N]\n        if self.debug:\n            print(f\"ax Shape: {ax.shape}\")\n            print(f\"ay Shape: {ay.shape}\")\n            \n        B, N = Fx.shape\n        v0 = self.v0.expand(B, N) # [B, N]\n        \n        if self.debug:\n            print(f\"v0 Shape: {v0.shape}\")\n            \n        time_step = torch.abs(self.adaptive_time)\n        vx = v0 + ax * time_step # [B, N]\n        vy = v0 + ay * time_step # [B, N]\n        vx = vx.reshape(B, self.imsize, self.imsize) # [B, self.imsize, self.imsize]\n        vy = vy.reshape(B, self.imsize, self.imsize) # [B, self.imsize, self.imsize]\n        if self.debug:\n            print(f\"vx Shape: {vx.shape}\")\n            print(f\"vy Shape: {vy.shape}\")\n            \n        if self.debug:\n            print(f\"zeta Shape: {zeta.shape}\")\n            print(f\"zeta[:, 0] Shape: {zeta[:, 0].shape}\")\n            print(f\"zeta[:, 1] Shape: {zeta[:, 1].shape}\")\n        new_zeta = zeta.clone()\n        new_zeta[:, 0] = zeta[:, 0] + vx # [B, 2, self.imsize, self.imsize]\n        new_zeta[:, 1] = zeta[:, 1] + vy # [B, 2, self.imsize, self.imsize]\n\n        \n        if self.debug:\n            print(f\"Updated zeta Shape: {new_zeta.shape}\")\n            print(f\"-\"*59)\n            \n        return new_zeta\n\n    def update_input(self, ipt, zeta):\n        \n        \"\"\"\n        Update Input by Unmapping zeta\n\n        Input:\n            - ipt: Input Image | [B, 3, self.imsize, self.imsize]\n            - zeta: Coordinates of each point (x, y) | [B, 2, self.imsize, self.imsize]\n\n        Output:\n            - new_ipt: New Input Image | [B, 3, self.imsize, self.imsize]\n\n        How the Function Works:\n            - You got the new zeta Coordinates\n            - You unmap it using a point-wise-convolutional-layer\n            - New Input will be the summation of the unmapped zeta and the Input Image\n        \"\"\"\n        if self.debug:\n            print(f\"Running update_input...\")\n            \n        map_to_ipt = self.mapping_to_ipt(zeta) # [B, 3, self.imsize, self.imsize]\n        new_ipt = map_to_ipt*self.beta + ipt*(1-self.beta) # [B, 3, self.imsize, self.imsize]\n        return new_ipt\n\n    def compute_energy_of_system(self, ipt, inverse_dist, chunk_size = 512):\n\n        \"\"\"\n        Compute total Energy of the System\n\n        Input:\n            - ipt: Final Input Image | [B, 3, self.imsize, self.imsize]\n            - inverse_dist: 1/r, the inverse distance of each point with respect to every point in zeta | [B, N, N]\n        \n        Output:\n            - V: Total Energy of Final Input Image | [B]\n\n        How the Function Works:\n            - V = sum(qi * Vj) = qi (k_E sum(qj/r^2))\n        \"\"\"\n        \n        k_E = 1.0 / (4.0 * math.pi * torch.abs(self.adaptive_epsilon))\n        \n        B, C, H, W = ipt.shape\n        N = H*W\n        \n        ipt_reshaped = ipt.reshape(B, N, C) # [B, N, C]\n        \n        qi_qj = ipt_reshaped.unsqueeze(2) * ipt_reshaped.unsqueeze(1) # [B, N, 1, C] * [B, 1, N, C] -> [B, N, N, C]\n        qi_qj = torch.mean(qi_qj, dim = 3) # [B, N, N]\n        if self.debug:\n            print(f\"Running compute_energy_of_system...\")\n            print(\" \")\n            print(f\"qi_qj Shape: {qi_qj.shape}\")\n\n        if N <= chunk_size:\n            V = k_E * qi_qj * inverse_dist # [B, N, N]\n            V = torch.sum(V, dim = 2) # [B, N]\n            V = torch.sum(V, dim = 1) # [B]\n        else:\n            if self.debug:\n                print(f\"Using chunking for energy computation with chunk_size={chunk_size} for N={N}\")\n            \n            V = torch.zeros(B, device=ipt.device, dtype=ipt.dtype) # [B]\n            \n            for i in range(0, N, chunk_size):\n                end_i = min(i + chunk_size, N)\n                \n                qi_qj_chunk = qi_qj[:, i:end_i, :] # [B, chunk_size, N]\n                inverse_dist_chunk = inverse_dist[:, i:end_i, :] # [B, chunk_size, N]\n                \n                V_chunk = k_E * qi_qj_chunk * inverse_dist_chunk # [B, chunk_size, N]\n                V_chunk = torch.sum(V_chunk, dim=2) # [B, chunk_size]\n                V_chunk = torch.sum(V_chunk, dim=1) # [B]\n                \n                V += V_chunk # [B]\n                \n        if self.debug:\n            print(f\"V Shape: {V.shape}\")\n            print(\"-\"*59)\n            \n        return V\n            \n    def forward(self, ipt, chunk_size=512):\n        assert ipt.shape[2] and ipt.shape[3] == self.imsize, \"Input Shape's Height and Width must match parameter imsize\"\n        for layer in range(self.layers):\n            if self.debug:\n                print(f\"Running Layer {layer + 1}/{self.layers}\")\n                print(f\"-\"*59)\n            for loop in range(self.loops):\n                if self.debug:\n                    print(f\"Starting {loop + 1}/{self.loops} for compute_electric_force...\")\n                    print(\" \")\n                zeta, x, y, inverse_sq_dist, inverse_dist = self.compute_distance(ipt, chunk_size)\n                theta_rad = self.compute_angle(x, y, chunk_size)\n                Fx, Fy = self.compute_electric_force(ipt, theta_rad, inverse_sq_dist, chunk_size)\n                zeta = self.compute_vector_translation(Fx, Fy, zeta)\n    \n            zeta = self.space_norm(zeta)\n            ipt = self.update_input(ipt, zeta)\n    \n        zeta, _, _, _, inverse_dist = self.compute_distance(ipt, chunk_size)\n        V = self.compute_energy_of_system(ipt, inverse_dist, chunk_size)\n        \n        return ipt, zeta, V","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T13:15:21.491536Z","iopub.execute_input":"2025-05-25T13:15:21.491763Z","iopub.status.idle":"2025-05-25T13:15:21.524633Z","shell.execute_reply.started":"2025-05-25T13:15:21.491748Z","shell.execute_reply":"2025-05-25T13:15:21.523977Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"model = ElectricForceModel(debug = True)\nmodel = model.to(DEVICE)\n\nout, zeta, V = model(inpt.to(DEVICE))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T13:15:21.525398Z","iopub.execute_input":"2025-05-25T13:15:21.525628Z","iopub.status.idle":"2025-05-25T13:15:23.119754Z","shell.execute_reply.started":"2025-05-25T13:15:21.525603Z","shell.execute_reply":"2025-05-25T13:15:23.118352Z"}},"outputs":[{"name":"stdout","text":"Running Layer 1/3\n-----------------------------------------------------------\nStarting 1/4 for compute_electric_force...\n \nRunning compute_distance...\n \nipt Shape: torch.Size([1, 3, 128, 128])\nzeta Shape: torch.Size([1, 2, 128, 128])\npts Shape: torch.Size([1, 2, 16384])\nx Shape: torch.Size([1, 16384])\ny Shape: torch.Size([1, 16384])\nzflat Shape: torch.Size([1, 16384, 2])\nUsing chunking with chunk_size=512 for N=16384\nD Shape: torch.Size([1, 16384, 16384])\ninverse_sq_dist Shape: torch.Size([1, 16384, 16384])\ninverse_dist Shape: torch.Size([1, 16384, 16384])\n-----------------------------------------------------------\nUsing chunking for angle computation with chunk_size=512 for N=16384\nRunning compute_angle...\n \ntheta_rad Shape: torch.Size([1, 16384, 16384])\ntheta_deg Shape: torch.Size([1, 16384, 16384])\n-----------------------------------------------------------\nRunning compute_electric_force...\n \nqi_qj Shape: torch.Size([1, 16384, 16384])\nUsing chunking for force computation with chunk_size=512 for N=16384\nFx Shape: torch.Size([1, 16384])\nFy Shape: torch.Size([1, 16384])\n-----------------------------------------------------------\nStarting compute_vector_translation...\n \nax Shape: torch.Size([1, 16384])\nay Shape: torch.Size([1, 16384])\nv0 Shape: torch.Size([1, 16384])\nvx Shape: torch.Size([1, 128, 128])\nvy Shape: torch.Size([1, 128, 128])\nzeta Shape: torch.Size([1, 2, 128, 128])\nzeta[:, 0] Shape: torch.Size([1, 128, 128])\nzeta[:, 1] Shape: torch.Size([1, 128, 128])\nUpdated zeta Shape: torch.Size([1, 2, 128, 128])\n-----------------------------------------------------------\nStarting 2/4 for compute_electric_force...\n \nRunning compute_distance...\n \nipt Shape: torch.Size([1, 3, 128, 128])\nzeta Shape: torch.Size([1, 2, 128, 128])\npts Shape: torch.Size([1, 2, 16384])\nx Shape: torch.Size([1, 16384])\ny Shape: torch.Size([1, 16384])\nzflat Shape: torch.Size([1, 16384, 2])\nUsing chunking with chunk_size=512 for N=16384\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/4074984933.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/350589638.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, ipt, chunk_size)\u001b[0m\n\u001b[1;32m    405\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Starting {loop + 1}/{self.loops} for compute_electric_force...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m                 \u001b[0mzeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minverse_sq_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minverse_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mipt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m                 \u001b[0mtheta_rad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_angle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0mFx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_electric_force\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mipt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_rad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minverse_sq_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/350589638.py\u001b[0m in \u001b[0;36mcompute_distance\u001b[0;34m(self, ipt, chunk_size)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Using chunking with chunk_size={chunk_size} for N={N}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzflat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzflat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [B, N, N]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 606.12 MiB is free. Process 10729 has 14.15 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 13.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 606.12 MiB is free. Process 10729 has 14.15 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 13.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"# Basic tensor to image display\ndef show_tensor_as_image(tensor):\n    # Move to CPU and detach from computational graph\n    tensor = tensor.detach().cpu()\n    \n    # Handle different tensor shapes\n    if tensor.dim() == 4:  # [B, C, H, W]\n        tensor = tensor[0]  # Take first batch\n    \n    if tensor.dim() == 3:  # [C, H, W]\n        if tensor.shape[0] == 1:  # Grayscale [1, H, W]\n            tensor = tensor.squeeze(0)  # Remove channel dim -> [H, W]\n        elif tensor.shape[0] == 3:  # RGB [3, H, W]\n            tensor = tensor.permute(1, 2, 0)  # -> [H, W, 3]\n    \n    # Convert to numpy\n    img = tensor.numpy()\n    \n    # Normalize if values are not in [0, 1] range\n    if img.max() > 1.0 or img.min() < 0.0:\n        img = (img - img.min()) / (img.max() - img.min())\n    \n    # Display\n    plt.figure(figsize=(8, 8))\n    if len(img.shape) == 2:  # Grayscale\n        plt.imshow(img, cmap='gray')\n    else:  # RGB\n        plt.imshow(img)\n    plt.axis('off')\n    plt.show()\n\nshow_tensor_as_image(out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T13:15:23.120136Z","iopub.status.idle":"2025-05-25T13:15:23.120360Z","shell.execute_reply.started":"2025-05-25T13:15:23.120255Z","shell.execute_reply":"2025-05-25T13:15:23.120265Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def denormalize_image(tensor, title=\"Denormalized Image\"):\n    \"\"\"Denormalize from [-1, 1] back to [0, 1] and display\"\"\"\n    tensor = tensor.detach().cpu()\n    \n    if tensor.dim() == 4:\n        tensor = tensor[0]  # Take first batch\n    \n    # Reverse the normalization: (x - 0.5) / 0.5\n    # So: x_original = (x_normalized * 0.5) + 0.5\n    denormalized = tensor * 0.5 + 0.5\n    \n    # Clamp to valid [0, 1] range\n    denormalized = torch.clamp(denormalized, 0, 1)\n    \n    # Convert to display format [H, W, C]\n    if denormalized.shape[0] == 3:\n        img = denormalized.permute(1, 2, 0)\n    else:\n        img = denormalized\n    \n    plt.figure(figsize=(8, 8))\n    plt.imshow(img.numpy())\n    plt.title(title)\n    plt.axis('off')\n    plt.show()\n    \n    return denormalized\n\ndenormalize_image(out, \"Model Output (Denormalized)\")\ndenormalize_image(inpt, \"Original Input (Denormalized)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T13:15:23.121187Z","iopub.status.idle":"2025-05-25T13:15:23.121438Z","shell.execute_reply.started":"2025-05-25T13:15:23.121299Z","shell.execute_reply":"2025-05-25T13:15:23.121309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"=== ORIGINAL INPUT ===\")\nshow_tensor_as_image(inpt)\ndenormalize_image(inpt, \"Original Input (Properly Denormalized)\")\n\nprint(\"\\n=== MODEL OUTPUT ===\") \nshow_tensor_as_image(out) \ndenormalize_image(out, \"Model Output (Properly Denormalized)\")\n\nprint(\"\\n=== COORDINATE VISUALIZATION ===\")\ndef show_zeta_coordinates(zeta, batch_idx=0):\n    zeta = zeta.detach().cpu()\n    x_coords = zeta[batch_idx, 0]\n    y_coords = zeta[batch_idx, 1]\n    \n    # Denormalize coordinates too\n    x_coords = x_coords * 0.5 + 0.5\n    y_coords = y_coords * 0.5 + 0.5\n    \n    plt.figure(figsize=(15, 5))\n    \n    plt.subplot(1, 3, 1)\n    plt.imshow(x_coords.numpy(), cmap='viridis')\n    plt.title('X Coordinates (Denormalized)')\n    plt.colorbar()\n    \n    plt.subplot(1, 3, 2)\n    plt.imshow(y_coords.numpy(), cmap='plasma')\n    plt.title('Y Coordinates (Denormalized)')\n    plt.colorbar()\n    \n    plt.subplot(1, 3, 3)\n    magnitude = torch.sqrt(x_coords**2 + y_coords**2)\n    plt.imshow(magnitude.numpy(), cmap='hot')\n    plt.title('Coordinate Magnitude')\n    plt.colorbar()\n    \n    plt.tight_layout()\n    plt.show()\n\nshow_zeta_coordinates(zeta)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T13:15:23.122471Z","iopub.status.idle":"2025-05-25T13:15:23.122739Z","shell.execute_reply.started":"2025-05-25T13:15:23.122586Z","shell.execute_reply":"2025-05-25T13:15:23.122598Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Data Prepocessing**","metadata":{}},{"cell_type":"code","source":"class SegmentationDataset(torch.utils.data.Dataset):\n    def __init__(self, image_paths, mask_paths, transforms=None):\n        self.image_paths = image_paths\n        self.mask_paths = mask_paths\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        # Đọc ảnh và mask\n        image = np.array(Image.open(self.image_paths[idx]).convert(\"RGB\"))\n        mask = np.array(Image.open(self.mask_paths[idx]).convert(\"L\"))\n        \n        mask = np.where(mask < 85, 0, np.where(mask < 170, 1, 2))\n        \n        if self.transforms:\n            augmented = self.transforms(image=image, mask=mask)\n            image = augmented['image']\n            mask = augmented['mask']\n            \n        mask = mask.long() if isinstance(mask, torch.Tensor) else torch.from_numpy(mask).long()\n        return image, mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T13:15:23.123684Z","iopub.status.idle":"2025-05-25T13:15:23.123981Z","shell.execute_reply.started":"2025-05-25T13:15:23.123798Z","shell.execute_reply":"2025-05-25T13:15:23.123813Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_multi_iou(outputs, masks, num_classes=3):\n    probs = torch.softmax(outputs, dim=1)\n    preds = torch.argmax(probs, dim=1)\n    iou_per_class = []\n    for cls in range(num_classes):\n        pred_cls = (preds == cls)\n        target_cls = (masks == cls)\n        intersection = (pred_cls & target_cls).sum().float()\n        union = (pred_cls | target_cls).sum().float()\n        iou = (intersection / (union + 1e-8)).item()\n        iou_per_class.append(iou)\n    return np.mean(iou_per_class)\n\ndef compute_dice_score(outputs, masks, num_classes=3):\n    probs = torch.softmax(outputs, dim=1)\n    preds = torch.argmax(probs, dim=1)\n    dice_per_class = []\n    for cls in range(num_classes):\n        pred_cls = (preds == cls)\n        target_cls = (masks == cls)\n        intersection = (pred_cls & target_cls).sum().float()\n        dice = (2. * intersection / (pred_cls.sum() + target_cls.sum() + 1e-8)).item()\n        dice_per_class.append(dice)\n    return np.mean(dice_per_class)\n\ndef compute_precision(outputs, masks, num_classes=3):\n    probs = torch.softmax(outputs, dim=1)\n    preds = torch.argmax(probs, dim=1)\n    precision_per_class = []\n    for cls in range(num_classes):\n        pred_cls = (preds == cls)\n        target_cls = (masks == cls)\n        true_positive = (pred_cls & target_cls).sum().float()\n        predicted_positive = pred_cls.sum().float()\n        precision = (true_positive / (predicted_positive + 1e-8)).item()\n        precision_per_class.append(precision)\n    return np.mean(precision_per_class)\n\ndef compute_accuracy(outputs, masks):\n    probs = torch.softmax(outputs, dim=1)\n    preds = torch.argmax(probs, dim=1)\n    correct = (preds == masks).sum().float()\n    total = torch.numel(masks)\n    return (correct / total).item()\n\n\ndef post_process(mask, kernel_size=3):\n    import cv2\n    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n    \n    cleaned = cv2.morphologyEx(mask.numpy().astype(np.uint8), \n                              cv2.MORPH_OPEN, kernel)\n\n    cleaned = cv2.morphologyEx(cleaned, \n                              cv2.MORPH_CLOSE, kernel)\n    \n    return torch.from_numpy(cleaned)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T13:15:23.125199Z","iopub.status.idle":"2025-05-25T13:15:23.125540Z","shell.execute_reply.started":"2025-05-25T13:15:23.125365Z","shell.execute_reply":"2025-05-25T13:15:23.125397Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Mixup:\n    def __init__(self, alpha=0.4, p=0.5):\n        self.alpha = alpha\n        self.p = p\n        \n    def __call__(self, images, masks):\n        if np.random.rand() < self.p:\n            batch_size = len(images) if isinstance(images, list) else 1\n            if batch_size > 1:\n                indices = np.random.permutation(batch_size)\n                img2, mask2 = images[indices], masks[indices]\n                \n                lam = np.random.beta(self.alpha, self.alpha)\n\n                mixed_img = lam * images + (1 - lam) * img2\n                mixed_mask = lam * masks + (1 - lam) * mask2\n                \n                return mixed_img, mixed_mask\n        return images, masks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T13:15:23.127082Z","iopub.status.idle":"2025-05-25T13:15:23.127405Z","shell.execute_reply.started":"2025-05-25T13:15:23.127231Z","shell.execute_reply":"2025-05-25T13:15:23.127244Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_train_transform():\n    return A.Compose([\n        A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.RandomRotate90(p=0.5),\n        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=15, p=0.5),\n        A.RandomBrightnessContrast(p=0.5),\n        A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n        ToTensorV2(),\n    ])\n\ndef get_valid_transform():\n    return A.Compose([\n        A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n        A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n        ToTensorV2(),\n    ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T13:15:23.128260Z","iopub.status.idle":"2025-05-25T13:15:23.128576Z","shell.execute_reply.started":"2025-05-25T13:15:23.128423Z","shell.execute_reply":"2025-05-25T13:15:23.128437Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Train**","metadata":{}},{"cell_type":"code","source":"def visualize_predictions(model, dataloader, device, num_samples=4):\n    model.eval()\n    images, masks, preds = [], [], []\n    \n    with torch.no_grad():\n        for img, mask in dataloader:\n            img = img.to(device)\n            output = model(img)\n            pred = torch.argmax(torch.softmax(output, dim=1), dim=1)\n            \n            for i in range(min(img.shape[0], num_samples - len(images))):\n                if len(images) < num_samples:\n                    images.append(img[i].cpu())\n                    masks.append(mask[i].cpu())\n                    preds.append(pred[i].cpu())\n            \n            if len(images) >= num_samples:\n                break\n    \n    fig, axes = plt.subplots(num_samples, 3, figsize=(15, num_samples * 5))\n    \n    for i in range(num_samples):\n        img = images[i].permute(1, 2, 0).numpy()\n        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n        img = np.clip(img, 0, 1)\n        \n        axes[i, 0].imshow(img)\n        axes[i, 0].set_title('Original Image')\n        axes[i, 0].axis('off')\n        \n        axes[i, 1].imshow(masks[i], cmap='jet')\n        axes[i, 1].set_title('Ground Truth')\n        axes[i, 1].axis('off')\n        \n        axes[i, 2].imshow(preds[i], cmap='jet')\n        axes[i, 2].set_title('Prediction')\n        axes[i, 2].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    return fig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T13:15:23.129405Z","iopub.status.idle":"2025-05-25T13:15:23.129704Z","shell.execute_reply.started":"2025-05-25T13:15:23.129552Z","shell.execute_reply":"2025-05-25T13:15:23.129565Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_one_epoch(model, dataloader, optimizer, criterion, device, tau):\n    model.train()\n    train_loss = 0.0\n    \n    pbar = tqdm(dataloader, desc='Training')\n    for images, masks in pbar:\n        images = images.to(device)\n        masks = masks.to(device)\n        \n        optimizer.zero_grad()\n        outputs, zeta, V = model(images)\n        loss = criterion(outputs, masks)*(1-tau) + V*tau\n        \n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item()\n        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n    \n    return train_loss / len(dataloader)\n\n# Validation function\ndef validate(model, dataloader, criterion, device, tau):\n    model.eval()\n    val_loss = 0.0\n    iou_scores, dsc_scores, precision_scores, acc_scores = [], [], [], []\n    \n    with torch.no_grad():\n        for images, masks in tqdm(dataloader, desc='Validation'):\n            images = images.to(device)\n            masks = masks.to(device)\n            \n            outputs, zeta, V = model(images)\n            loss = criterion(outputs, masks)*(1-tau) + V*tau\n            \n            val_loss += loss.item()\n            \n            iou_scores.append(compute_multi_iou(outputs, masks))\n            dsc_scores.append(compute_dice_score(outputs, masks))\n            precision_scores.append(compute_precision(outputs, masks))\n            acc_scores.append(compute_accuracy(outputs, masks))\n    \n    mean_iou = np.mean(iou_scores)\n    mean_dsc = np.mean(dsc_scores)\n    mean_precision = np.mean(precision_scores)\n    mean_acc = np.mean(acc_scores)\n    \n    return val_loss / len(dataloader), mean_iou, mean_dsc, mean_precision, mean_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T13:15:23.130477Z","iopub.status.idle":"2025-05-25T13:15:23.130793Z","shell.execute_reply.started":"2025-05-25T13:15:23.130618Z","shell.execute_reply":"2025-05-25T13:15:23.130631Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs, device, fold=None):\n    best_metrics = {'mean_iou': 0.0, 'mean_dsc': 0.0}\n    history = {\n        'train_loss': [], 'val_loss': [],\n        'iou': [], 'dsc': [], 'precision': [], 'accuracy': []\n    }\n    \n    for epoch in range(num_epochs):\n        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, tau = 0.2)\n        history['train_loss'].append(train_loss)\n        \n        val_loss, mean_iou, mean_dsc, mean_precision, mean_acc = validate(model, val_loader, criterion, device, tau = 0.2)\n        history['val_loss'].append(val_loss)\n        history['iou'].append(mean_iou)\n        history['dsc'].append(mean_dsc)\n        history['precision'].append(mean_precision)\n        history['accuracy'].append(mean_acc)\n        \n        scheduler.step(mean_dsc)\n        \n        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n        print(f\"Accuracy: {mean_acc:.4f}, Mean IoU: {mean_iou:.4f}, Mean DSC/F1: {mean_dsc:.4f}, Mean Precision: {mean_precision:.4f}\")\n        \n        if mean_dsc > best_metrics['mean_dsc']:\n            best_metrics['mean_dsc'] = mean_dsc\n            best_metrics['mean_iou'] = mean_iou\n            best_metrics['mean_precision'] = mean_precision\n            best_metrics['mean_acc'] = mean_acc\n            \n            model_name = f\"best_model_fold{fold}.pth\" if fold is not None else \"best_model.pth\"\n            torch.save(model.state_dict(), model_name)\n            print(f\"Saved best model with DSC: {mean_dsc:.4f}\")\n        \n        print(f\"Best DSC: {best_metrics['mean_dsc']:.4f}, Best IoU: {best_metrics['mean_iou']:.4f}\")\n        print(\"-\" * 50)\n    \n    return model, history, best_metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T13:15:23.132153Z","iopub.status.idle":"2025-05-25T13:15:23.132430Z","shell.execute_reply.started":"2025-05-25T13:15:23.132281Z","shell.execute_reply":"2025-05-25T13:15:23.132292Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_optimal_k(all_images, all_masks, k_values=[3, 5, 7, 10], num_epochs=10, tau = 0.2):\n\n    k_scores = []\n    k_variances = []\n    train_times = []\n    \n    for k in k_values:\n        fold_scores = []\n        \n        kf = KFold(n_splits=k, shuffle=True, random_state=SEED)\n        start_time = time.time()\n        \n        for fold, (train_idx, val_idx) in enumerate(kf.split(all_images)):\n            print(f\"  Fold {fold+1}/{k}\")\n            \n            train_img = [all_images[i] for i in train_idx]\n            train_mask = [all_masks[i] for i in train_idx]\n            val_img = [all_images[i] for i in val_idx]\n            val_mask = [all_masks[i] for i in val_idx]\n            \n            # Tạo dataset\n            train_dataset = SegmentationDataset(train_img, train_mask, get_train_transform())\n            val_dataset = SegmentationDataset(val_img, val_mask, get_valid_transform())\n            \n            # Tạo dataloader\n            train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n            \n            # Khởi tạo model mới cho mỗi fold\n            model = ElectricForceModel().to(DEVICE)\n            \n            # Setup optimizer và loss\n            optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n            criterion = nn.CrossEntropyLoss()\n            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3)\n            \n            best_metrics = {'mean_dsc': 0.0}\n            \n            for epoch in range(num_epochs):\n                model.train()\n                for images, masks in train_loader:\n                    images, masks = images.to(DEVICE), masks.to(DEVICE)\n                    optimizer.zero_grad()\n                    outputs, zeta, V = model(images)\n                    loss = criterion(outputs, masks)*(1-tau) + V*tau\n                    loss.backward()\n                    optimizer.step()\n                \n                model.eval()\n                dsc_scores = []\n                with torch.no_grad():\n                    for images, masks in val_loader:\n                        images, masks = images.to(DEVICE), masks.to(DEVICE)\n                        outputs, zeta, V = model(images)\n                        dsc_scores.append(compute_dice_score(outputs, masks))\n                \n                mean_dsc = np.mean(dsc_scores)\n                scheduler.step(mean_dsc)\n                \n                if mean_dsc > best_metrics['mean_dsc']:\n                    best_metrics['mean_dsc'] = mean_dsc\n            \n            # Lưu Dice score của fold này\n            fold_scores.append(best_metrics['mean_dsc'])\n            print(f\"    Dice score: {best_metrics['mean_dsc']:.4f}\")\n        \n        train_time = time.time() - start_time\n        train_times.append(train_time)\n        \n        mean_score = np.mean(fold_scores)\n        score_variance = np.var(fold_scores)\n        \n        k_scores.append(mean_score)\n        k_variances.append(score_variance)\n        \n        print(f\"K={k}, Điểm trung bình: {mean_score:.4f}, Phương sai: {score_variance:.6f}\")\n        print(f\"Thời gian training: {train_time:.2f} giây\")\n    \n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n    \n    ax1.plot(k_values, k_scores, 'o-', linewidth=2, color='blue')\n    ax1.set_xlabel('Số lượng Fold (K)')\n    ax1.set_ylabel('Điểm DSC trung bình')\n    ax1.set_title('Hiệu suất theo giá trị K')\n    ax1.grid(True)\n    \n    # Vẽ phương sai\n    ax2.plot(k_values, k_variances, 'o-', linewidth=2, color='orange')\n    ax2.set_xlabel('Số lượng Fold (K)')\n    ax2.set_ylabel('Phương sai')\n    ax2.set_title('Phương sai theo giá trị K')\n    ax2.grid(True)\n    \n    # Vẽ thời gian training\n    ax3.plot(k_values, train_times, 'o-', linewidth=2, color='green')\n    ax3.set_xlabel('Số lượng Fold (K)')\n    ax3.set_ylabel('Thời gian training (giây)')\n    ax3.set_title('Thời gian training theo giá trị K')\n    ax3.grid(True)\n    \n    plt.tight_layout()\n    plt.savefig('k_fold_elbow_analysis.png')\n    plt.show()\n    \n    optimal_k = find_elbow_point(k_values, k_scores, k_variances)\n\n    \n    return optimal_k\n\ndef find_elbow_point(k_values, k_scores, k_variances):\n    \"\"\"\n    Xác định điểm elbow dựa trên phần trăm cải thiện và phương sai\n    \"\"\"\n    \n    improvements = []\n    for i in range(1, len(k_values)):\n        improvement = (k_scores[i] - k_scores[i-1]) / k_scores[i-1] * 100\n        improvements.append(improvement)\n    \n    for i in range(len(improvements)):\n        if improvements[i] < 1.0:\n            return k_values[i+1]\n    \n    var_index = np.argmin(k_variances)\n    return k_values[var_index]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T13:15:23.134109Z","iopub.status.idle":"2025-05-25T13:15:23.134343Z","shell.execute_reply.started":"2025-05-25T13:15:23.134220Z","shell.execute_reply":"2025-05-25T13:15:23.134229Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_training():\n    print(f\"Using device: {DEVICE}\")\n    \n    root_dir = Path('/kaggle/input/data-1/content')\n    all_images = sorted(list((root_dir / 'images').glob('*.bmp')))\n    all_masks = sorted(list((root_dir / 'masks').glob('*.png')))\n    \n    all_images = sorted(all_images, key=lambda x: x.stem)\n    all_masks = sorted(all_masks, key=lambda x: x.stem)\n    \n    print(f\"Total images: {len(all_images)}\")\n    print(f\"Total masks: {len(all_masks)}\")\n    \n    optimal_k = find_optimal_k(\n        all_images, all_masks, \n        k_values=[3, 5, 7, 10], \n        num_epochs=10\n    )\n    \n    kf = KFold(n_splits=optimal_k, shuffle=True, random_state=SEED)\n    fold_metrics = []\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(all_images)):\n        print(f\"Training Fold {fold+1}/{optimal_k}\")\n\n        train_img = [all_images[i] for i in train_idx]\n        train_mask = [all_masks[i] for i in train_idx]\n        val_img = [all_images[i] for i in val_idx]\n        val_mask = [all_masks[i] for i in val_idx]\n        \n        # Create datasets\n        train_dataset = SegmentationDataset(train_img, train_mask, get_train_transform())\n        val_dataset = SegmentationDataset(val_img, val_mask, get_valid_transform())\n        \n        # Create dataloaders\n        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n        \n        # Initialize model\n        model = ElectricForceModel().to(DEVICE)\n        \n        # Setup optimizer and loss\n        optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n        criterion = nn.CrossEntropyLoss()\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=5, factor=0.5)\n        \n        # Train model\n        model, history, best_metrics = train_model(\n            model, \n            train_loader, \n            val_loader,\n            optimizer,\n            criterion,\n            scheduler,\n            NUM_EPOCHS,\n            DEVICE,\n            fold=fold\n        )\n        \n        # Save metrics\n        fold_metrics.append(best_metrics)\n        \n        # Visualize predictions\n        visualize_predictions(model, val_loader, DEVICE)\n        \n        # Plot learning curves\n        plt.figure(figsize=(12, 8))\n        \n        plt.subplot(2, 2, 1)\n        plt.plot(history['train_loss'], label='Train Loss')\n        plt.plot(history['val_loss'], label='Validation Loss')\n        plt.legend()\n        plt.title('Loss Curves')\n        \n        plt.subplot(2, 2, 2)\n        plt.plot(history['iou'], label='IoU')\n        plt.plot(history['dsc'], label='DSC/F1')\n        plt.legend()\n        plt.title('IoU and DSC Curves')\n        \n        plt.subplot(2, 2, 3)\n        plt.plot(history['precision'], label='Precision')\n        plt.legend()\n        plt.title('Precision Curve')\n        \n        plt.subplot(2, 2, 4)\n        plt.plot(history['accuracy'], label='Accuracy')\n        plt.legend()\n        plt.title('Accuracy Curve')\n        \n        plt.tight_layout()\n        plt.savefig(f'learning_curves_fold{fold}.png')\n        plt.show()\n    \n    # Print overall metrics\n    print(\"\\nAverage metrics across all folds:\")\n    avg_iou = np.mean([metrics['mean_iou'] for metrics in fold_metrics])\n    avg_dsc = np.mean([metrics['mean_dsc'] for metrics in fold_metrics])\n    avg_precision = np.mean([metrics['mean_precision'] for metrics in fold_metrics])\n    avg_acc = np.mean([metrics['mean_acc'] for metrics in fold_metrics])\n    \n    print(f\"Avg IoU: {avg_iou:.4f}\")\n    print(f\"Avg DSC/F1: {avg_dsc:.4f}\")\n    print(f\"Avg Precision: {avg_precision:.4f}\")\n    print(f\"Avg Accuracy: {avg_acc:.4f}\")\n\n# Run the training\nif __name__ == \"__main__\":\n    run_training()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T13:15:23.135405Z","iopub.status.idle":"2025-05-25T13:15:23.135665Z","shell.execute_reply.started":"2025-05-25T13:15:23.135547Z","shell.execute_reply":"2025-05-25T13:15:23.135559Z"}},"outputs":[],"execution_count":null}]}