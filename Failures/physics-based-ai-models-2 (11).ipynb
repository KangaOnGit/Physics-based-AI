{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11643754,"sourceType":"datasetVersion","datasetId":7306477},{"sourceId":11681844,"sourceType":"datasetVersion","datasetId":7331802}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Imports**","metadata":{}},{"cell_type":"code","source":"!pip install albumentations==1.3.0\n!pip install segmentation-models-pytorch==0.3.0","metadata":{"execution":{"iopub.status.busy":"2025-06-03T07:49:27.742662Z","iopub.execute_input":"2025-06-03T07:49:27.742907Z","iopub.status.idle":"2025-06-03T07:50:57.651141Z","shell.execute_reply.started":"2025-06-03T07:49:27.742880Z","shell.execute_reply":"2025-06-03T07:50:57.650465Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","text":"Collecting albumentations==1.3.0\n  Downloading albumentations-1.3.0-py3-none-any.whl.metadata (34 kB)\nRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.3.0) (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from albumentations==1.3.0) (1.15.2)\nRequirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.3.0) (0.25.2)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations==1.3.0) (6.0.2)\nCollecting qudida>=0.0.4 (from albumentations==1.3.0)\n  Downloading qudida-0.0.4-py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.3.0) (4.11.0.86)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.11.1->albumentations==1.3.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.11.1->albumentations==1.3.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.11.1->albumentations==1.3.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.11.1->albumentations==1.3.0) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.11.1->albumentations==1.3.0) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.11.1->albumentations==1.3.0) (2.4.1)\nRequirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from qudida>=0.0.4->albumentations==1.3.0) (1.2.2)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from qudida>=0.0.4->albumentations==1.3.0) (4.13.2)\nRequirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (3.4.2)\nRequirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (11.1.0)\nRequirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (2.37.0)\nRequirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (2025.3.30)\nRequirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (25.0)\nRequirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (0.4)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.0) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.0) (3.6.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.11.1->albumentations==1.3.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.11.1->albumentations==1.3.0) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.11.1->albumentations==1.3.0) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.11.1->albumentations==1.3.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.11.1->albumentations==1.3.0) (2024.2.0)\nDownloading albumentations-1.3.0-py3-none-any.whl (123 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\nInstalling collected packages: qudida, albumentations\n  Attempting uninstall: albumentations\n    Found existing installation: albumentations 2.0.5\n    Uninstalling albumentations-2.0.5:\n      Successfully uninstalled albumentations-2.0.5\nSuccessfully installed albumentations-1.3.0 qudida-0.0.4\nCollecting segmentation-models-pytorch==0.3.0\n  Downloading segmentation_models_pytorch-0.3.0-py3-none-any.whl.metadata (28 kB)\nRequirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch==0.3.0) (0.21.0+cu124)\nCollecting pretrainedmodels==0.7.4 (from segmentation-models-pytorch==0.3.0)\n  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting efficientnet-pytorch==0.7.1 (from segmentation-models-pytorch==0.3.0)\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting timm==0.4.12 (from segmentation-models-pytorch==0.3.0)\n  Downloading timm-0.4.12-py3-none-any.whl.metadata (30 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch==0.3.0) (4.67.1)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch==0.3.0) (11.1.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.0) (2.6.0+cu124)\nCollecting munch (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.3.0)\n  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch==0.3.0) (1.26.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.0) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.0) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.0) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.0) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.0) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.0)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.0)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.0)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.0)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.0)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.0)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.0) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.0) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.0) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.0)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.0) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.0) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision>=0.5.0->segmentation-models-pytorch==0.3.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision>=0.5.0->segmentation-models-pytorch==0.3.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision>=0.5.0->segmentation-models-pytorch==0.3.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision>=0.5.0->segmentation-models-pytorch==0.3.0) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision>=0.5.0->segmentation-models-pytorch==0.3.0) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision>=0.5.0->segmentation-models-pytorch==0.3.0) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.0) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision>=0.5.0->segmentation-models-pytorch==0.3.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision>=0.5.0->segmentation-models-pytorch==0.3.0) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision>=0.5.0->segmentation-models-pytorch==0.3.0) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision>=0.5.0->segmentation-models-pytorch==0.3.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision>=0.5.0->segmentation-models-pytorch==0.3.0) (2024.2.0)\nDownloading segmentation_models_pytorch-0.3.0-py3-none-any.whl (97 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading timm-0.4.12-py3-none-any.whl (376 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\nBuilding wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16426 sha256=53e4e472eabd48ed7cf7d0aec108414f54524e2707c63f657cd37811f302fa24\n  Stored in directory: /root/.cache/pip/wheels/8b/6f/9b/231a832f811ab6ebb1b32455b177ffc6b8b1cd8de19de70c09\n  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=69cbe88a969fcb19c9026d308fc5921a68b39bd9aa871ada8c950754eee16fbd\n  Stored in directory: /root/.cache/pip/wheels/5f/5b/96/fd94bc35962d7c6b699e8814db545155ac91d2b95785e1b035\nSuccessfully built efficientnet-pytorch pretrainedmodels\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, munch, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, efficientnet-pytorch, timm, pretrainedmodels, segmentation-models-pytorch\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n  Attempting uninstall: timm\n    Found existing installation: timm 1.0.15\n    Uninstalling timm-1.0.15:\n      Successfully uninstalled timm-1.0.15\nSuccessfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.0 timm-0.4.12\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Standard library\nimport os\nimport gc\nimport time\nimport math\nimport random\nimport logging\nimport warnings\nlogging.basicConfig(level=logging.ERROR)\n\n# Scientific computing\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\n\n# Deep learning (PyTorch)\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.parametrizations import weight_norm\nfrom sklearn.metrics import pairwise_distances\n\n# Computer vision\nimport cv2\nimport torchvision\nfrom PIL import Image\nfrom torchvision import transforms\n\n# Miscellaneous utilities\nfrom tqdm import tqdm\nimport timm\nimport einops\nfrom einops import rearrange\nfrom sklearn.model_selection import KFold\nfrom pathlib import Path\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport kagglehub","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-03T07:50:57.653579Z","iopub.execute_input":"2025-06-03T07:50:57.653775Z","iopub.status.idle":"2025-06-03T07:51:08.083129Z","shell.execute_reply.started":"2025-06-03T07:50:57.653755Z","shell.execute_reply":"2025-06-03T07:51:08.082562Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nNUM_EPOCHS = 5\nBATCH_SIZE = 1\nLEARNING_RATE = 0.001\nIMAGE_SIZE = 64\nNUM_CLASSES = 3\nSEED = 42\nN_FOLDS = 5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T07:51:08.083808Z","iopub.execute_input":"2025-06-03T07:51:08.084139Z","iopub.status.idle":"2025-06-03T07:51:08.236997Z","shell.execute_reply.started":"2025-06-03T07:51:08.084122Z","shell.execute_reply":"2025-06-03T07:51:08.236358Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"path = kagglehub.dataset_download(\"khanhpt1999/data-1\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T07:51:08.237700Z","iopub.execute_input":"2025-06-03T07:51:08.237919Z","iopub.status.idle":"2025-06-03T07:51:11.356103Z","shell.execute_reply.started":"2025-06-03T07:51:08.237895Z","shell.execute_reply":"2025-06-03T07:51:11.355530Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/data-1\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# **Model**","metadata":{}},{"cell_type":"code","source":"img = Image.open(\"/kaggle/input/test-images/image_2025-05-05_142003010.png\").convert(\"RGB\")\nprint(\"PIL image size:\", img.size, \"mode:\", img.mode)\n\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),                 \n    transforms.ToTensor(),                \n    transforms.Normalize( \n        mean=[0.5,0.5,0.5],\n        std=[0.5,0.5,0.5]\n    ),\n])\n\ninpt: torch.Tensor = transform(img)\nprint(\"Tensor shape:\", inpt.shape, \"dtype:\", inpt.dtype, \"range:\", inpt.min(),\"→\",inpt.max())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T07:51:11.356751Z","iopub.execute_input":"2025-06-03T07:51:11.356921Z","iopub.status.idle":"2025-06-03T07:51:11.522716Z","shell.execute_reply.started":"2025-06-03T07:51:11.356906Z","shell.execute_reply":"2025-06-03T07:51:11.521972Z"}},"outputs":[{"name":"stdout","text":"PIL image size: (1080, 608) mode: RGB\nTensor shape: torch.Size([3, 256, 256]) dtype: torch.float32 range: tensor(-1.) → tensor(0.9922)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"inpt = inpt.unsqueeze(0)\nprint(f\"Shape of Input: {inpt.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T07:51:11.523493Z","iopub.execute_input":"2025-06-03T07:51:11.523686Z","iopub.status.idle":"2025-06-03T07:51:11.527516Z","shell.execute_reply.started":"2025-06-03T07:51:11.523671Z","shell.execute_reply":"2025-06-03T07:51:11.526828Z"}},"outputs":[{"name":"stdout","text":"Shape of Input: torch.Size([1, 3, 256, 256])\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"class MultiHeadSelfAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads=8, dropout=0.1, img_size=64):  # Added img_size parameter\n        super().__init__()\n        assert embed_dim % num_heads == 0\n        \n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        self.img_size = img_size  # Store image size\n        \n        # Linear projections for Q, K, V\n        self.qkv = nn.Linear(embed_dim, embed_dim * 3, bias=False)\n        self.proj = nn.Linear(embed_dim, embed_dim)\n        self.dropout = nn.Dropout(dropout)\n        \n        # FIXED: Relative position embeddings for actual image size\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros((2 * img_size - 1) * (2 * img_size - 1), num_heads)\n        )\n        \n        # FIXED: Initialize relative position bias for actual image size\n        coords_h = torch.arange(img_size)\n        coords_w = torch.arange(img_size)\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing='ij'))\n        coords_flatten = torch.flatten(coords, 1)\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n        relative_coords[:, :, 0] += img_size - 1\n        relative_coords[:, :, 1] += img_size - 1\n        relative_coords[:, :, 0] *= 2 * img_size - 1\n        relative_position_index = relative_coords.sum(-1)\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n        \n    def forward(self, x):\n        B, C, H, W = x.shape\n        N = H * W\n        \n        # Reshape to sequence format [B, N, C]\n        x = x.reshape(B, C, N).transpose(1, 2)\n        \n        # Generate Q, K, V\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        \n        # Scaled dot-product attention\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        \n        # Add relative position bias (only if image size matches)\n        if H == self.img_size and W == self.img_size:\n            relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].reshape(\n                H * W, H * W, -1)\n            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n            attn = attn + relative_position_bias.unsqueeze(0)\n        \n        attn = F.softmax(attn, dim=-1)\n        attn = self.dropout(attn)\n        \n        # Apply attention to values\n        x = (attn @ v).transpose(1, 2).reshape(B, N, self.embed_dim)\n        x = self.proj(x)\n        \n        # Reshape back to spatial format [B, C, H, W]\n        x = x.transpose(1, 2).view(B, C, H, W)\n        \n        return x\n\nclass SelfAttentionConv(nn.Module):\n    \"\"\"Self-attention layer that can replace convolutions\"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, num_heads=8):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.num_heads = num_heads\n        \n        # If channels change, use 1x1 conv for projection\n        if in_channels != out_channels:\n            self.channel_proj = nn.Conv2d(in_channels, out_channels, 1, 1, 0)\n        else:\n            self.channel_proj = nn.Identity()\n            \n        # Self-attention components\n        self.attention = MultiHeadSelfAttention(out_channels, num_heads)\n        \n        # Learnable scale parameter for gradual integration\n        self.gamma = nn.Parameter(torch.zeros(1))\n        \n    def forward(self, x):\n        # Project channels if needed\n        identity = self.channel_proj(x)\n        \n        # Apply self-attention\n        attended = self.attention(identity)\n        \n        # Gradual integration with learnable scale\n        out = self.gamma * attended + identity\n        \n        # Handle stride if needed\n        if self.stride > 1:\n            out = F.avg_pool2d(out, kernel_size=self.stride, stride=self.stride)\n            \n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T07:51:11.529797Z","iopub.execute_input":"2025-06-03T07:51:11.530117Z","iopub.status.idle":"2025-06-03T07:51:11.543511Z","shell.execute_reply.started":"2025-06-03T07:51:11.530099Z","shell.execute_reply":"2025-06-03T07:51:11.542857Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class ElectricForceModel(nn.Module):\n\n    def __init__(\n        self,\n        imsize = 256,\n        v0 = 0,\n        layers = 2,\n        loops = 1,\n        debug = False,\n        beta = 0.8,\n        use_self_attention = True\n    ):\n        super().__init__()\n\n        self.register_buffer('v0', torch.tensor([v0], dtype=torch.float32))\n        self.register_buffer('beta', torch.tensor([beta], dtype=torch.float32))\n\n        self.adaptive_epsilon = nn.Parameter(torch.tensor([1.0]))\n        self.adaptive_mass = nn.Parameter(torch.tensor([1.0]))\n        self.adaptive_time = nn.Parameter(torch.tensor([0.1]))\n        \n        with torch.no_grad():\n            self.adaptive_epsilon.data.fill_(1.0)\n            self.adaptive_mass.data.fill_(1.0)\n            self.adaptive_time.data.fill_(0.1)\n\n        self.imsize = imsize\n        self.layers = layers\n        self.loops = loops\n        self.debug = debug\n        self.use_self_attention = use_self_attention\n\n        if self.use_self_attention:\n            self.input_attention = MultiHeadSelfAttention(3, num_heads=3, img_size=imsize)\n            self.attention_scale = nn.Parameter(torch.zeros(1))\n        \n        if self.use_self_attention:\n            self.mapping_to_vector_space = SelfAttentionConv(3, 2, num_heads=2)\n            self.mapping_to_ipt = SelfAttentionConv(2, 3, num_heads=3)\n        else:\n            self.mapping_to_vector_space = nn.Conv2d(3, 2, 1, 1, 0)\n            self.mapping_to_ipt = nn.Conv2d(2, 3, 1, 1, 0)\n            \n        self.space_norm = nn.Tanh()\n        \n\n    def compute_distance(self, ipt, chunk_size=512):\n        \n        \"\"\"\n        Compute the distance of each point with respect to every point in zeta\n\n        Input:\n            - ipt: Input Image | [B, 3, self.imsize, self.imsize]\n\n        Outpt:\n            - zeta: The coordinates map for each pixel in image (x, y) | [B, 2, self.imsize, self.imsize]\n            - x: x-coordinates of every point in zeta | [B, N]\n            - y: y-coordinates of every point in zeta | [B, N]\n            - inverse_sq_dist: 1/r^2, the inverse squared distance of each point with respect to every point in zeta | [B, N, N]\n            - inverse_dist: 1/r, the inverse distance of each point with respect to every point in zeta | [B, N, N]\n            \n        How the Function Works:\n            - Find zeta using a point-wise-convolutional-layer, the number of channel output will be the number of dimensions in vector space\n                            (2 -> (x, y) == Vector Space\n                             3 -> (x, y, z) == 3D Space\n                             ...\n                             ...\n                            )\n            - Find x and y by slicing zeta along the 0th and 1st of the 1st Dimension\n            - Calculate the Euclidean Distance between each point in zeta\n            - inverse_sq_dist is D**(-2) and inverse_dist is D**(-1)\n        \"\"\"\n                \n        zeta = self.mapping_to_vector_space(ipt)\n        zeta = self.space_norm(zeta) # [B, 2, self.imsize, self.imsize]\n        if self.debug:\n            print(f\"zeta Shape: {zeta.shape}\")\n\n        B, C, H, W = zeta.shape\n        N = H*W\n\n        pts = zeta.reshape(B, 2, N) # [B, 2, N]\n        if self.debug:\n            print(f\"pts Shape: {pts.shape}\")\n\n        x, y = pts[:, 0], pts[:, 1] # [B, N], [B, N]\n        if self.debug:\n            print(f\"x Shape: {x.shape}\")\n            print(f\"y Shape: {y.shape}\")\n\n        zflat = pts.permute(0, 2, 1) # [B, N, 2]\n        if self.debug:\n            print(f\"zflat Shape: {zflat.shape}\")\n\n        if N <= chunk_size:\n            D = torch.cdist(zflat, zflat, p=2) # [B, N, N]\n            D = torch.where(D==0, float('inf'), D) # [B, N, N]\n        else:\n            if self.debug:\n                print(f\"Using chunking with chunk_size={chunk_size} for N={N}\")\n        \n            D = torch.zeros(B, N, N, device=zflat.device, dtype=zflat.dtype) # [B, N, N]\n            \n            for i in range(0, N, chunk_size):\n                end_i = min(i + chunk_size, N)\n                chunk_points = zflat[:, i:end_i, :] # [B, chunk_size, 2]\n                distances_chunk = torch.cdist(chunk_points, zflat, p=2) # [B, chunk_size, N]\n\n                diag_mask = torch.zeros_like(distances_chunk, dtype=torch.bool)\n                for idx in range(end_i - i):\n                    diag_mask[:, idx, i + idx] = True\n                    \n                distances_chunk = torch.where(diag_mask, float('inf'), distances_chunk)\n                D[:, i:end_i, :] = distances_chunk # [B, chunk_size, N]\n\n        D = torch.where(D == 0, float('inf'), D)\n        if self.debug:\n            print(f\"D Shape: {D.shape}\") # [B, N, N]\n            \n        #inverse_sq_dist = D.pow(-2) # [B, N, N]\n        #inverse_dist = D.pow(-1) # [B, N, N]\n        #if self.debug:\n            #print(f\"inverse_sq_dist Shape: {inverse_sq_dist.shape}\") # [B, N, N]\n            #print(f\"inverse_dist Shape: {inverse_dist.shape}\") # [B, N, N]\n            \n        if self.debug:\n            print(\"-\"*59)\n        \n        return zeta, x, y, D #inverse_sq_dist, #inverse_dist\n\n    def compute_angle(self, x, y, chunk_size=512):\n        \n        \"\"\"\n        Compute angle of forces acted on each point with respect to the x-axis\n        Input: \n            - x: x-coordinates of every point in zeta | [B, N]\n            - y: y-coordinates of every point in zeta | [B, N]\n\n        Output:\n            - theta_rad: Radian of angle of forces with respect to the x-axis | [B, N, N]\n\n        How the Function Works:\n            - Applying Inverse Function of tan to find the angle | tan = dy/dx\n        \"\"\"\n        \n        B, N = x.shape\n        \n        if self.debug:\n            print(f\"Running compute_angle...\")\n            print(f\" \")\n            \n        if N <= chunk_size:\n            theta_rad = torch.atan2( # [B, N, N]\n                y.unsqueeze(2) - y.unsqueeze(1), # [B, N, 1] - [B, 1, N] -> [B, N, N]\n                x.unsqueeze(2) - x.unsqueeze(1), # [B, N, 1] - [B, 1, N] -> [B, N, N]\n            )\n        else:\n            if self.debug:\n                print(f\"Using chunking for angle computation with chunk_size={chunk_size} for N={N}\")\n            \n            theta_rad = torch.zeros(B, N, N, device=x.device, dtype=x.dtype) # [B, N, N]\n            \n            for i in range(0, N, chunk_size):\n                end_i = min(i + chunk_size, N)\n                \n                x_chunk = x[:, i:end_i] # [B, chunk_size]\n                y_chunk = y[:, i:end_i] # [B, chunk_size]\n                \n                dy = y_chunk.unsqueeze(2) - y.unsqueeze(1) # [B, chunk_size, 1] - [B, 1, N] -> [B, chunk_size, N]\n                dx = x_chunk.unsqueeze(2) - x.unsqueeze(1) # [B, chunk_size, 1] - [B, 1, N] -> [B, chunk_size, N]\n                \n                theta_chunk = torch.atan2(dy, dx) # [B, chunk_size, N]\n                theta_rad[:, i:end_i, :] = theta_chunk # [B, chunk_size, N]\n                \n        theta_deg = theta_rad * (180.0 / math.pi) # [B, N, N]\n        if self.debug:\n            print(f\"theta_rad Shape: {theta_rad.shape}\")\n            print(f\"theta_deg Shape: {theta_deg.shape}\")\n            \n        if self.debug:\n            print(\"-\"*59)\n        return theta_rad\n\n    def compute_electric_force(self, ipt, theta_rad, inverse_sq_dist, chunk_size = 512):\n        \n        \"\"\"\n        Compute the Net Electric Force acted on each point\n        \n        Input:\n            - ipt: Input Image | [B, 3, self.imsize, self.imsize]\n            - theta_rad: The angle of forces acted on each point with respect to the x-axis | [B, N, N]\n            - inverse_sq_dist: 1/r^2, the inverse squared distance of each point with respect to every point in zeta | [B, N, N]\n    \n        Output:\n            - Fx: Force acted on each point in the x-axis | [B, N]\n            - Fy: Force acted on each point in the y-axis | [B, N]\n    \n        How the Function Works:\n            - F_E = qE = qi (k_E sum(qj/r^2))\n                + Since all variables are vectors, we split them into x and y axis, respectively\n        \"\"\"\n        \n        k_E = 1.0 / (4.0 * math.pi *torch.abs(self.adaptive_epsilon))\n        \n        B, C, H, W = ipt.shape\n        N = H*W\n        \n        ipt_reshaped = ipt.reshape(B, N, C) # [B, N, C]\n        \n        if self.debug:\n            print(f\"Running compute_electric_force...\")\n            print(\" \")\n    \n        if N <= chunk_size:\n            # Small enough - use original approach\n            qi_qj = ipt_reshaped.unsqueeze(2) * ipt_reshaped.unsqueeze(1) # [B, N, 1, C] * [B, 1, N, C] -> [B, N, N, C]\n            qi_qj = torch.mean(qi_qj, dim = 3) # [B, N, N]\n            if self.debug:\n                print(f\"qi_qj Shape: {qi_qj.shape}\") # [B, N, N]\n    \n            Fx = k_E * qi_qj * inverse_sq_dist * torch.cos(theta_rad) # [B, N, N]\n            Fy = k_E * qi_qj * inverse_sq_dist * torch.sin(theta_rad) # [B, N, N]\n    \n            Fx = torch.sum(Fx, dim = 2) # [B, N]\n            Fy = torch.sum(Fy, dim = 2) # [B, N]\n        else:\n            # CHUNKED BROADCASTING - avoid creating large [B, N, N, C] tensor\n            if self.debug:\n                print(f\"Using chunked broadcasting for force computation with chunk_size={chunk_size} for N={N}\")\n            \n            Fx = torch.zeros(B, N, device=ipt.device, dtype=ipt.dtype) # [B, N]\n            Fy = torch.zeros(B, N, device=ipt.device, dtype=ipt.dtype) # [B, N]\n            \n            for i in range(0, N, chunk_size):\n                end_i = min(i + chunk_size, N)\n                \n                # Get chunk of qi values\n                qi_chunk = ipt_reshaped[:, i:end_i, :] # [B, chunk_size, C]\n                \n                # Initialize qi_qj chunk for this row chunk\n                qi_qj_chunk = torch.zeros(B, end_i - i, N, device=ipt.device, dtype=ipt.dtype) # [B, chunk_size, N]\n                \n                # CHUNKED COMPUTATION of qi_qj to avoid large broadcasting\n                for j in range(0, N, chunk_size):\n                    end_j = min(j + chunk_size, N)\n                    \n                    # Get chunk of qj values  \n                    qj_chunk = ipt_reshaped[:, j:end_j, :] # [B, chunk_size_j, C]\n                    \n                    # Small broadcasting: [B, chunk_i, 1, C] * [B, 1, chunk_j, C] -> [B, chunk_i, chunk_j, C]\n                    qi_expanded = qi_chunk.unsqueeze(2) # [B, chunk_i, 1, C]\n                    qj_expanded = qj_chunk.unsqueeze(1) # [B, 1, chunk_j, C]\n                    qi_qj_small = qi_expanded * qj_expanded # [B, chunk_i, chunk_j, C]\n                    qi_qj_small = torch.mean(qi_qj_small, dim=3) # [B, chunk_i, chunk_j]\n                    \n                    # Store in the chunk\n                    qi_qj_chunk[:, :, j:end_j] = qi_qj_small # [B, chunk_i, chunk_j]\n                    \n                    # Cleanup small tensors\n                    del qi_expanded, qj_expanded, qi_qj_small\n                \n                # Get corresponding chunks for force computation\n                inverse_sq_dist_chunk = inverse_sq_dist[:, i:end_i, :] # [B, chunk_size, N]\n                theta_rad_chunk = theta_rad[:, i:end_i, :] # [B, chunk_size, N]\n                \n                # Compute forces for this chunk\n                Fx_chunk = k_E * qi_qj_chunk * inverse_sq_dist_chunk * torch.cos(theta_rad_chunk) # [B, chunk_size, N]\n                Fy_chunk = k_E * qi_qj_chunk * inverse_sq_dist_chunk * torch.sin(theta_rad_chunk) # [B, chunk_size, N]\n                \n                # Sum across interactions\n                Fx[:, i:end_i] = torch.sum(Fx_chunk, dim=2) # [B, chunk_size]\n                Fy[:, i:end_i] = torch.sum(Fy_chunk, dim=2) # [B, chunk_size]\n                \n                # Cleanup chunk tensors\n                del qi_chunk, qi_qj_chunk, inverse_sq_dist_chunk, theta_rad_chunk, Fx_chunk, Fy_chunk\n        \n        if self.debug:\n            print(f\"Fx Shape: {Fx.shape}\") # [B, N]\n            print(f\"Fy Shape: {Fy.shape}\") # [B, N]\n            print(\"-\"*59)\n            \n        return Fx, Fy\n\n    def compute_vector_translation(self, Fx, Fy, zeta):\n        \"\"\"\n        Move each point in the vector space corresponding to forces acted on it.\n\n        Input:\n            - Fx: Force acted on each point in the x-axis | [B, N]\n            - Fy: Force acted on each point in the y-axis | [B, N]\n            - zeta: Coordinates of each point (x, y) | [B, 2, self.imsize, self.imsize]\n\n        Output:\n            - new_zeta: Fully updated Coordinates of each point (x, y) | [B, 2, self.imsize, self.imsize]\n\n        How the Function Works:\n            - Displacement = Velocity x time + Initial Position -> D =  vt + x0\n            - v = Acceleration x time + Initial Velocity = at + v0\n            - Acceleration = Force/Mass = F/m\n                + Since all variables are vectors, we split them into x and y axis, respectively\n        \"\"\"\n        \n        if self.debug:\n            print(f\"Starting compute_vector_translation...\")\n            print(\" \")\n            \n        ax = Fx / torch.abs(self.adaptive_mass) # [B, N]\n        ay = Fy / torch.abs(self.adaptive_mass) # [B, N]\n        if self.debug:\n            print(f\"ax Shape: {ax.shape}\")\n            print(f\"ay Shape: {ay.shape}\")\n            \n        B, N = Fx.shape\n        v0 = self.v0.expand(B, N) # [B, N]\n        \n        if self.debug:\n            print(f\"v0 Shape: {v0.shape}\")\n            \n        time_step = torch.abs(self.adaptive_time)\n        vx = v0 + ax * time_step # [B, N]\n        vy = v0 + ay * time_step # [B, N]\n        vx = vx.reshape(B, self.imsize, self.imsize) # [B, self.imsize, self.imsize]\n        vy = vy.reshape(B, self.imsize, self.imsize) # [B, self.imsize, self.imsize]\n        if self.debug:\n            print(f\"vx Shape: {vx.shape}\")\n            print(f\"vy Shape: {vy.shape}\")\n            \n        if self.debug:\n            print(f\"zeta Shape: {zeta.shape}\")\n            print(f\"zeta[:, 0] Shape: {zeta[:, 0].shape}\")\n            print(f\"zeta[:, 1] Shape: {zeta[:, 1].shape}\")\n        new_zeta = zeta.clone()\n        new_zeta[:, 0] = zeta[:, 0] + vx # [B, 2, self.imsize, self.imsize]\n        new_zeta[:, 1] = zeta[:, 1] + vy # [B, 2, self.imsize, self.imsize]\n\n        \n        if self.debug:\n            print(f\"Updated zeta Shape: {new_zeta.shape}\")\n            print(f\"-\"*59)\n            \n        return new_zeta\n\n    def update_input(self, ipt, zeta):\n        \n        \"\"\"\n        Update Input by Unmapping zeta\n\n        Input:\n            - ipt: Input Image | [B, 3, self.imsize, self.imsize]\n            - zeta: Coordinates of each point (x, y) | [B, 2, self.imsize, self.imsize]\n\n        Output:\n            - new_ipt: New Input Image | [B, 3, self.imsize, self.imsize]\n\n        How the Function Works:\n            - You got the new zeta Coordinates\n            - You unmap it using a point-wise-convolutional-layer\n            - New Input will be the summation of the unmapped zeta and the Input Image\n        \"\"\"\n        if self.debug:\n            print(f\"Running update_input...\")\n            \n        map_to_ipt = self.mapping_to_ipt(zeta) # [B, 3, self.imsize, self.imsize]\n        new_ipt = map_to_ipt*self.beta + ipt*(1-self.beta) # [B, 3, self.imsize, self.imsize]\n        return new_ipt\n\n    def compute_energy_of_system(self, ipt, inverse_dist, chunk_size = 512):\n    \n        \"\"\"\n        Compute total Energy of the System\n    \n        Input:\n            - ipt: Final Input Image | [B, 3, self.imsize, self.imsize]\n            - inverse_dist: 1/r, the inverse distance of each point with respect to every point in zeta | [B, N, N]\n        \n        Output:\n            - V: Total Energy of Final Input Image | [B]\n    \n        How the Function Works:\n            - V = sum(qi * Vj) = qi (k_E sum(qj/r^2))\n        \"\"\"\n        \n        k_E = 1.0 / (4.0 * math.pi * torch.abs(self.adaptive_epsilon))\n        \n        B, C, H, W = ipt.shape\n        N = H*W\n        \n        ipt_reshaped = ipt.reshape(B, N, C) # [B, N, C]\n        \n        if self.debug:\n            print(f\"Running compute_energy_of_system...\")\n            print(\" \")\n    \n        if N <= chunk_size:\n            # Small enough - use original approach\n            qi_qj = ipt_reshaped.unsqueeze(2) * ipt_reshaped.unsqueeze(1) # [B, N, 1, C] * [B, 1, N, C] -> [B, N, N, C]\n            qi_qj = torch.mean(qi_qj, dim = 3) # [B, N, N]\n            if self.debug:\n                print(f\"qi_qj Shape: {qi_qj.shape}\") # [B, N, N]\n    \n            V = k_E * qi_qj * inverse_dist # [B, N, N]\n            V = torch.sum(V, dim = 2) # [B, N]\n            V = torch.sum(V, dim = 1) # [B]\n        else:\n            # CHUNKED BROADCASTING - avoid creating large [B, N, N, C] tensor\n            if self.debug:\n                print(f\"Using chunked broadcasting for energy computation with chunk_size={chunk_size} for N={N}\")\n            \n            V = torch.zeros(B, device=ipt.device, dtype=ipt.dtype) # [B]\n            \n            for i in range(0, N, chunk_size):\n                end_i = min(i + chunk_size, N)\n                \n                # Get chunk of qi values\n                qi_chunk = ipt_reshaped[:, i:end_i, :] # [B, chunk_size, C]\n                \n                # Initialize qi_qj chunk for this row chunk\n                qi_qj_chunk = torch.zeros(B, end_i - i, N, device=ipt.device, dtype=ipt.dtype) # [B, chunk_size, N]\n                \n                # CHUNKED COMPUTATION of qi_qj to avoid large broadcasting\n                for j in range(0, N, chunk_size):\n                    end_j = min(j + chunk_size, N)\n                    \n                    # Get chunk of qj values\n                    qj_chunk = ipt_reshaped[:, j:end_j, :] # [B, chunk_size_j, C]\n                    \n                    # Small broadcasting: [B, chunk_i, 1, C] * [B, 1, chunk_j, C] -> [B, chunk_i, chunk_j, C]\n                    qi_expanded = qi_chunk.unsqueeze(2) # [B, chunk_i, 1, C]\n                    qj_expanded = qj_chunk.unsqueeze(1) # [B, 1, chunk_j, C]\n                    qi_qj_small = qi_expanded * qj_expanded # [B, chunk_i, chunk_j, C]\n                    qi_qj_small = torch.mean(qi_qj_small, dim=3) # [B, chunk_i, chunk_j]\n                    \n                    # Store in the chunk\n                    qi_qj_chunk[:, :, j:end_j] = qi_qj_small # [B, chunk_i, chunk_j]\n                    \n                    # Cleanup small tensors\n                    del qi_expanded, qj_expanded, qi_qj_small\n                \n                # Get corresponding distance chunk\n                inverse_dist_chunk = inverse_dist[:, i:end_i, :] # [B, chunk_size, N]\n                \n                # Compute energy for this chunk\n                V_chunk = k_E * qi_qj_chunk * inverse_dist_chunk # [B, chunk_size, N]\n                V_chunk = torch.sum(V_chunk, dim=2) # [B, chunk_size]\n                V_chunk = torch.sum(V_chunk, dim=1) # [B]\n                \n                # Accumulate energy\n                V += V_chunk # [B]\n                \n                # Cleanup chunk tensors\n                del qi_chunk, qi_qj_chunk, inverse_dist_chunk, V_chunk\n                \n        if self.debug:\n            print(f\"V Shape: {V.shape}\") # [B]\n            print(\"-\"*59)\n            \n        return V\n\n    def aggregate_to_one_function(self, ipt, zeta, chunk_size):\n        \n        k_E = 1.0 / (4.0 * math.pi *torch.abs(self.adaptive_epsilon))\n        \n        if self.debug:\n            print(f\"Running aggregate_to_one_function\")\n            print(f\" \")\n            print(f\"ipt Shape: {ipt.shape}\") # [B, 3, self.imsize, self.imsize]\n                \n        zeta = self.mapping_to_vector_space(ipt)\n        zeta = self.space_norm(zeta) # [B, 2, self.imsize, self.imsize]\n        if self.debug:\n            print(f\"zeta Shape: {zeta.shape}\")\n\n        B, C, H, W = zeta.shape\n        N = H*W\n\n        pts = zeta.reshape(B, 2, N) # [B, 2, N]\n        if self.debug:\n            print(f\"pts Shape: {pts.shape}\")\n\n        x, y = pts[:, 0], pts[:, 1] # [B, N], [B, N]\n        if self.debug:\n            print(f\"x Shape: {x.shape}\")\n            print(f\"y Shape: {y.shape}\")\n    \n        zflat = pts.permute(0, 2, 1) # [B, N, 2]\n        if self.debug:\n            print(f\"zflat Shape: {zflat.shape}\")\n            \n        ipt_reshaped = ipt.reshape(B, N, C) # [B, N, C]\n        \n        for i in range(0, N, chunk_size):\n            end_i = min(i + chunk_size, N)\n            chunk_points = zflat[:, i:end_i, :] # [B, chunk_size, 2]\n            distances_chunk = torch.cdist(chunk_points, zflat, p=2) # [B, chunk_size, N]\n\n            diag_mask = torch.zeros_like(distances_chunk, dtype=torch.bool)\n            for idx in range(end_i - i):\n                diag_mask[:, idx, i + idx] = True\n                    \n            distances_chunk = torch.where(diag_mask, float('inf'), distances_chunk)\n                \n            x_chunk = x[:, i:end_i] # [B, chunk_size]\n            y_chunk = y[:, i:end_i] # [B, chunk_size]\n                \n            dy = y_chunk.unsqueeze(2) - y.unsqueeze(1) # [B, chunk_size, 1] - [B, 1, N] -> [B, chunk_size, N]\n            dx = x_chunk.unsqueeze(2) - x.unsqueeze(1) # [B, chunk_size, 1] - [B, 1, N] -> [B, chunk_size, N]\n                \n            theta_chunk = torch.atan2(dy, dx) # [B, chunk_size, N]\n\n            # Get chunk of qi values\n            qi_chunk = ipt_reshaped[:, i:end_i, :] # [B, chunk_size, C]\n                \n            # CHUNKED COMPUTATION of qi_qj to avoid large broadcasting\n            for j in range(0, N, chunk_size):\n                end_j = min(j + chunk_size, N)\n                    \n                # Get chunk of qj values  \n                qj_chunk = ipt_reshaped[:, j:end_j, :] # [B, chunk_size_j, C]\n                    \n                # Small broadcasting: [B, chunk_i, 1, C] * [B, 1, chunk_j, C] -> [B, chunk_i, chunk_j, C]\n                qi_expanded = qi_chunk.unsqueeze(2) # [B, chunk_i, 1, C]\n                qj_expanded = qj_chunk.unsqueeze(1) # [B, 1, chunk_j, C]\n                qi_qj_small = qi_expanded * qj_expanded # [B, chunk_i, chunk_j, C]\n                qi_qj_small = torch.mean(qi_qj_small, dim=3) # [B, chunk_i, chunk_j]\n                    \n                # Store in the chunk\n                qi_qj_chunk[:, :, j:end_j] = qi_qj_small # [B, chunk_i, chunk_j]\n                    \n                # Cleanup small tensors\n                del qi_expanded, qj_expanded, qi_qj_small\n                \n            # Get corresponding chunks for force computation\n            inverse_sq_dist_chunk = distances_chunk # [B, chunk_size, N]\n            theta_rad_chunk = theta_chunk # [B, chunk_size, N]\n                \n            # Compute forces for this chunk\n            Fx_chunk = k_E * qi_qj_chunk * inverse_sq_dist_chunk * torch.cos(theta_rad_chunk) # [B, chunk_size, N]\n            Fy_chunk = k_E * qi_qj_chunk * inverse_sq_dist_chunk * torch.sin(theta_rad_chunk) # [B, chunk_size, N]\n                \n            # Sum across interactions\n            Fx[:, i:end_i] = torch.sum(Fx_chunk, dim=2) # [B, chunk_size]\n            Fy[:, i:end_i] = torch.sum(Fy_chunk, dim=2) # [B, chunk_size]\n                \n            # Cleanup chunk tensors\n            del qi_chunk, qi_qj_chunk, inverse_sq_dist_chunk, theta_rad_chunk, Fx_chunk, Fy_chunk\n            \n        return Fx, Fy\n            \n            \n    def forward(self, ipt, chunk_size=512):\n        assert ipt.shape[2] and ipt.shape[3] == self.imsize, \"Input Shape's Height and Width must match parameter imsize\"\n        for layer in range(self.layers):\n            \n            if self.use_self_attention:\n                attended_ipt = self.input_attention(ipt)\n                ipt = self.attention_scale * attended_ipt + ipt\n            if self.debug:\n                print(f\"After self-attention ipt Shape: {ipt.shape}\")\n                \n            if self.debug:\n                print(f\"Running Layer {layer + 1}/{self.layers}\")\n                print(f\"-\"*59)\n            for loop in range(self.loops):\n                if self.debug:\n                    print(f\"Starting {loop + 1}/{self.loops} for compute_electric_force...\")\n                    print(\" \")\n                #zeta, x, y, D = self.compute_distance(ipt, chunk_size)\n                #theta_rad = self.compute_angle(x, y, chunk_size)\n                #Fx, Fy = self.compute_electric_force(ipt, theta_rad, D.pow(-2), chunk_size)\n                Fx, Fy = self.aggregate_to_one_function(self, ipt, zeta, chunk_size)\n                zeta = self.compute_vector_translation(Fx, Fy, zeta)\n    \n            zeta = self.space_norm(zeta)\n            ipt = self.update_input(ipt, zeta)\n            ipt = self.space_norm(ipt)\n            \n        ipt = self.space_norm(ipt)\n        zeta, _, _, D = self.compute_distance(ipt, chunk_size)\n        V = torch.abs(self.compute_energy_of_system(ipt, D.pow(-1), chunk_size))\n        \n        return ipt, zeta, V","metadata":{"execution":{"iopub.status.busy":"2025-06-03T07:52:20.914843Z","iopub.execute_input":"2025-06-03T07:52:20.915365Z","iopub.status.idle":"2025-06-03T07:52:20.956015Z","shell.execute_reply.started":"2025-06-03T07:52:20.915339Z","shell.execute_reply":"2025-06-03T07:52:20.955324Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"model = ElectricForceModel(debug = True)\nmodel = model.to(DEVICE)\n\nout, zeta, V = model(inpt.to(DEVICE))","metadata":{"execution":{"iopub.status.busy":"2025-06-03T07:52:25.629483Z","iopub.execute_input":"2025-06-03T07:52:25.629762Z","execution_failed":"2025-06-03T07:52:45.850Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Basic tensor to image display\ndef show_tensor_as_image(tensor):\n    # Move to CPU and detach from computational graph\n    tensor = tensor.detach().cpu()\n    \n    # Handle different tensor shapes\n    if tensor.dim() == 4:  # [B, C, H, W]\n        tensor = tensor[0]  # Take first batch\n    \n    if tensor.dim() == 3:  # [C, H, W]\n        if tensor.shape[0] == 1:  # Grayscale [1, H, W]\n            tensor = tensor.squeeze(0)  # Remove channel dim -> [H, W]\n        elif tensor.shape[0] == 3:  # RGB [3, H, W]\n            tensor = tensor.permute(1, 2, 0)  # -> [H, W, 3]\n    \n    # Convert to numpy\n    img = tensor.numpy()\n    \n    # Normalize if values are not in [0, 1] range\n    if img.max() > 1.0 or img.min() < 0.0:\n        img = (img - img.min()) / (img.max() - img.min())\n    \n    # Display\n    plt.figure(figsize=(8, 8))\n    if len(img.shape) == 2:  # Grayscale\n        plt.imshow(img, cmap='gray')\n    else:  # RGB\n        plt.imshow(img)\n    plt.axis('off')\n    plt.show()\n\nshow_tensor_as_image(out)","metadata":{"execution":{"iopub.status.busy":"2025-06-03T07:51:11.592390Z","iopub.status.idle":"2025-06-03T07:51:11.592688Z","shell.execute_reply.started":"2025-06-03T07:51:11.592549Z","shell.execute_reply":"2025-06-03T07:51:11.592564Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def denormalize_image(tensor, title=\"Denormalized Image\"):\n    \"\"\"Denormalize from [-1, 1] back to [0, 1] and display\"\"\"\n    tensor = tensor.detach().cpu()\n    \n    if tensor.dim() == 4:\n        tensor = tensor[0]  # Take first batch\n    \n    # Reverse the normalization: (x - 0.5) / 0.5\n    # So: x_original = (x_normalized * 0.5) + 0.5\n    denormalized = tensor * 0.5 + 0.5\n    \n    # Clamp to valid [0, 1] range\n    denormalized = torch.clamp(denormalized, 0, 1)\n    \n    # Convert to display format [H, W, C]\n    if denormalized.shape[0] == 3:\n        img = denormalized.permute(1, 2, 0)\n    else:\n        img = denormalized\n    \n    plt.figure(figsize=(8, 8))\n    plt.imshow(img.numpy())\n    plt.title(title)\n    plt.axis('off')\n    plt.show()\n    \n    return denormalized\n\ndenormalize_image(out, \"Model Output (Denormalized)\")\ndenormalize_image(inpt, \"Original Input (Denormalized)\")","metadata":{"execution":{"iopub.status.busy":"2025-06-03T07:51:11.593446Z","iopub.status.idle":"2025-06-03T07:51:11.593662Z","shell.execute_reply.started":"2025-06-03T07:51:11.593559Z","shell.execute_reply":"2025-06-03T07:51:11.593569Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"=== ORIGINAL INPUT ===\")\nshow_tensor_as_image(inpt)\ndenormalize_image(inpt, \"Original Input (Properly Denormalized)\")\n\nprint(\"\\n=== MODEL OUTPUT ===\") \nshow_tensor_as_image(out) \ndenormalize_image(out, \"Model Output (Properly Denormalized)\")\n\nprint(\"\\n=== COORDINATE VISUALIZATION ===\")\ndef show_zeta_coordinates(zeta, batch_idx=0):\n    zeta = zeta.detach().cpu()\n    x_coords = zeta[batch_idx, 0]\n    y_coords = zeta[batch_idx, 1]\n    \n    # Denormalize coordinates too\n    x_coords = x_coords * 0.5 + 0.5\n    y_coords = y_coords * 0.5 + 0.5\n    \n    plt.figure(figsize=(15, 5))\n    \n    plt.subplot(1, 3, 1)\n    plt.imshow(x_coords.numpy(), cmap='viridis')\n    plt.title('X Coordinates (Denormalized)')\n    plt.colorbar()\n    \n    plt.subplot(1, 3, 2)\n    plt.imshow(y_coords.numpy(), cmap='plasma')\n    plt.title('Y Coordinates (Denormalized)')\n    plt.colorbar()\n    \n    plt.subplot(1, 3, 3)\n    magnitude = torch.sqrt(x_coords**2 + y_coords**2)\n    plt.imshow(magnitude.numpy(), cmap='hot')\n    plt.title('Coordinate Magnitude')\n    plt.colorbar()\n    \n    plt.tight_layout()\n    plt.show()\n\nshow_zeta_coordinates(zeta)","metadata":{"execution":{"iopub.status.busy":"2025-06-03T07:51:11.594806Z","iopub.status.idle":"2025-06-03T07:51:11.595102Z","shell.execute_reply.started":"2025-06-03T07:51:11.594951Z","shell.execute_reply":"2025-06-03T07:51:11.594967Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Data Prepocessing**","metadata":{}},{"cell_type":"code","source":"class SegmentationDataset(torch.utils.data.Dataset):\n    def __init__(self, image_paths, mask_paths, transforms=None):\n        self.image_paths = image_paths\n        self.mask_paths = mask_paths\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        # Đọc ảnh và mask\n        image = np.array(Image.open(self.image_paths[idx]).convert(\"RGB\"))\n        mask = np.array(Image.open(self.mask_paths[idx]).convert(\"L\"))\n        \n        mask = np.where(mask < 85, 0, np.where(mask < 170, 1, 2))\n        \n        if self.transforms:\n            augmented = self.transforms(image=image, mask=mask)\n            image = augmented['image']\n            mask = augmented['mask']\n            \n        mask = mask.long() if isinstance(mask, torch.Tensor) else torch.from_numpy(mask).long()\n        #print(mask.shape)\n        return image, mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T07:51:11.596414Z","iopub.status.idle":"2025-06-03T07:51:11.596659Z","shell.execute_reply.started":"2025-06-03T07:51:11.596529Z","shell.execute_reply":"2025-06-03T07:51:11.596542Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_multi_iou(outputs, masks, num_classes=3):\n    probs = torch.softmax(outputs, dim=1)\n    preds = torch.argmax(probs, dim=1)\n    iou_per_class = []\n    for cls in range(num_classes):\n        pred_cls = (preds == cls)\n        target_cls = (masks == cls)\n        intersection = (pred_cls & target_cls).sum().float()\n        union = (pred_cls | target_cls).sum().float()\n        iou = (intersection / (union + 1e-8)).item()\n        iou_per_class.append(iou)\n    return np.mean(iou_per_class)\n\ndef compute_dice_score(outputs, masks, num_classes=3):\n    probs = torch.softmax(outputs, dim=1)\n    preds = torch.argmax(probs, dim=1)\n    dice_per_class = []\n    for cls in range(num_classes):\n        pred_cls = (preds == cls)\n        target_cls = (masks == cls)\n        intersection = (pred_cls & target_cls).sum().float()\n        dice = (2. * intersection / (pred_cls.sum() + target_cls.sum() + 1e-8)).item()\n        dice_per_class.append(dice)\n    return np.mean(dice_per_class)\n\ndef compute_precision(outputs, masks, num_classes=3):\n    probs = torch.softmax(outputs, dim=1)\n    preds = torch.argmax(probs, dim=1)\n    precision_per_class = []\n    for cls in range(num_classes):\n        pred_cls = (preds == cls)\n        target_cls = (masks == cls)\n        true_positive = (pred_cls & target_cls).sum().float()\n        predicted_positive = pred_cls.sum().float()\n        precision = (true_positive / (predicted_positive + 1e-8)).item()\n        precision_per_class.append(precision)\n    return np.mean(precision_per_class)\n\ndef compute_accuracy(outputs, masks):\n    probs = torch.softmax(outputs, dim=1)\n    preds = torch.argmax(probs, dim=1)\n    correct = (preds == masks).sum().float()\n    total = torch.numel(masks)\n    return (correct / total).item()\n\n\ndef post_process(mask, kernel_size=3):\n    import cv2\n    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n    \n    cleaned = cv2.morphologyEx(mask.numpy().astype(np.uint8), \n                              cv2.MORPH_OPEN, kernel)\n\n    cleaned = cv2.morphologyEx(cleaned, \n                              cv2.MORPH_CLOSE, kernel)\n    \n    return torch.from_numpy(cleaned)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T07:51:11.597360Z","iopub.status.idle":"2025-06-03T07:51:11.597634Z","shell.execute_reply.started":"2025-06-03T07:51:11.597519Z","shell.execute_reply":"2025-06-03T07:51:11.597533Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Mixup:\n    def __init__(self, alpha=0.4, p=0.5):\n        self.alpha = alpha\n        self.p = p\n        \n    def __call__(self, images, masks):\n        if np.random.rand() < self.p:\n            batch_size = len(images) if isinstance(images, list) else 1\n            if batch_size > 1:\n                indices = np.random.permutation(batch_size)\n                img2, mask2 = images[indices], masks[indices]\n                \n                lam = np.random.beta(self.alpha, self.alpha)\n\n                mixed_img = lam * images + (1 - lam) * img2\n                mixed_mask = lam * masks + (1 - lam) * mask2\n                \n                return mixed_img, mixed_mask\n        return images, masks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T07:51:11.599845Z","iopub.status.idle":"2025-06-03T07:51:11.600136Z","shell.execute_reply.started":"2025-06-03T07:51:11.599984Z","shell.execute_reply":"2025-06-03T07:51:11.600012Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_train_transform():\n    return A.Compose([\n        A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.RandomRotate90(p=0.5),\n        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=15, p=0.5),\n        A.RandomBrightnessContrast(p=0.5),\n        A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n        ToTensorV2(),\n    ])\n\ndef get_valid_transform():\n    return A.Compose([\n        A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n        A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n        ToTensorV2(),\n    ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T07:51:11.600840Z","iopub.status.idle":"2025-06-03T07:51:11.601120Z","shell.execute_reply.started":"2025-06-03T07:51:11.600984Z","shell.execute_reply":"2025-06-03T07:51:11.600998Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Train**","metadata":{}},{"cell_type":"code","source":"def visualize_predictions(model, dataloader, device, num_samples=4):\n    model.eval()\n    images, masks, preds = [], [], []\n    \n    with torch.no_grad():\n        for img, mask in dataloader:\n            img = img.to(device)\n            output = model(img)\n            pred = torch.argmax(torch.softmax(output, dim=1), dim=1)\n            \n            for i in range(min(img.shape[0], num_samples - len(images))):\n                if len(images) < num_samples:\n                    images.append(img[i].cpu())\n                    masks.append(mask[i].cpu())\n                    preds.append(pred[i].cpu())\n            \n            if len(images) >= num_samples:\n                break\n    \n    fig, axes = plt.subplots(num_samples, 3, figsize=(15, num_samples * 5))\n    \n    for i in range(num_samples):\n        img = images[i].permute(1, 2, 0).numpy()\n        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n        img = np.clip(img, 0, 1)\n        \n        axes[i, 0].imshow(img)\n        axes[i, 0].set_title('Original Image')\n        axes[i, 0].axis('off')\n        \n        axes[i, 1].imshow(masks[i], cmap='jet')\n        axes[i, 1].set_title('Ground Truth')\n        axes[i, 1].axis('off')\n        \n        axes[i, 2].imshow(preds[i], cmap='jet')\n        axes[i, 2].set_title('Prediction')\n        axes[i, 2].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    return fig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T07:51:11.602171Z","iopub.status.idle":"2025-06-03T07:51:11.602431Z","shell.execute_reply.started":"2025-06-03T07:51:11.602330Z","shell.execute_reply":"2025-06-03T07:51:11.602340Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_one_epoch(model, dataloader, optimizer, criterion, device, tau):\n    model.train()\n    train_loss = 0.0\n    \n    pbar = tqdm(dataloader, desc='Training')\n    for images, masks in pbar:\n        images = images.to(device)\n        masks = masks.to(device)\n        \n        optimizer.zero_grad()\n        outputs, zeta, V = model(images)\n        loss = criterion(outputs, masks)*(1-tau) + V*tau\n        \n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item()\n        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n    \n    return train_loss / len(dataloader)\n\n# Validation function\ndef validate(model, dataloader, criterion, device, tau):\n    model.eval()\n    val_loss = 0.0\n    iou_scores, dsc_scores, precision_scores, acc_scores = [], [], [], []\n    \n    with torch.no_grad():\n        for images, masks in tqdm(dataloader, desc='Validation'):\n            images = images.to(device)\n            masks = masks.to(device)\n            \n            outputs, zeta, V = model(images)\n            #outputs = convert_model_output_to_segmentation(outputs)\n            loss = criterion(outputs, masks)*(1-tau) + V*tau\n            \n            val_loss += loss.item()\n            \n            iou_scores.append(compute_multi_iou(outputs, masks))\n            dsc_scores.append(compute_dice_score(outputs, masks))\n            precision_scores.append(compute_precision(outputs, masks))\n            acc_scores.append(compute_accuracy(outputs, masks))\n    \n    mean_iou = np.mean(iou_scores)\n    mean_dsc = np.mean(dsc_scores)\n    mean_precision = np.mean(precision_scores)\n    mean_acc = np.mean(acc_scores)\n    \n    return val_loss / len(dataloader), mean_iou, mean_dsc, mean_precision, mean_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T07:51:11.603114Z","iopub.status.idle":"2025-06-03T07:51:11.603350Z","shell.execute_reply.started":"2025-06-03T07:51:11.603241Z","shell.execute_reply":"2025-06-03T07:51:11.603251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs, device, fold=None):\n    best_metrics = {'mean_iou': 0.0, 'mean_dsc': 0.0}\n    history = {\n        'train_loss': [], 'val_loss': [],\n        'iou': [], 'dsc': [], 'precision': [], 'accuracy': []\n    }\n    \n    for epoch in range(num_epochs):\n        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, tau = 0.2)\n        history['train_loss'].append(train_loss)\n        \n        val_loss, mean_iou, mean_dsc, mean_precision, mean_acc = validate(model, val_loader, criterion, device, tau = 0.2)\n        history['val_loss'].append(val_loss)\n        history['iou'].append(mean_iou)\n        history['dsc'].append(mean_dsc)\n        history['precision'].append(mean_precision)\n        history['accuracy'].append(mean_acc)\n        \n        scheduler.step(mean_dsc)\n        \n        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n        print(f\"Accuracy: {mean_acc:.4f}, Mean IoU: {mean_iou:.4f}, Mean DSC/F1: {mean_dsc:.4f}, Mean Precision: {mean_precision:.4f}\")\n        \n        if mean_dsc > best_metrics['mean_dsc']:\n            best_metrics['mean_dsc'] = mean_dsc\n            best_metrics['mean_iou'] = mean_iou\n            best_metrics['mean_precision'] = mean_precision\n            best_metrics['mean_acc'] = mean_acc\n            \n            model_name = f\"best_model_fold{fold}.pth\" if fold is not None else \"best_model.pth\"\n            torch.save(model.state_dict(), model_name)\n            print(f\"Saved best model with DSC: {mean_dsc:.4f}\")\n        \n        print(f\"Best DSC: {best_metrics['mean_dsc']:.4f}, Best IoU: {best_metrics['mean_iou']:.4f}\")\n        print(\"-\" * 50)\n    \n    return model, history, best_metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T07:51:11.604699Z","iopub.status.idle":"2025-06-03T07:51:11.605005Z","shell.execute_reply.started":"2025-06-03T07:51:11.604848Z","shell.execute_reply":"2025-06-03T07:51:11.604861Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_optimal_k(all_images, all_masks, k_values=[3, 5, 7, 10], num_epochs=10, tau = 0.2):\n\n    k_scores = []\n    k_variances = []\n    train_times = []\n    \n    for k in k_values:\n        fold_scores = []\n        \n        kf = KFold(n_splits=k, shuffle=True, random_state=SEED)\n        start_time = time.time()\n        \n        for fold, (train_idx, val_idx) in enumerate(kf.split(all_images)):\n            print(f\"  Fold {fold+1}/{k}\")\n            \n            train_img = [all_images[i] for i in train_idx]\n            train_mask = [all_masks[i] for i in train_idx]\n            val_img = [all_images[i] for i in val_idx]\n            val_mask = [all_masks[i] for i in val_idx]\n            \n            # Tạo dataset\n            train_dataset = SegmentationDataset(train_img, train_mask, get_train_transform())\n            val_dataset = SegmentationDataset(val_img, val_mask, get_valid_transform())\n            \n            # Tạo dataloader\n            train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n            \n            # Khởi tạo model mới cho mỗi fold\n            model = ElectricForceModel().to(DEVICE)\n            \n            # Setup optimizer và loss\n            optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n            criterion = nn.CrossEntropyLoss()\n            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3)\n            \n            best_metrics = {'mean_dsc': 0.0}\n            \n            for epoch in range(num_epochs):\n                model.train()\n                for images, masks in train_loader:\n                    images, masks = images.to(DEVICE), masks.to(DEVICE)\n                    optimizer.zero_grad()\n                    outputs, zeta, V = model(images)\n                    #outputs = convert_model_output_to_segmentation(outputs)\n                    loss = criterion(outputs, masks)*(1-tau) + V*tau\n                    loss.backward()\n                    optimizer.step()\n                \n                model.eval()\n                dsc_scores = []\n                with torch.no_grad():\n                    for images, masks in val_loader:\n                        images, masks = images.to(DEVICE), masks.to(DEVICE)\n                        outputs, zeta, V = model(images)\n                        #outputs = conconvert_model_output_to_segmentation(outputs)\n                        dsc_scores.append(compute_dice_score(outputs, masks))\n                \n                mean_dsc = np.mean(dsc_scores)\n                scheduler.step(mean_dsc)\n                \n                if mean_dsc > best_metrics['mean_dsc']:\n                    best_metrics['mean_dsc'] = mean_dsc\n            \n            # Lưu Dice score của fold này\n            fold_scores.append(best_metrics['mean_dsc'])\n            print(f\"    Dice score: {best_metrics['mean_dsc']:.4f}\")\n        \n        train_time = time.time() - start_time\n        train_times.append(train_time)\n        \n        mean_score = np.mean(fold_scores)\n        score_variance = np.var(fold_scores)\n        \n        k_scores.append(mean_score)\n        k_variances.append(score_variance)\n        \n        print(f\"K={k}, Điểm trung bình: {mean_score:.4f}, Phương sai: {score_variance:.6f}\")\n        print(f\"Thời gian training: {train_time:.2f} giây\")\n    \n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n    \n    ax1.plot(k_values, k_scores, 'o-', linewidth=2, color='blue')\n    ax1.set_xlabel('Số lượng Fold (K)')\n    ax1.set_ylabel('Điểm DSC trung bình')\n    ax1.set_title('Hiệu suất theo giá trị K')\n    ax1.grid(True)\n    \n    # Vẽ phương sai\n    ax2.plot(k_values, k_variances, 'o-', linewidth=2, color='orange')\n    ax2.set_xlabel('Số lượng Fold (K)')\n    ax2.set_ylabel('Phương sai')\n    ax2.set_title('Phương sai theo giá trị K')\n    ax2.grid(True)\n    \n    # Vẽ thời gian training\n    ax3.plot(k_values, train_times, 'o-', linewidth=2, color='green')\n    ax3.set_xlabel('Số lượng Fold (K)')\n    ax3.set_ylabel('Thời gian training (giây)')\n    ax3.set_title('Thời gian training theo giá trị K')\n    ax3.grid(True)\n    \n    plt.tight_layout()\n    plt.savefig('k_fold_elbow_analysis.png')\n    plt.show()\n    \n    optimal_k = find_elbow_point(k_values, k_scores, k_variances)\n\n    \n    return optimal_k\n\ndef find_elbow_point(k_values, k_scores, k_variances):\n    \"\"\"\n    Xác định điểm elbow dựa trên phần trăm cải thiện và phương sai\n    \"\"\"\n    \n    improvements = []\n    for i in range(1, len(k_values)):\n        improvement = (k_scores[i] - k_scores[i-1]) / k_scores[i-1] * 100\n        improvements.append(improvement)\n    \n    for i in range(len(improvements)):\n        if improvements[i] < 1.0:\n            return k_values[i+1]\n    \n    var_index = np.argmin(k_variances)\n    return k_values[var_index]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T07:51:11.606165Z","iopub.status.idle":"2025-06-03T07:51:11.606490Z","shell.execute_reply.started":"2025-06-03T07:51:11.606336Z","shell.execute_reply":"2025-06-03T07:51:11.606351Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_training():\n    print(f\"Using device: {DEVICE}\")\n    \n    root_dir = Path('/kaggle/input/data-1/content')\n    all_images = sorted(list((root_dir / 'images').glob('*.bmp')))\n    all_masks = sorted(list((root_dir / 'masks').glob('*.png')))\n    \n    all_images = sorted(all_images, key=lambda x: x.stem)\n    all_masks = sorted(all_masks, key=lambda x: x.stem)\n    \n    print(f\"Total images: {len(all_images)}\")\n    print(f\"Total masks: {len(all_masks)}\")\n    \n    optimal_k = find_optimal_k(\n        all_images, all_masks, \n        k_values=[3, 5, 7, 10], \n        num_epochs=10\n    )\n    \n    kf = KFold(n_splits=optimal_k, shuffle=True, random_state=SEED)\n    fold_metrics = []\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(all_images)):\n        print(f\"Training Fold {fold+1}/{optimal_k}\")\n\n        train_img = [all_images[i] for i in train_idx]\n        train_mask = [all_masks[i] for i in train_idx]\n        val_img = [all_images[i] for i in val_idx]\n        val_mask = [all_masks[i] for i in val_idx]\n        \n        # Create datasets\n        train_dataset = SegmentationDataset(train_img, train_mask, get_train_transform())\n        val_dataset = SegmentationDataset(val_img, val_mask, get_valid_transform())\n        \n        # Create dataloaders\n        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n        \n        # Initialize model\n        model = ElectricForceModel().to(DEVICE)\n        \n        # Setup optimizer and loss\n        optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n        criterion = nn.CrossEntropyLoss()\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=5, factor=0.5)\n        \n        # Train model\n        model, history, best_metrics = train_model(\n            model, \n            train_loader, \n            val_loader,\n            optimizer,\n            criterion,\n            scheduler,\n            NUM_EPOCHS,\n            DEVICE,\n            fold=fold\n        )\n        \n        # Save metrics\n        fold_metrics.append(best_metrics)\n        \n        # Visualize predictions\n        visualize_predictions(model, val_loader, DEVICE)\n        \n        # Plot learning curves\n        plt.figure(figsize=(12, 8))\n        \n        plt.subplot(2, 2, 1)\n        plt.plot(history['train_loss'], label='Train Loss')\n        plt.plot(history['val_loss'], label='Validation Loss')\n        plt.legend()\n        plt.title('Loss Curves')\n        \n        plt.subplot(2, 2, 2)\n        plt.plot(history['iou'], label='IoU')\n        plt.plot(history['dsc'], label='DSC/F1')\n        plt.legend()\n        plt.title('IoU and DSC Curves')\n        \n        plt.subplot(2, 2, 3)\n        plt.plot(history['precision'], label='Precision')\n        plt.legend()\n        plt.title('Precision Curve')\n        \n        plt.subplot(2, 2, 4)\n        plt.plot(history['accuracy'], label='Accuracy')\n        plt.legend()\n        plt.title('Accuracy Curve')\n        \n        plt.tight_layout()\n        plt.savefig(f'learning_curves_fold{fold}.png')\n        plt.show()\n    \n    # Print overall metrics\n    print(\"\\nAverage metrics across all folds:\")\n    avg_iou = np.mean([metrics['mean_iou'] for metrics in fold_metrics])\n    avg_dsc = np.mean([metrics['mean_dsc'] for metrics in fold_metrics])\n    avg_precision = np.mean([metrics['mean_precision'] for metrics in fold_metrics])\n    avg_acc = np.mean([metrics['mean_acc'] for metrics in fold_metrics])\n    \n    print(f\"Avg IoU: {avg_iou:.4f}\")\n    print(f\"Avg DSC/F1: {avg_dsc:.4f}\")\n    print(f\"Avg Precision: {avg_precision:.4f}\")\n    print(f\"Avg Accuracy: {avg_acc:.4f}\")\n\n# Run the training\nif __name__ == \"__main__\":\n    run_training()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T07:51:11.607285Z","iopub.status.idle":"2025-06-03T07:51:11.607584Z","shell.execute_reply.started":"2025-06-03T07:51:11.607434Z","shell.execute_reply":"2025-06-03T07:51:11.607447Z"},"scrolled":true},"outputs":[],"execution_count":null}]}