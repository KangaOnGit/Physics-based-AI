{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86023,"databundleVersionId":11376393,"sourceType":"competition"},{"sourceId":11643754,"sourceType":"datasetVersion","datasetId":7306477},{"sourceId":11681844,"sourceType":"datasetVersion","datasetId":7331802}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"!pip install albumentations\n!pip install segmentation-models-pytorch","metadata":{"execution":{"iopub.status.busy":"2025-06-08T07:47:49.221566Z","iopub.execute_input":"2025-06-08T07:47:49.221805Z"},"scrolled":true}},{"cell_type":"code","source":"# Standard library\nimport os\nimport gc\nimport time\nimport math\nimport random\nimport logging\nimport warnings\n#logging.basicConfig(level=logging.ERROR)\n\n# Scientific computing\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\n\n# Deep learning (PyTorch)\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.parametrizations import weight_norm\nfrom sklearn.metrics import pairwise_distances\n\n# Computer vision\nimport cv2\nimport torchvision\nfrom PIL import Image\nfrom torchvision import transforms\n\n# Miscellaneous utilities\nfrom tqdm import tqdm\nimport timm\nimport einops\nfrom einops import rearrange\nfrom sklearn.model_selection import KFold\nfrom pathlib import Path\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport kagglehub","metadata":{"execution":{"iopub.status.busy":"2025-06-08T07:51:47.070427Z","iopub.execute_input":"2025-06-08T07:51:47.071024Z","iopub.status.idle":"2025-06-08T07:52:33.651426Z","shell.execute_reply.started":"2025-06-08T07:51:47.070996Z","shell.execute_reply":"2025-06-08T07:52:33.650820Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/albumentations/check_version.py:107: UserWarning: Error fetching version info <urlopen error [Errno -3] Temporary failure in name resolution>\n  data = fetch_version_info()\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n#DEVICE = torch.device('cpu')\nNUM_EPOCHS = 5\nBATCH_SIZE = 5\nLEARNING_RATE = 0.001\nIMAGE_SIZE = 224\nNUM_CLASSES = 3\nSEED = 42\nN_FOLDS = 5","metadata":{"execution":{"iopub.status.busy":"2025-06-08T07:52:33.652431Z","iopub.execute_input":"2025-06-08T07:52:33.652825Z","iopub.status.idle":"2025-06-08T07:52:33.764370Z","shell.execute_reply.started":"2025-06-08T07:52:33.652807Z","shell.execute_reply":"2025-06-08T07:52:33.763715Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T07:52:33.764967Z","iopub.execute_input":"2025-06-08T07:52:33.765153Z","iopub.status.idle":"2025-06-08T07:52:33.777924Z","shell.execute_reply.started":"2025-06-08T07:52:33.765138Z","shell.execute_reply":"2025-06-08T07:52:33.777246Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"path = kagglehub.dataset_download(\"khanhpt1999/data-1\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"execution":{"iopub.status.busy":"2025-06-08T07:52:33.779035Z","iopub.execute_input":"2025-06-08T07:52:33.779336Z","iopub.status.idle":"2025-06-08T07:52:33.912969Z","shell.execute_reply.started":"2025-06-08T07:52:33.779321Z","shell.execute_reply":"2025-06-08T07:52:33.912454Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/data-1\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"img = Image.open(\"/kaggle/input/test-images/image_2025-05-05_142003010.png\").convert(\"RGB\")\nprint(\"PIL image size:\", img.size, \"mode:\", img.mode)\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),                 \n    transforms.ToTensor(),                \n    transforms.Normalize( \n        mean=[0.5,0.5,0.5],\n        std=[0.5,0.5,0.5]\n    ),\n])\n\ninpt: torch.Tensor = transform(img)\nprint(\"Tensor shape:\", inpt.shape, \"dtype:\", inpt.dtype, \"range:\", inpt.min(),\"→\",inpt.max())","metadata":{"execution":{"iopub.status.busy":"2025-06-08T07:06:49.674410Z","iopub.execute_input":"2025-06-08T07:06:49.674600Z","iopub.status.idle":"2025-06-08T07:06:49.785712Z","shell.execute_reply.started":"2025-06-08T07:06:49.674585Z","shell.execute_reply":"2025-06-08T07:06:49.785198Z"}}},{"cell_type":"markdown","source":"inpt = inpt.unsqueeze(0)\nprint(f\"Shape of Input: {inpt.shape}\")","metadata":{"execution":{"iopub.status.busy":"2025-06-08T07:06:49.786294Z","iopub.execute_input":"2025-06-08T07:06:49.786479Z","iopub.status.idle":"2025-06-08T07:06:49.789768Z","shell.execute_reply.started":"2025-06-08T07:06:49.786465Z","shell.execute_reply":"2025-06-08T07:06:49.789291Z"}}},{"cell_type":"code","source":"class MultiHeadSelfAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads=8, dropout=0.1, img_size=64):  # Added img_size parameter\n        super().__init__()\n        assert embed_dim % num_heads == 0\n        \n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        self.img_size = img_size  # Store image size\n        \n        # Linear projections for Q, K, V\n        self.qkv = nn.Linear(embed_dim, embed_dim * 3, bias=False)\n        self.proj = nn.Linear(embed_dim, embed_dim)\n        self.dropout = nn.Dropout(dropout)\n        \n        # FIXED: Relative position embeddings for actual image size\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros((2 * img_size - 1) * (2 * img_size - 1), num_heads)\n        )\n        \n        # FIXED: Initialize relative position bias for actual image size\n        coords_h = torch.arange(img_size)\n        coords_w = torch.arange(img_size)\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing='ij'))\n        coords_flatten = torch.flatten(coords, 1)\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n        relative_coords[:, :, 0] += img_size - 1\n        relative_coords[:, :, 1] += img_size - 1\n        relative_coords[:, :, 0] *= 2 * img_size - 1\n        relative_position_index = relative_coords.sum(-1)\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n        \n    def forward(self, x):\n        B, C, H, W = x.shape\n        N = H * W\n        \n        # Reshape to sequence format [B, N, C]\n        x = x.reshape(B, C, N).transpose(1, 2)\n        \n        # Generate Q, K, V\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        \n        # Scaled dot-product attention\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        \n        # Add relative position bias (only if image size matches)\n        if H == self.img_size and W == self.img_size:\n            relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].reshape(\n                H * W, H * W, -1)\n            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n            attn = attn + relative_position_bias.unsqueeze(0)\n        \n        attn = F.softmax(attn, dim=-1)\n        attn = self.dropout(attn)\n        \n        # Apply attention to values\n        x = (attn @ v).transpose(1, 2).reshape(B, N, self.embed_dim)\n        x = self.proj(x)\n        \n        # Reshape back to spatial format [B, C, H, W]\n        x = x.transpose(1, 2).view(B, C, H, W)\n        \n        return x\n\nclass SelfAttentionConv(nn.Module):\n    \"\"\"Self-attention layer that can replace convolutions\"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, num_heads=8):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.num_heads = num_heads\n        \n        # If channels change, use 1x1 conv for projection\n        if in_channels != out_channels:\n            self.channel_proj = nn.Conv2d(in_channels, out_channels, 1, 1, 0)\n        else:\n            self.channel_proj = nn.Identity()\n            \n        # Self-attention components\n        self.attention = MultiHeadSelfAttention(out_channels, num_heads)\n        \n        # Learnable scale parameter for gradual integration\n        self.gamma = nn.Parameter(torch.zeros(1))\n        \n    def forward(self, x):\n        # Project channels if needed\n        identity = self.channel_proj(x)\n        \n        # Apply self-attention\n        attended = self.attention(identity)\n        \n        # Gradual integration with learnable scale\n        out = self.gamma * attended + identity\n        \n        # Handle stride if needed\n        if self.stride > 1:\n            out = F.avg_pool2d(out, kernel_size=self.stride, stride=self.stride)\n            \n        return out","metadata":{"execution":{"iopub.status.busy":"2025-06-08T07:52:33.913473Z","iopub.execute_input":"2025-06-08T07:52:33.913635Z","iopub.status.idle":"2025-06-08T07:52:33.925775Z","shell.execute_reply.started":"2025-06-08T07:52:33.913621Z","shell.execute_reply":"2025-06-08T07:52:33.925166Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class ElectricForceModel(nn.Module):\n\n    def __init__(\n        self,\n        imsize = 224,\n        v0 = 0,\n        layers = 3,\n        loops = 3,\n        debug = False,\n        beta = 0.8,\n        use_self_attention = False\n    ):\n        super().__init__()\n\n        self.register_buffer('v0', torch.tensor([v0], dtype=torch.float32))\n        self.register_buffer('beta', torch.tensor([beta], dtype=torch.float32))\n\n        self.adaptive_epsilon = nn.Parameter(torch.tensor([1.0]))\n        self.adaptive_mass = nn.Parameter(torch.tensor([1.0]))\n        self.adaptive_time = nn.Parameter(torch.tensor([0.1]))\n        \n        with torch.no_grad():\n            self.adaptive_epsilon.data.fill_(1.0)\n            self.adaptive_mass.data.fill_(1.0)\n            self.adaptive_time.data.fill_(0.1)\n\n        self.imsize = imsize\n        self.layers = layers\n        self.loops = loops\n        self.debug = debug\n        self.use_self_attention = use_self_attention\n\n        if self.use_self_attention:\n            self.input_attention = MultiHeadSelfAttention(3, num_heads=3, img_size=imsize)\n            self.attention_scale = nn.Parameter(torch.zeros(1))\n        \n        if self.use_self_attention:\n            self.mapping_to_vector_space = SelfAttentionConv(3, 2, num_heads=2)\n            self.mapping_to_ipt = SelfAttentionConv(2, 3, num_heads=3)\n        else:\n            self.mapping_to_vector_space = nn.Conv2d(3, 2, 1, 1, 0)\n            self.mapping_to_ipt = nn.Conv2d(2, 3, 1, 1, 0)\n            \n        self.space_norm = nn.Tanh()\n        self.conv1 = nn.Conv2d(3, 3, 1, 1, 0)\n        self.conv2 = nn.Conv2d(3, 3, 1, 1, 0)\n         \n\n    def compute_distance(self, ipt, chunk_size=512):\n        \n        \"\"\"\n        Compute the distance of each point with respect to every point in zeta\n\n        Input:\n            - ipt: Input Image | [B, 3, self.imsize, self.imsize]\n\n        Outpt:\n            - zeta: The coordinates map for each pixel in image (x, y) | [B, 2, self.imsize, self.imsize]\n            - x: x-coordinates of every point in zeta | [B, N]\n            - y: y-coordinates of every point in zeta | [B, N]\n            - inverse_sq_dist: 1/r^2, the inverse squared distance of each point with respect to every point in zeta | [B, N, N]\n            - inverse_dist: 1/r, the inverse distance of each point with respect to every point in zeta | [B, N, N]\n            \n        How the Function Works:\n            - Find zeta using a point-wise-convolutional-layer, the number of channel output will be the number of dimensions in vector space\n                            (2 -> (x, y) == Vector Space\n                             3 -> (x, y, z) == 3D Space\n                             ...\n                             ...\n                            )\n            - Find x and y by slicing zeta along the 0th and 1st of the 1st Dimension\n            - Calculate the Euclidean Distance between each point in zeta\n            - inverse_sq_dist is D**(-2) and inverse_dist is D**(-1)\n        \"\"\"\n                \n        zeta = self.mapping_to_vector_space(ipt)\n        zeta = self.space_norm(zeta) # [B, 2, self.imsize, self.imsize]\n        if self.debug:\n            print(f\"zeta Shape: {zeta.shape}\")\n\n        B, C, H, W = zeta.shape\n        N = H*W\n\n        pts = zeta.reshape(B, 2, N) # [B, 2, N]\n        if self.debug:\n            print(f\"pts Shape: {pts.shape}\")\n\n        x, y = pts[:, 0], pts[:, 1] # [B, N], [B, N]\n        if self.debug:\n            print(f\"x Shape: {x.shape}\")\n            print(f\"y Shape: {y.shape}\")\n\n        zflat = pts.permute(0, 2, 1) # [B, N, 2]\n        if self.debug:\n            print(f\"zflat Shape: {zflat.shape}\")\n\n        if N <= chunk_size:\n            D = torch.cdist(zflat, zflat, p=2) # [B, N, N]\n            D = torch.where(D==0, float('inf'), D) # [B, N, N]\n        else:\n            if self.debug:\n                print(f\"Using chunking with chunk_size={chunk_size} for N={N}\")\n        \n            D = torch.zeros(B, N, N, device=zflat.device, dtype=zflat.dtype) # [B, N, N]\n            \n            for i in range(0, N, chunk_size):\n                end_i = min(i + chunk_size, N)\n                chunk_points = zflat[:, i:end_i, :] # [B, chunk_size, 2]\n                distances_chunk = torch.cdist(chunk_points, zflat, p=2) # [B, chunk_size, N]\n\n                diag_mask = torch.zeros_like(distances_chunk, dtype=torch.bool)\n                for idx in range(end_i - i):\n                    diag_mask[:, idx, i + idx] = True\n                    \n                distances_chunk = torch.where(diag_mask, float('inf'), distances_chunk)\n                D[:, i:end_i, :] = distances_chunk # [B, chunk_size, N]\n\n        D = torch.where(D == 0, float('inf'), D)\n        if self.debug:\n            print(f\"D Shape: {D.shape}\") # [B, N, N]\n            \n        #inverse_sq_dist = D.pow(-2) # [B, N, N]\n        #inverse_dist = D.pow(-1) # [B, N, N]\n        #if self.debug:\n            #print(f\"inverse_sq_dist Shape: {inverse_sq_dist.shape}\") # [B, N, N]\n            #print(f\"inverse_dist Shape: {inverse_dist.shape}\") # [B, N, N]\n            \n        if self.debug:\n            print(\"-\"*59)\n        \n        return zeta, x, y, D #inverse_sq_dist, #inverse_dist\n\n    def compute_angle(self, x, y, chunk_size=512):\n        \n        \"\"\"\n        Compute angle of forces acted on each point with respect to the x-axis\n        Input: \n            - x: x-coordinates of every point in zeta | [B, N]\n            - y: y-coordinates of every point in zeta | [B, N]\n\n        Output:\n            - theta_rad: Radian of angle of forces with respect to the x-axis | [B, N, N]\n\n        How the Function Works:\n            - Applying Inverse Function of tan to find the angle | tan = dy/dx\n        \"\"\"\n        \n        B, N = x.shape\n        \n        if self.debug:\n            print(f\"Running compute_angle...\")\n            print(f\" \")\n            \n        if N <= chunk_size:\n            theta_rad = torch.atan2( # [B, N, N]\n                y.unsqueeze(2) - y.unsqueeze(1), # [B, N, 1] - [B, 1, N] -> [B, N, N]\n                x.unsqueeze(2) - x.unsqueeze(1), # [B, N, 1] - [B, 1, N] -> [B, N, N]\n            )\n        else:\n            if self.debug:\n                print(f\"Using chunking for angle computation with chunk_size={chunk_size} for N={N}\")\n            \n            theta_rad = torch.zeros(B, N, N, device=x.device, dtype=x.dtype) # [B, N, N]\n            \n            for i in range(0, N, chunk_size):\n                end_i = min(i + chunk_size, N)\n                \n                x_chunk = x[:, i:end_i] # [B, chunk_size]\n                y_chunk = y[:, i:end_i] # [B, chunk_size]\n                \n                dy = y_chunk.unsqueeze(2) - y.unsqueeze(1) # [B, chunk_size, 1] - [B, 1, N] -> [B, chunk_size, N]\n                dx = x_chunk.unsqueeze(2) - x.unsqueeze(1) # [B, chunk_size, 1] - [B, 1, N] -> [B, chunk_size, N]\n                \n                theta_chunk = torch.atan2(dy, dx) # [B, chunk_size, N]\n                theta_rad[:, i:end_i, :] = theta_chunk # [B, chunk_size, N]\n                \n        theta_deg = theta_rad * (180.0 / math.pi) # [B, N, N]\n        if self.debug:\n            print(f\"theta_rad Shape: {theta_rad.shape}\")\n            print(f\"theta_deg Shape: {theta_deg.shape}\")\n            \n        if self.debug:\n            print(\"-\"*59)\n        return theta_rad\n\n    def compute_electric_force(self, ipt, theta_rad, inverse_sq_dist, chunk_size = 512):\n        \n        \"\"\"\n        Compute the Net Electric Force acted on each point\n        \n        Input:\n            - ipt: Input Image | [B, 3, self.imsize, self.imsize]\n            - theta_rad: The angle of forces acted on each point with respect to the x-axis | [B, N, N]\n            - inverse_sq_dist: 1/r^2, the inverse squared distance of each point with respect to every point in zeta | [B, N, N]\n    \n        Output:\n            - Fx: Force acted on each point in the x-axis | [B, N]\n            - Fy: Force acted on each point in the y-axis | [B, N]\n    \n        How the Function Works:\n            - F_E = qE = qi (k_E sum(qj/r^2))\n                + Since all variables are vectors, we split them into x and y axis, respectively\n        \"\"\"\n        \n        k_E = 1.0 / (4.0 * math.pi *torch.abs(self.adaptive_epsilon))\n        \n        B, C, H, W = ipt.shape\n        N = H*W\n        \n        ipt_reshaped = ipt.reshape(B, N, C) # [B, N, C]\n        \n        if self.debug:\n            print(f\"Running compute_electric_force...\")\n            print(\" \")\n    \n        if N <= chunk_size:\n            # Small enough - use original approach\n            qi_qj = ipt_reshaped.unsqueeze(2) * ipt_reshaped.unsqueeze(1) # [B, N, 1, C] * [B, 1, N, C] -> [B, N, N, C]\n            qi_qj = torch.mean(qi_qj, dim = 3) # [B, N, N]\n            if self.debug:\n                print(f\"qi_qj Shape: {qi_qj.shape}\") # [B, N, N]\n    \n            Fx = k_E * qi_qj * inverse_sq_dist * torch.cos(theta_rad) # [B, N, N]\n            Fy = k_E * qi_qj * inverse_sq_dist * torch.sin(theta_rad) # [B, N, N]\n    \n            Fx = torch.sum(Fx, dim = 2) # [B, N]\n            Fy = torch.sum(Fy, dim = 2) # [B, N]\n        else:\n            # CHUNKED BROADCASTING - avoid creating large [B, N, N, C] tensor\n            if self.debug:\n                print(f\"Using chunked broadcasting for force computation with chunk_size={chunk_size} for N={N}\")\n            \n            Fx = torch.zeros(B, N, device=ipt.device, dtype=ipt.dtype) # [B, N]\n            Fy = torch.zeros(B, N, device=ipt.device, dtype=ipt.dtype) # [B, N]\n            \n            for i in range(0, N, chunk_size):\n                end_i = min(i + chunk_size, N)\n                \n                # Get chunk of qi values\n                qi_chunk = ipt_reshaped[:, i:end_i, :] # [B, chunk_size, C]\n                \n                # Initialize qi_qj chunk for this row chunk\n                qi_qj_chunk = torch.zeros(B, end_i - i, N, device=ipt.device, dtype=ipt.dtype) # [B, chunk_size, N]\n                \n                # CHUNKED COMPUTATION of qi_qj to avoid large broadcasting\n                for j in range(0, N, chunk_size):\n                    end_j = min(j + chunk_size, N)\n                    \n                    # Get chunk of qj values  \n                    qj_chunk = ipt_reshaped[:, j:end_j, :] # [B, chunk_size_j, C]\n                    \n                    # Small broadcasting: [B, chunk_i, 1, C] * [B, 1, chunk_j, C] -> [B, chunk_i, chunk_j, C]\n                    qi_expanded = qi_chunk.unsqueeze(2) # [B, chunk_i, 1, C]\n                    qj_expanded = qj_chunk.unsqueeze(1) # [B, 1, chunk_j, C]\n                    qi_qj_small = qi_expanded * qj_expanded # [B, chunk_i, chunk_j, C]\n                    qi_qj_small = torch.mean(qi_qj_small, dim=3) # [B, chunk_i, chunk_j]\n                    \n                    # Store in the chunk\n                    qi_qj_chunk[:, :, j:end_j] = qi_qj_small # [B, chunk_i, chunk_j]\n                    \n                    # Cleanup small tensors\n                    del qi_expanded, qj_expanded, qi_qj_small\n                \n                # Get corresponding chunks for force computation\n                inverse_sq_dist_chunk = inverse_sq_dist[:, i:end_i, :] # [B, chunk_size, N]\n                theta_rad_chunk = theta_rad[:, i:end_i, :] # [B, chunk_size, N]\n                \n                # Compute forces for this chunk\n                Fx_chunk = k_E * qi_qj_chunk * inverse_sq_dist_chunk * torch.cos(theta_rad_chunk) # [B, chunk_size, N]\n                Fy_chunk = k_E * qi_qj_chunk * inverse_sq_dist_chunk * torch.sin(theta_rad_chunk) # [B, chunk_size, N]\n                \n                # Sum across interactions\n                Fx[:, i:end_i] = torch.sum(Fx_chunk, dim=2) # [B, chunk_size]\n                Fy[:, i:end_i] = torch.sum(Fy_chunk, dim=2) # [B, chunk_size]\n                \n                # Cleanup chunk tensors\n                del qi_chunk, qi_qj_chunk, inverse_sq_dist_chunk, theta_rad_chunk, Fx_chunk, Fy_chunk\n        \n        if self.debug:\n            print(f\"Fx Shape: {Fx.shape}\") # [B, N]\n            print(f\"Fy Shape: {Fy.shape}\") # [B, N]\n            print(\"-\"*59)\n            \n        return Fx, Fy\n\n    def compute_vector_translation(self, Fx, Fy, zeta):\n        \"\"\"\n        Move each point in the vector space corresponding to forces acted on it.\n\n        Input:\n            - Fx: Force acted on each point in the x-axis | [B, N]\n            - Fy: Force acted on each point in the y-axis | [B, N]\n            - zeta: Coordinates of each point (x, y) | [B, 2, self.imsize, self.imsize]\n\n        Output:\n            - new_zeta: Fully updated Coordinates of each point (x, y) | [B, 2, self.imsize, self.imsize]\n\n        How the Function Works:\n            - Displacement = Velocity x time + Initial Position -> D =  vt + x0\n            - v = Acceleration x time + Initial Velocity = at + v0\n            - Acceleration = Force/Mass = F/m\n                + Since all variables are vectors, we split them into x and y axis, respectively\n        \"\"\"\n        \n        if self.debug:\n            print(f\"Starting compute_vector_translation...\")\n            print(\" \")\n            \n        ax = Fx / torch.abs(self.adaptive_mass) # [B, N]\n        ay = Fy / torch.abs(self.adaptive_mass) # [B, N]\n        if self.debug:\n            print(f\"ax Shape: {ax.shape}\")\n            print(f\"ay Shape: {ay.shape}\")\n            \n        B, N = Fx.shape\n        v0 = self.v0.expand(B, N) # [B, N]\n        \n        if self.debug:\n            print(f\"v0 Shape: {v0.shape}\")\n            \n        time_step = torch.abs(self.adaptive_time)\n        vx = v0 + ax * time_step # [B, N]\n        vy = v0 + ay * time_step # [B, N]\n        vx = vx.reshape(B, self.imsize, self.imsize) # [B, self.imsize, self.imsize]\n        vy = vy.reshape(B, self.imsize, self.imsize) # [B, self.imsize, self.imsize]\n        if self.debug:\n            print(f\"vx Shape: {vx.shape}\")\n            print(f\"vy Shape: {vy.shape}\")\n            \n        if self.debug:\n            print(f\"zeta Shape: {zeta.shape}\")\n            print(f\"zeta[:, 0] Shape: {zeta[:, 0].shape}\")\n            print(f\"zeta[:, 1] Shape: {zeta[:, 1].shape}\")\n        new_zeta = zeta.clone()\n        new_zeta[:, 0] = zeta[:, 0] + vx # [B, 2, self.imsize, self.imsize]\n        new_zeta[:, 1] = zeta[:, 1] + vy # [B, 2, self.imsize, self.imsize]\n\n        \n        if self.debug:\n            print(f\"Updated zeta Shape: {new_zeta.shape}\")\n            print(f\"-\"*59)\n            \n        return new_zeta\n\n    def update_input(self, ipt, zeta):\n        \n        \"\"\"\n        Update Input by Unmapping zeta\n\n        Input:\n            - ipt: Input Image | [B, 3, self.imsize, self.imsize]\n            - zeta: Coordinates of each point (x, y) | [B, 2, self.imsize, self.imsize]\n\n        Output:\n            - new_ipt: New Input Image | [B, 3, self.imsize, self.imsize]\n\n        How the Function Works:\n            - You got the new zeta Coordinates\n            - You unmap it using a point-wise-convolutional-layer\n            - New Input will be the summation of the unmapped zeta and the Input Image\n        \"\"\"\n        if self.debug:\n            print(f\"Running update_input...\")\n            \n        map_to_ipt = self.mapping_to_ipt(zeta) # [B, 3, self.imsize, self.imsize]\n        new_ipt = map_to_ipt*self.beta + (1/self.loops)*ipt*(1-self.beta) # [B, 3, self.imsize, self.imsize]\n        return new_ipt\n\n    def compute_energy_of_system(self, ipt, inverse_dist, chunk_size = 512):\n    \n        \"\"\"\n        Compute total Energy of the System\n    \n        Input:\n            - ipt: Final Input Image | [B, 3, self.imsize, self.imsize]\n            - inverse_dist: 1/r, the inverse distance of each point with respect to every point in zeta | [B, N, N]\n        \n        Output:\n            - V: Total Energy of Final Input Image | [B]\n    \n        How the Function Works:\n            - V = sum(qi * Vj) = qi (k_E sum(qj/r^2))\n        \"\"\"\n        \n        k_E = 1.0 / (4.0 * math.pi * torch.abs(self.adaptive_epsilon))\n        \n        B, C, H, W = ipt.shape\n        N = H*W\n        \n        ipt_reshaped = ipt.reshape(B, N, C) # [B, N, C]\n        \n        if self.debug:\n            print(f\"Running compute_energy_of_system...\")\n            print(\" \")\n    \n        if N <= chunk_size:\n            # Small enough - use original approach\n            qi_qj = ipt_reshaped.unsqueeze(2) * ipt_reshaped.unsqueeze(1) # [B, N, 1, C] * [B, 1, N, C] -> [B, N, N, C]\n            qi_qj = torch.mean(qi_qj, dim = 3) # [B, N, N]\n            if self.debug:\n                print(f\"qi_qj Shape: {qi_qj.shape}\") # [B, N, N]\n    \n            V = k_E * qi_qj * inverse_dist # [B, N, N]\n            V = torch.sum(V, dim = 2) # [B, N]\n            V = torch.sum(V, dim = 1) # [B]\n        else:\n            # CHUNKED BROADCASTING - avoid creating large [B, N, N, C] tensor\n            if self.debug:\n                print(f\"Using chunked broadcasting for energy computation with chunk_size={chunk_size} for N={N}\")\n            \n            V = torch.zeros(B, device=ipt.device, dtype=ipt.dtype) # [B]\n            \n            for i in range(0, N, chunk_size):\n                end_i = min(i + chunk_size, N)\n                \n                # Get chunk of qi values\n                qi_chunk = ipt_reshaped[:, i:end_i, :] # [B, chunk_size, C]\n                \n                # Initialize qi_qj chunk for this row chunk\n                qi_qj_chunk = torch.zeros(B, end_i - i, N, device=ipt.device, dtype=ipt.dtype) # [B, chunk_size, N]\n                \n                # CHUNKED COMPUTATION of qi_qj to avoid large broadcasting\n                for j in range(0, N, chunk_size):\n                    end_j = min(j + chunk_size, N)\n                    \n                    # Get chunk of qj values\n                    qj_chunk = ipt_reshaped[:, j:end_j, :] # [B, chunk_size_j, C]\n                    \n                    # Small broadcasting: [B, chunk_i, 1, C] * [B, 1, chunk_j, C] -> [B, chunk_i, chunk_j, C]\n                    qi_expanded = qi_chunk.unsqueeze(2) # [B, chunk_i, 1, C]\n                    qj_expanded = qj_chunk.unsqueeze(1) # [B, 1, chunk_j, C]\n                    qi_qj_small = qi_expanded * qj_expanded # [B, chunk_i, chunk_j, C]\n                    qi_qj_small = torch.mean(qi_qj_small, dim=3) # [B, chunk_i, chunk_j]\n                    \n                    # Store in the chunk\n                    qi_qj_chunk[:, :, j:end_j] = qi_qj_small # [B, chunk_i, chunk_j]\n                    \n                    # Cleanup small tensors\n                    del qi_expanded, qj_expanded, qi_qj_small\n                \n                # Get corresponding distance chunk\n                inverse_dist_chunk = inverse_dist[:, i:end_i, :] # [B, chunk_size, N]\n                \n                # Compute energy for this chunk\n                V_chunk = k_E * qi_qj_chunk * inverse_dist_chunk # [B, chunk_size, N]\n                V_chunk = torch.sum(V_chunk, dim=2) # [B, chunk_size]\n                V_chunk = torch.sum(V_chunk, dim=1) # [B]\n                \n                # Accumulate energy\n                V += V_chunk # [B]\n                \n                # Cleanup chunk tensors\n                del qi_chunk, qi_qj_chunk, inverse_dist_chunk, V_chunk\n                \n        if self.debug:\n            print(f\"V Shape: {V.shape}\") # [B]\n            print(\"-\"*59)\n            \n        return V\n\n    def aggregate_to_one_function(self, ipt, chunk_size):\n    \n        k_E = 1.0 / (4.0 * math.pi * torch.abs(self.adaptive_epsilon))\n        epsilon_dist_sq = 1e-12 # For squared distances to avoid division by zero\n    \n        if self.debug:\n            print(f\"Running aggregate_to_one_function (ULTRA EFFICIENT)\")\n            print(f\"  ipt Shape: {ipt.shape}\")\n    \n        # 1. Compute zeta (spatial coordinates)\n        # These are computed once and reused. Their memory is relatively small.\n        zeta_raw = self.mapping_to_vector_space(ipt)\n        zeta = self.space_norm(zeta_raw)  # [B, 2, H, W]\n        B, C_zeta, H, W = zeta.shape\n        N = H * W\n    \n        if self.debug: print(f\"  zeta Shape: {zeta.shape}\")\n    \n        # Global accumulators for total forces and system potential\n        Fx_total = torch.zeros(B, N, device=ipt.device, dtype=ipt.dtype)\n        Fy_total = torch.zeros(B, N, device=ipt.device, dtype=ipt.dtype)\n        #V_total_system = torch.zeros(B, device=ipt.device, dtype=ipt.dtype) # Scalar potential for the whole system\n    \n        # Flattened coordinates and charges (properties from ipt)\n        # These are [B, N, Dims]\n        pts_zeta_coords_all = zeta.reshape(B, 2, N)\n        x_coords_all = pts_zeta_coords_all[:, 0, :] # [B, N]\n        y_coords_all = pts_zeta_coords_all[:, 1, :] # [B, N]\n        zflat_for_cdist_all = pts_zeta_coords_all.permute(0, 2, 1) # [B, N, 2] (for torch.cdist)\n    \n        B_ipt, C_ipt, H_ipt, W_ipt = ipt.shape\n        ipt_flat_charges_all = ipt.reshape(B_ipt, C_ipt, N).permute(0, 2, 1) # [B, N, C_ipt]\n    \n        # Outer loop: Iterate over TARGET chunks (points experiencing the force)\n        for i_start in range(0, N, chunk_size):\n            i_end = min(i_start + chunk_size, N)\n            current_i_chunk_size = i_end - i_start\n    \n            if self.debug: print(f\"  Processing TARGET chunk: points {i_start} to {i_end-1}\")\n    \n            # Data for current TARGET chunk_i (small tensors)\n            target_coords_i_cdist = zflat_for_cdist_all[:, i_start:i_end, :]     # [B, current_i_chunk_size, 2]\n            target_x_i = x_coords_all[:, i_start:i_end]                         # [B, current_i_chunk_size]\n            target_y_i = y_coords_all[:, i_start:i_end]                         # [B, current_i_chunk_size]\n            target_charges_qi = ipt_flat_charges_all[:, i_start:i_end, :]       # [B, current_i_chunk_size, C_ipt]\n    \n            # Accumulators for forces/potential ON this i-th target chunk\n            Fx_on_i_accum = torch.zeros(B, current_i_chunk_size, device=ipt.device, dtype=ipt.dtype)\n            Fy_on_i_accum = torch.zeros(B, current_i_chunk_size, device=ipt.device, dtype=ipt.dtype)\n            #V_on_i_accum = torch.zeros(B, current_i_chunk_size, device=ipt.device, dtype=ipt.dtype) # Potential energy for each point in i-chunk\n    \n            # Inner loop: Iterate over SOURCE chunks (points exerting the force)\n            for j_start in range(0, N, chunk_size):\n                j_end = min(j_start + chunk_size, N)\n                current_j_chunk_size = j_end - j_start\n    \n                # Data for current SOURCE chunk_j (small tensors)\n                source_coords_j_cdist = zflat_for_cdist_all[:, j_start:j_end, :]   # [B, current_j_chunk_size, 2]\n                source_x_j = x_coords_all[:, j_start:j_end]                       # [B, current_j_chunk_size]\n                source_y_j = y_coords_all[:, j_start:j_end]                       # [B, current_j_chunk_size]\n                source_charges_qj = ipt_flat_charges_all[:, j_start:j_end, :]     # [B, current_j_chunk_size, C_ipt]\n    \n                # --- Compute interactions between current chunk_i and current chunk_j ---\n                # All tensors created here are [B, current_i_chunk_size, current_j_chunk_size]\n    \n                # A. Distances (r_ij)\n                # distances_ij: [B, current_i_chunk_size, current_j_chunk_size]\n                distances_ij = torch.cdist(target_coords_i_cdist, source_coords_j_cdist, p=2)\n    \n                # B. Angles (theta_ij of vector from j to i)\n                # dx_ji = x_i - x_j ; dy_ji = y_i - y_j\n                dx_ji_sub = target_x_i.unsqueeze(2) - source_x_j.unsqueeze(1)\n                dy_ji_sub = target_y_i.unsqueeze(2) - source_y_j.unsqueeze(1)\n                theta_ji_sub = torch.atan2(dy_ji_sub, dx_ji_sub) # Angle of vector R_ji (from source j to target i)\n    \n                # C. Charge interaction term (qi_qj_effective)\n                qi_expanded = target_charges_qi.unsqueeze(2)             # [B, size_i, 1, C_ipt]\n                qj_expanded = source_charges_qj.unsqueeze(1)             # [B, 1, size_j, C_ipt]\n                qi_qj_channels = qi_expanded * qj_expanded               # [B, size_i, size_j, C_ipt]\n                qi_qj_scalar_sub = torch.mean(qi_qj_channels, dim=3)   # [B, size_i, size_j]\n    \n                # D. Handle self-interaction if target and source chunks overlap\n                if i_start == j_start: # Only diagonal sub-chunks can have self-interaction\n                    diag_sub_mask = torch.eye(current_i_chunk_size, current_j_chunk_size,\n                                              device=ipt.device, dtype=torch.bool)\n                    # Ensure broadcasting if one chunk is smaller at the edge\n                    min_dim_self = min(current_i_chunk_size, current_j_chunk_size)\n                    diag_sub_mask_final = diag_sub_mask[:min_dim_self, :min_dim_self]\n\n                    distances_ij_corrected = distances_ij.clone()\n                    distances_ij_corrected[:, :min_dim_self, :min_dim_self] = torch.where(\n                        diag_sub_mask_final,\n                        float('inf'),\n                        distances_ij_corrected[:, :min_dim_self, :min_dim_self]\n                    )\n                    distances_ij = distances_ij_corrected\n    \n    \n                # E. Potential Energy V_ij = k_E * (qi_qj)_ij / r_ij\n                \n                #inv_dist_ij = torch.pow(distances_ij, -1) # Handles inf correctly -> 0.0\n                #V_ij_sub_values = k_E * qi_qj_scalar_sub * inv_dist_ij\n    \n                # F. Force Components F_ij = (k_E * (qi_qj)_ij / r_ij^2) * unit_vector_ji\n                # F_radial_signed = k_E * qi_qj_scalar_sub * (1/r_ij^2)\n                # (1/r_ij^2) can be inv_dist_ij.pow(2) or 1.0 / (distances_ij.pow(2) + epsilon_dist_sq)\n                distances_ij_inf = torch.where(distances_ij == 0, float('inf'), distances_ij)\n                inv_sq_dist_ij = torch.pow(distances_ij_inf, -2) # Add epsilon for stability if r can be 0 for non-self\n                #inv_sq_dist_ij = torch.where(torch.isinf(distances_ij), torch.zeros_like(inv_sq_dist_ij), inv_sq_dist_ij) # Ensure inf dist -> 0 force\n    \n                F_radial_signed_ij = k_E * qi_qj_scalar_sub * inv_sq_dist_ij\n                \n                normalization_factor = torch.pow(torch.tensor([N], dtype=torch.float32, device=ipt.device), 2)\n                \n                Fx_ij_sub = F_radial_signed_ij * torch.cos(theta_ji_sub)/normalization_factor\n                Fy_ij_sub = F_radial_signed_ij * torch.sin(theta_ji_sub)/normalization_factor\n    \n                # G. Accumulate forces and potential for the current target_i_chunk\n                Fx_on_i_accum = torch.sum(Fx_ij_sub, dim=2)\n                Fy_on_i_accum = torch.sum(Fy_ij_sub, dim=2)\n\n                Fx_on_i_accum = self.space_norm(Fx_on_i_accum)\n                Fy_on_i_accum = self.space_norm(Fy_on_i_accum)\n                #V_on_i_accum += torch.sum(V_ij_sub_values, dim=2) # Sum potential contributions for each point in i_chunk\n    \n                # H. Explicitly delete intermediate sub-chunk tensors\n                del distances_ij, dx_ji_sub, dy_ji_sub, theta_ji_sub, qi_expanded, qj_expanded\n                del qi_qj_channels, qi_qj_scalar_sub, #inv_dist_ij, V_ij_sub_values\n                del inv_sq_dist_ij, F_radial_signed_ij, Fx_ij_sub, Fy_ij_sub\n                if torch.cuda.is_available(): torch.cuda.empty_cache() # Aggressive cache clearing\n    \n            # Store results for the fully processed i-th target chunk\n            Fx_total[:, i_start:i_end] = Fx_on_i_accum\n            Fy_total[:, i_start:i_end] = Fy_on_i_accum\n            #V_total_system += torch.sum(V_on_i_accum, dim=1) # Sum potential over points in i-chunk, then add to system total\n    \n            # Explicitly delete i-chunk accumulators\n            del target_coords_i_cdist, target_x_i, target_y_i, target_charges_qi\n            del Fx_on_i_accum, Fy_on_i_accum, #V_on_i_accum\n            if torch.cuda.is_available(): torch.cuda.empty_cache()\n    \n        if self.debug:\n            print(f\"  aggregate_to_one_function (ULTRA EFFICIENT) finished.\")\n            print(f\"  Fx_total: {Fx_total}\")\n            print(f\"  Fy_total: {Fy_total}\")\n            #if V_total_system.numel() > 0: print(f\"  V_total_system: {V_total_system.item()}\")\n            print(\"-\" * 59)\n    \n        return Fx_total, Fy_total, zeta, #V_total_system # Make sure zeta is returned\n\n    def forward(self, ipt, chunk_size=1024):\n        \n        ipt = self.conv1(ipt)\n        ipt = self.space_norm(ipt)\n        \n        assert ipt.shape[2] and ipt.shape[3] == self.imsize, \"Input Shape's Height and Width must match parameter imsize\"\n        for layer in range(self.layers):\n            \n            if self.use_self_attention:\n                attended_ipt = self.input_attention(ipt)\n                ipt = self.attention_scale * attended_ipt + ipt\n            else:\n                ipt = self.conv2(ipt)\n                ipt = self.space_norm(ipt)\n            if self.debug:\n                print(f\"After self-attention ipt Shape: {ipt.shape}\")\n                \n            if self.debug:\n                print(f\"Running Layer {layer + 1}/{self.layers}\")\n                print(f\"-\"*59)\n            for loop in range(self.loops):\n                if self.debug:\n                    print(f\"Starting {loop + 1}/{self.loops} for compute_electric_force...\")\n                    print(\" \")\n                #zeta, x, y, D = self.compute_distance(ipt, chunk_size)\n                #theta_rad = self.compute_angle(x, y, chunk_size)\n                #Fx, Fy = self.compute_electric_force(ipt, theta_rad, D.pow(-2), chunk_size)\n                Fx, Fy, zeta = self.aggregate_to_one_function(ipt, chunk_size)\n                zeta = self.compute_vector_translation(Fx, Fy, zeta)\n    \n            zeta = self.space_norm(zeta)\n            ipt = self.update_input(ipt, zeta)\n            \n        ipt = self.space_norm(ipt)\n        Fx, Fy, zeta = self.aggregate_to_one_function(ipt, chunk_size)\n        #V = torch.abs(V)\n        \n        return ipt, zeta, #V","metadata":{"execution":{"iopub.status.busy":"2025-06-08T07:52:33.926427Z","iopub.execute_input":"2025-06-08T07:52:33.926599Z","iopub.status.idle":"2025-06-08T07:52:33.975317Z","shell.execute_reply.started":"2025-06-08T07:52:33.926586Z","shell.execute_reply":"2025-06-08T07:52:33.974792Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"model = ElectricForceModel(debug = True)\nmodel = model.to(DEVICE)\nmodel = model.to(torch.bfloat16)","metadata":{"execution":{"iopub.status.busy":"2025-06-08T07:52:33.975865Z","iopub.execute_input":"2025-06-08T07:52:33.976031Z","iopub.status.idle":"2025-06-08T07:52:34.195600Z","shell.execute_reply.started":"2025-06-08T07:52:33.976018Z","shell.execute_reply":"2025-06-08T07:52:34.194924Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n    output, zeta = model(inpt.to(DEVICE).to(torch.bfloat16), chunk_size=512)","metadata":{"execution":{"iopub.status.busy":"2025-06-08T07:06:50.062884Z","iopub.execute_input":"2025-06-08T07:06:50.063064Z","execution_failed":"2025-06-08T07:08:41.058Z"}}},{"cell_type":"code","source":"class SegmentationDataset(torch.utils.data.Dataset):\n    def __init__(self, image_paths, mask_paths, transforms=None):\n        self.image_paths = image_paths\n        self.mask_paths = mask_paths\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        # Đọc ảnh và mask\n        image = np.array(Image.open(self.image_paths[idx]).convert(\"RGB\"))\n        mask = np.array(Image.open(self.mask_paths[idx]).convert(\"L\"))\n        \n        mask = np.where(mask < 85, 0, np.where(mask < 170, 1, 2))\n        \n        if self.transforms:\n            augmented = self.transforms(image=image, mask=mask)\n            image = augmented['image']\n            mask = augmented['mask']\n            \n        mask = mask.long() if isinstance(mask, torch.Tensor) else torch.from_numpy(mask).long()\n        #print(mask.shape)\n        return image, mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T07:52:34.196253Z","iopub.execute_input":"2025-06-08T07:52:34.196451Z","iopub.status.idle":"2025-06-08T07:52:34.201836Z","shell.execute_reply.started":"2025-06-08T07:52:34.196436Z","shell.execute_reply":"2025-06-08T07:52:34.201349Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def compute_multi_iou(outputs, masks, num_classes=3):\n    probs = torch.softmax(outputs, dim=1)\n    preds = torch.argmax(probs, dim=1)\n    iou_per_class = []\n    for cls in range(num_classes):\n        pred_cls = (preds == cls)\n        target_cls = (masks == cls)\n        intersection = (pred_cls & target_cls).sum().float()\n        union = (pred_cls | target_cls).sum().float()\n        iou = (intersection / (union + 1e-8)).item()\n        iou_per_class.append(iou)\n    return np.mean(iou_per_class)\n\ndef compute_dice_score(outputs, masks, num_classes=3):\n    probs = torch.softmax(outputs, dim=1)\n    preds = torch.argmax(probs, dim=1)\n    dice_per_class = []\n    for cls in range(num_classes):\n        pred_cls = (preds == cls)\n        target_cls = (masks == cls)\n        intersection = (pred_cls & target_cls).sum().float()\n        dice = (2. * intersection / (pred_cls.sum() + target_cls.sum() + 1e-8)).item()\n        dice_per_class.append(dice)\n    return np.mean(dice_per_class)\n\ndef compute_precision(outputs, masks, num_classes=3):\n    probs = torch.softmax(outputs, dim=1)\n    preds = torch.argmax(probs, dim=1)\n    precision_per_class = []\n    for cls in range(num_classes):\n        pred_cls = (preds == cls)\n        target_cls = (masks == cls)\n        true_positive = (pred_cls & target_cls).sum().float()\n        predicted_positive = pred_cls.sum().float()\n        precision = (true_positive / (predicted_positive + 1e-8)).item()\n        precision_per_class.append(precision)\n    return np.mean(precision_per_class)\n\ndef compute_accuracy(outputs, masks):\n    probs = torch.softmax(outputs, dim=1)\n    preds = torch.argmax(probs, dim=1)\n    correct = (preds == masks).sum().float()\n    total = torch.numel(masks)\n    return (correct / total).item()\n\n\ndef post_process(mask, kernel_size=3):\n    import cv2\n    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n    \n    cleaned = cv2.morphologyEx(mask.numpy().astype(np.uint8), \n                              cv2.MORPH_OPEN, kernel)\n\n    cleaned = cv2.morphologyEx(cleaned, \n                              cv2.MORPH_CLOSE, kernel)\n    \n    return torch.from_numpy(cleaned)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T07:52:34.202375Z","iopub.execute_input":"2025-06-08T07:52:34.202540Z","iopub.status.idle":"2025-06-08T07:52:34.216877Z","shell.execute_reply.started":"2025-06-08T07:52:34.202527Z","shell.execute_reply":"2025-06-08T07:52:34.216398Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class Mixup:\n    def __init__(self, alpha=0.4, p=0.5):\n        self.alpha = alpha\n        self.p = p\n        \n    def __call__(self, images, masks):\n        if np.random.rand() < self.p:\n            batch_size = len(images) if isinstance(images, list) else 1\n            if batch_size > 1:\n                indices = np.random.permutation(batch_size)\n                img2, mask2 = images[indices], masks[indices]\n                \n                lam = np.random.beta(self.alpha, self.alpha)\n\n                mixed_img = lam * images + (1 - lam) * img2\n                mixed_mask = lam * masks + (1 - lam) * mask2\n                \n                return mixed_img, mixed_mask\n        return images, masks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T07:52:34.218129Z","iopub.execute_input":"2025-06-08T07:52:34.218297Z","iopub.status.idle":"2025-06-08T07:52:34.228702Z","shell.execute_reply.started":"2025-06-08T07:52:34.218284Z","shell.execute_reply":"2025-06-08T07:52:34.228276Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def get_train_transform():\n    return A.Compose([\n        A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.RandomRotate90(p=0.5),\n        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=15, p=0.5),\n        A.RandomBrightnessContrast(p=0.5),\n        A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n        ToTensorV2(),\n    ])\n\ndef get_valid_transform():\n    return A.Compose([\n        A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n        A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n        ToTensorV2(),\n    ])","metadata":{"execution":{"iopub.status.busy":"2025-06-08T07:52:34.229131Z","iopub.execute_input":"2025-06-08T07:52:34.229280Z","iopub.status.idle":"2025-06-08T07:52:34.235543Z","shell.execute_reply.started":"2025-06-08T07:52:34.229268Z","shell.execute_reply":"2025-06-08T07:52:34.234889Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"def visualize_predictions(model, dataloader, device, num_samples=4):\n    model.eval()\n    images, masks, preds = [], [], []\n    \n    with torch.no_grad():\n        for img, mask in dataloader:\n            img = img.to(device)\n            output = model(img)\n            pred = torch.argmax(torch.softmax(output, dim=1), dim=1)\n            \n            for i in range(min(img.shape[0], num_samples - len(images))):\n                if len(images) < num_samples:\n                    images.append(img[i].cpu())\n                    masks.append(mask[i].cpu())\n                    preds.append(pred[i].cpu())\n            \n            if len(images) >= num_samples:\n                break\n    \n    fig, axes = plt.subplots(num_samples, 3, figsize=(15, num_samples * 5))\n    \n    for i in range(num_samples):\n        img = images[i].permute(1, 2, 0).numpy()\n        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n        img = np.clip(img, 0, 1)\n        \n        axes[i, 0].imshow(img)\n        axes[i, 0].set_title('Original Image')\n        axes[i, 0].axis('off')\n        \n        axes[i, 1].imshow(masks[i], cmap='jet')\n        axes[i, 1].set_title('Ground Truth')\n        axes[i, 1].axis('off')\n        \n        axes[i, 2].imshow(preds[i], cmap='jet')\n        axes[i, 2].set_title('Prediction')\n        axes[i, 2].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    return fig","metadata":{"execution":{"iopub.status.busy":"2025-06-08T06:59:31.782307Z","iopub.status.idle":"2025-06-08T06:59:31.782506Z","shell.execute_reply.started":"2025-06-08T06:59:31.782409Z","shell.execute_reply":"2025-06-08T06:59:31.782417Z"}}},{"cell_type":"markdown","source":"def train_one_epoch(model, dataloader, optimizer, criterion, device, tau):\n    model.train()\n    train_loss = 0.0\n    \n    pbar = tqdm(dataloader, desc='Training')\n    for images, masks in pbar:\n        images = images.to(device)\n        masks = masks.to(device)\n        \n        optimizer.zero_grad()\n        outputs, zeta= model(images)\n        loss = criterion(outputs, masks)#*(1-tau) + V*tau\n        \n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item()\n        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n    \n    return train_loss / len(dataloader)\n\n# Validation function\ndef validate(model, dataloader, criterion, device, tau):\n    model.eval()\n    val_loss = 0.0\n    iou_scores, dsc_scores, precision_scores, acc_scores = [], [], [], []\n    \n    with torch.no_grad():\n        for images, masks in tqdm(dataloader, desc='Validation'):\n            images = images.to(device)\n            masks = masks.to(device)\n            \n            outputs, zeta= model(images)\n            #outputs = convert_model_output_to_segmentation(outputs)\n            loss = criterion(outputs, masks)#*(1-tau) + V*tau\n            \n            val_loss += loss.item()\n            \n            iou_scores.append(compute_multi_iou(outputs, masks))\n            dsc_scores.append(compute_dice_score(outputs, masks))\n            precision_scores.append(compute_precision(outputs, masks))\n            acc_scores.append(compute_accuracy(outputs, masks))\n    \n    mean_iou = np.mean(iou_scores)\n    mean_dsc = np.mean(dsc_scores)\n    mean_precision = np.mean(precision_scores)\n    mean_acc = np.mean(acc_scores)\n    \n    return val_loss / len(dataloader), mean_iou, mean_dsc, mean_precision, mean_acc","metadata":{"execution":{"iopub.status.busy":"2025-06-08T06:59:31.783298Z","iopub.status.idle":"2025-06-08T06:59:31.783498Z","shell.execute_reply.started":"2025-06-08T06:59:31.783402Z","shell.execute_reply":"2025-06-08T06:59:31.783410Z"}}},{"cell_type":"markdown","source":"def train_model(model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs, device, fold=None):\n    best_metrics = {'mean_iou': 0.0, 'mean_dsc': 0.0}\n    history = {\n        'train_loss': [], 'val_loss': [],\n        'iou': [], 'dsc': [], 'precision': [], 'accuracy': []\n    }\n    \n    for epoch in range(num_epochs):\n        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, tau = 0.2)\n        history['train_loss'].append(train_loss)\n        \n        val_loss, mean_iou, mean_dsc, mean_precision, mean_acc = validate(model, val_loader, criterion, device, tau = 0.2)\n        history['val_loss'].append(val_loss)\n        history['iou'].append(mean_iou)\n        history['dsc'].append(mean_dsc)\n        history['precision'].append(mean_precision)\n        history['accuracy'].append(mean_acc)\n        \n        scheduler.step(mean_dsc)\n        \n        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n        print(f\"Accuracy: {mean_acc:.4f}, Mean IoU: {mean_iou:.4f}, Mean DSC/F1: {mean_dsc:.4f}, Mean Precision: {mean_precision:.4f}\")\n        \n        if mean_dsc > best_metrics['mean_dsc']:\n            best_metrics['mean_dsc'] = mean_dsc\n            best_metrics['mean_iou'] = mean_iou\n            best_metrics['mean_precision'] = mean_precision\n            best_metrics['mean_acc'] = mean_acc\n            \n            model_name = f\"best_model_fold{fold}.pth\" if fold is not None else \"best_model.pth\"\n            torch.save(model.state_dict(), model_name)\n            print(f\"Saved best model with DSC: {mean_dsc:.4f}\")\n        \n        print(f\"Best DSC: {best_metrics['mean_dsc']:.4f}, Best IoU: {best_metrics['mean_iou']:.4f}\")\n        print(\"-\" * 50)\n    \n    return model, history, best_metrics","metadata":{"execution":{"iopub.status.busy":"2025-06-08T06:59:31.784303Z","iopub.status.idle":"2025-06-08T06:59:31.784503Z","shell.execute_reply.started":"2025-06-08T06:59:31.784406Z","shell.execute_reply":"2025-06-08T06:59:31.784415Z"}}},{"cell_type":"markdown","source":"def find_optimal_k(all_images, all_masks, k_values=[3, 5, 7, 10], num_epochs=10, tau = 0.2):\n\n    k_scores = []\n    k_variances = []\n    train_times = []\n    \n    for k in k_values:\n        fold_scores = []\n        \n        kf = KFold(n_splits=k, shuffle=True, random_state=SEED)\n        start_time = time.time()\n        \n        for fold, (train_idx, val_idx) in enumerate(kf.split(all_images)):\n            print(f\"  Fold {fold+1}/{k}\")\n            \n            train_img = [all_images[i] for i in train_idx]\n            train_mask = [all_masks[i] for i in train_idx]\n            val_img = [all_images[i] for i in val_idx]\n            val_mask = [all_masks[i] for i in val_idx]\n            \n            # Tạo dataset\n            train_dataset = SegmentationDataset(train_img, train_mask, get_train_transform())\n            val_dataset = SegmentationDataset(val_img, val_mask, get_valid_transform())\n            \n            # Tạo dataloader\n            train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n            \n            # Khởi tạo model mới cho mỗi fold\n            model = ElectricForceModel().to(DEVICE)\n            \n            # Setup optimizer và loss\n            optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n            criterion = nn.CrossEntropyLoss()\n            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3)\n            \n            best_metrics = {'mean_dsc': 0.0}\n            \n            for epoch in range(num_epochs):\n                model.train()\n                for images, masks in train_loader:\n                    images, masks = images.to(DEVICE), masks.to(DEVICE)\n                    optimizer.zero_grad()\n                    outputs, zeta= model(images)\n                    loss = criterion(outputs, masks)#*(1-tau) + V*tau\n                    loss.backward()\n                    optimizer.step()\n                \n                model.eval()\n                dsc_scores = []\n                with torch.no_grad():\n                    for images, masks in val_loader:\n                        images, masks = images.to(DEVICE), masks.to(DEVICE)\n                        outputs, zeta= model(images)\n                        #outputs = conconvert_model_output_to_segmentation(outputs)\n                        dsc_scores.append(compute_dice_score(outputs, masks))\n                \n                mean_dsc = np.mean(dsc_scores)\n                scheduler.step(mean_dsc)\n                \n                if mean_dsc > best_metrics['mean_dsc']:\n                    best_metrics['mean_dsc'] = mean_dsc\n            \n            # Lưu Dice score của fold này\n            fold_scores.append(best_metrics['mean_dsc'])\n            print(f\"    Dice score: {best_metrics['mean_dsc']:.4f}\")\n        \n        train_time = time.time() - start_time\n        train_times.append(train_time)\n        \n        mean_score = np.mean(fold_scores)\n        score_variance = np.var(fold_scores)\n        \n        k_scores.append(mean_score)\n        k_variances.append(score_variance)\n        \n        print(f\"K={k}, Điểm trung bình: {mean_score:.4f}, Phương sai: {score_variance:.6f}\")\n        print(f\"Thời gian training: {train_time:.2f} giây\")\n    \n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n    \n    ax1.plot(k_values, k_scores, 'o-', linewidth=2, color='blue')\n    ax1.set_xlabel('Số lượng Fold (K)')\n    ax1.set_ylabel('Điểm DSC trung bình')\n    ax1.set_title('Hiệu suất theo giá trị K')\n    ax1.grid(True)\n    \n    # Vẽ phương sai\n    ax2.plot(k_values, k_variances, 'o-', linewidth=2, color='orange')\n    ax2.set_xlabel('Số lượng Fold (K)')\n    ax2.set_ylabel('Phương sai')\n    ax2.set_title('Phương sai theo giá trị K')\n    ax2.grid(True)\n    \n    # Vẽ thời gian training\n    ax3.plot(k_values, train_times, 'o-', linewidth=2, color='green')\n    ax3.set_xlabel('Số lượng Fold (K)')\n    ax3.set_ylabel('Thời gian training (giây)')\n    ax3.set_title('Thời gian training theo giá trị K')\n    ax3.grid(True)\n    \n    plt.tight_layout()\n    plt.savefig('k_fold_elbow_analysis.png')\n    plt.show()\n    \n    optimal_k = find_elbow_point(k_values, k_scores, k_variances)\n\n    \n    return optimal_k\n\ndef find_elbow_point(k_values, k_scores, k_variances):\n    \"\"\"\n    Xác định điểm elbow dựa trên phần trăm cải thiện và phương sai\n    \"\"\"\n    \n    improvements = []\n    for i in range(1, len(k_values)):\n        improvement = (k_scores[i] - k_scores[i-1]) / k_scores[i-1] * 100\n        improvements.append(improvement)\n    \n    for i in range(len(improvements)):\n        if improvements[i] < 1.0:\n            return k_values[i+1]\n    \n    var_index = np.argmin(k_variances)\n    return k_values[var_index]","metadata":{"execution":{"iopub.status.busy":"2025-06-08T06:59:31.785092Z","iopub.status.idle":"2025-06-08T06:59:31.785311Z","shell.execute_reply.started":"2025-06-08T06:59:31.785213Z","shell.execute_reply":"2025-06-08T06:59:31.785222Z"}}},{"cell_type":"markdown","source":"def run_training():\n    print(f\"Using device: {DEVICE}\")\n    \n    root_dir = Path('/kaggle/input/data-1/content')\n    all_images = sorted(list((root_dir / 'images').glob('*.bmp')))\n    all_masks = sorted(list((root_dir / 'masks').glob('*.png')))\n    \n    all_images = sorted(all_images, key=lambda x: x.stem)\n    all_masks = sorted(all_masks, key=lambda x: x.stem)\n    \n    print(f\"Total images: {len(all_images)}\")\n    print(f\"Total masks: {len(all_masks)}\")\n    \n    optimal_k = find_optimal_k(\n        all_images, all_masks, \n        k_values=[3, 5, 7, 10], \n        num_epochs=10\n    )\n    \n    kf = KFold(n_splits=optimal_k, shuffle=True, random_state=SEED)\n    fold_metrics = []\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(all_images)):\n        print(f\"Training Fold {fold+1}/{optimal_k}\")\n\n        train_img = [all_images[i] for i in train_idx]\n        train_mask = [all_masks[i] for i in train_idx]\n        val_img = [all_images[i] for i in val_idx]\n        val_mask = [all_masks[i] for i in val_idx]\n        \n        # Create datasets\n        train_dataset = SegmentationDataset(train_img, train_mask, get_train_transform())\n        val_dataset = SegmentationDataset(val_img, val_mask, get_valid_transform())\n        \n        # Create dataloaders\n        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n        \n        # Initialize model\n        model = ElectricForceModel().to(DEVICE)\n        \n        # Setup optimizer and loss\n        optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n        criterion = nn.CrossEntropyLoss()\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=5, factor=0.5)\n        \n        # Train model\n        model, history, best_metrics = train_model(\n            model, \n            train_loader, \n            val_loader,\n            optimizer,\n            criterion,\n            scheduler,\n            NUM_EPOCHS,\n            DEVICE,\n            fold=fold\n        )\n        \n        # Save metrics\n        fold_metrics.append(best_metrics)\n        \n        # Visualize predictions\n        visualize_predictions(model, val_loader, DEVICE)\n        \n        # Plot learning curves\n        plt.figure(figsize=(12, 8))\n        \n        plt.subplot(2, 2, 1)\n        plt.plot(history['train_loss'], label='Train Loss')\n        plt.plot(history['val_loss'], label='Validation Loss')\n        plt.legend()\n        plt.title('Loss Curves')\n        \n        plt.subplot(2, 2, 2)\n        plt.plot(history['iou'], label='IoU')\n        plt.plot(history['dsc'], label='DSC/F1')\n        plt.legend()\n        plt.title('IoU and DSC Curves')\n        \n        plt.subplot(2, 2, 3)\n        plt.plot(history['precision'], label='Precision')\n        plt.legend()\n        plt.title('Precision Curve')\n        \n        plt.subplot(2, 2, 4)\n        plt.plot(history['accuracy'], label='Accuracy')\n        plt.legend()\n        plt.title('Accuracy Curve')\n        \n        plt.tight_layout()\n        plt.savefig(f'learning_curves_fold{fold}.png')\n        plt.show()\n    \n    # Print overall metrics\n    print(\"\\nAverage metrics across all folds:\")\n    avg_iou = np.mean([metrics['mean_iou'] for metrics in fold_metrics])\n    avg_dsc = np.mean([metrics['mean_dsc'] for metrics in fold_metrics])\n    avg_precision = np.mean([metrics['mean_precision'] for metrics in fold_metrics])\n    avg_acc = np.mean([metrics['mean_acc'] for metrics in fold_metrics])\n    \n    print(f\"Avg IoU: {avg_iou:.4f}\")\n    print(f\"Avg DSC/F1: {avg_dsc:.4f}\")\n    print(f\"Avg Precision: {avg_precision:.4f}\")\n    print(f\"Avg Accuracy: {avg_acc:.4f}\")\n\n# Run the training\nif __name__ == \"__main__\":\n    run_training()","metadata":{"execution":{"iopub.status.busy":"2025-06-08T06:59:31.785976Z","iopub.status.idle":"2025-06-08T06:59:31.786192Z","shell.execute_reply.started":"2025-06-08T06:59:31.786074Z","shell.execute_reply":"2025-06-08T06:59:31.786083Z"}}},{"cell_type":"code","source":"from torch.amp import autocast, GradScaler\nimport time\nfrom pathlib import Path\nimport json\nfrom datetime import datetime\nimport logging\n\n# Setup logging\ndef setup_logging():\n    \"\"\"Setup detailed logging for training\"\"\"\n    log_dir = Path(\"training_logs\")\n    log_dir.mkdir(exist_ok=True)\n    \n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    log_file = log_dir / f\"training_{timestamp}.log\"\n    \n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)s - %(levelname)s - %(message)s',\n        handlers=[\n            logging.FileHandler(log_file),\n            logging.StreamHandler()\n        ]\n    )\n    return logging.getLogger(__name__)\n\ndef train_one_epoch(model, dataloader, optimizer, criterion, device, epoch, logger):\n    \"\"\"Train for one epoch without gradient scaling\"\"\"\n    model.train()\n    \n    epoch_loss = 0.0\n    epoch_steps = 0\n    batch_losses = []\n    \n    pbar = tqdm(dataloader, desc=f'Epoch {epoch+1} - Training')\n    \n    for batch_idx, (images, masks) in enumerate(pbar):\n        step_start_time = time.time()\n        \n        # Move data to device and cast\n        images = images.to(device).to(torch.bfloat16)\n        masks = masks.to(device)\n        \n        # Zero gradients\n        optimizer.zero_grad()\n        \n        # Forward pass with autocast\n        with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n            outputs, zeta = model(images, chunk_size=512)\n            loss = criterion(outputs, masks)\n        \n        loss.backward()\n        optimizer.step()\n        \n        # Track metrics\n        batch_loss = loss.item()\n        epoch_loss += batch_loss\n        batch_losses.append(batch_loss)\n        epoch_steps += 1\n        \n        step_time = time.time() - step_start_time\n        \n        # Update progress bar\n        pbar.set_postfix({\n            'loss': f'{batch_loss:.4f}',\n            'avg_loss': f'{epoch_loss/epoch_steps:.4f}',\n            'step_time': f'{step_time:.2f}s'\n        })\n    \n    return epoch_loss / len(dataloader), batch_losses\n\n\ndef validate_one_epoch(model, dataloader, criterion, device, epoch, logger):\n    \"\"\"Validate for one epoch with detailed metrics\"\"\"\n    model.eval()\n    \n    val_loss = 0.0\n    iou_scores, dsc_scores, precision_scores, acc_scores = [], [], [], []\n    \n    pbar = tqdm(dataloader, desc=f'Epoch {epoch+1} - Validation')\n    \n    with torch.no_grad():\n        for batch_idx, (images, masks) in enumerate(pbar):\n            # Move data to device\n            images = images.to(device).to(torch.bfloat16)\n            masks = masks.to(device)\n            \n            # Forward pass with mixed precision\n            with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n                outputs, zeta = model(images, chunk_size=512)\n                loss = criterion(outputs, masks)\n            \n            val_loss += loss.item()\n            \n            # Compute metrics\n            iou = compute_multi_iou(outputs, masks)\n            dsc = compute_dice_score(outputs, masks)\n            precision = compute_precision(outputs, masks)\n            accuracy = compute_accuracy(outputs, masks)\n            \n            iou_scores.append(iou)\n            dsc_scores.append(dsc)\n            precision_scores.append(precision)\n            acc_scores.append(accuracy)\n            \n            # Update progress bar\n            pbar.set_postfix({\n                'val_loss': f'{loss.item():.4f}',\n                'iou': f'{iou:.4f}',\n                'dsc': f'{dsc:.4f}'\n            })\n    \n    # Calculate averages\n    avg_val_loss = val_loss / len(dataloader)\n    mean_iou = np.mean(iou_scores)\n    mean_dsc = np.mean(dsc_scores)\n    mean_precision = np.mean(precision_scores)\n    mean_acc = np.mean(acc_scores)\n    \n    # Calculate standard deviations\n    std_iou = np.std(iou_scores)\n    std_dsc = np.std(dsc_scores)\n    std_precision = np.std(precision_scores)\n    std_acc = np.std(acc_scores)\n    \n    logger.info(f\"Epoch {epoch+1} Validation Summary:\")\n    logger.info(f\"  Val_Loss={avg_val_loss:.4f}\")\n    logger.info(f\"  IoU: {mean_iou:.4f} ± {std_iou:.4f}\")\n    logger.info(f\"  DSC: {mean_dsc:.4f} ± {std_dsc:.4f}\")\n    logger.info(f\"  Precision: {mean_precision:.4f} ± {std_precision:.4f}\")\n    logger.info(f\"  Accuracy: {mean_acc:.4f} ± {std_acc:.4f}\")\n    \n    return avg_val_loss, mean_iou, mean_dsc, mean_precision, mean_acc\n\ndef save_checkpoint(model, optimizer, scaler, epoch, metrics, filename):\n    \"\"\"Save model checkpoint with training state\"\"\"\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scaler_state_dict': scaler.state_dict(),\n        'metrics': metrics,\n        'timestamp': datetime.now().isoformat()\n    }\n    torch.save(checkpoint, filename)\n\ndef plot_training_curves(history, save_path):\n    \"\"\"Plot and save training curves\"\"\"\n    epochs = range(1, len(history['train_loss']) + 1)\n    \n    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n    \n    # Loss curves\n    axes[0, 0].plot(epochs, history['train_loss'], 'b-', label='Training Loss')\n    axes[0, 0].plot(epochs, history['val_loss'], 'r-', label='Validation Loss')\n    axes[0, 0].set_title('Loss Curves')\n    axes[0, 0].set_xlabel('Epoch')\n    axes[0, 0].set_ylabel('Loss')\n    axes[0, 0].legend()\n    axes[0, 0].grid(True)\n    \n    # IoU curve\n    axes[0, 1].plot(epochs, history['iou'], 'g-', label='IoU')\n    axes[0, 1].set_title('IoU Curve')\n    axes[0, 1].set_xlabel('Epoch')\n    axes[0, 1].set_ylabel('IoU')\n    axes[0, 1].legend()\n    axes[0, 1].grid(True)\n    \n    # DSC curve\n    axes[0, 2].plot(epochs, history['dsc'], 'purple', label='DSC/F1')\n    axes[0, 2].set_title('DSC/F1 Curve')\n    axes[0, 2].set_xlabel('Epoch')\n    axes[0, 2].set_ylabel('DSC')\n    axes[0, 2].legend()\n    axes[0, 2].grid(True)\n    \n    # Precision curve\n    axes[1, 0].plot(epochs, history['precision'], 'orange', label='Precision')\n    axes[1, 0].set_title('Precision Curve')\n    axes[1, 0].set_xlabel('Epoch')\n    axes[1, 0].set_ylabel('Precision')\n    axes[1, 0].legend()\n    axes[1, 0].grid(True)\n    \n    # Accuracy curve\n    axes[1, 1].plot(epochs, history['accuracy'], 'brown', label='Accuracy')\n    axes[1, 1].set_title('Accuracy Curve')\n    axes[1, 1].set_xlabel('Epoch')\n    axes[1, 1].set_ylabel('Accuracy')\n    axes[1, 1].legend()\n    axes[1, 1].grid(True)\n    \n    # Learning rate curve (if available)\n    if 'learning_rate' in history:\n        axes[1, 2].plot(epochs, history['learning_rate'], 'cyan', label='Learning Rate')\n        axes[1, 2].set_title('Learning Rate')\n        axes[1, 2].set_xlabel('Epoch')\n        axes[1, 2].set_ylabel('LR')\n        axes[1, 2].legend()\n        axes[1, 2].grid(True)\n    else:\n        axes[1, 2].axis('off')\n    \n    plt.tight_layout()\n    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    plt.show()\n\ndef train_model(model, train_loader, val_loader, optimizer, criterion, scheduler, \n                num_epochs, device, save_dir=\"checkpoints\"):\n    \"\"\"Main training loop with comprehensive logging\"\"\"\n    \n    # Setup logging\n    logger = setup_logging()\n    \n    # Create save directory\n    save_dir = Path(save_dir)\n    save_dir.mkdir(exist_ok=True)\n    \n    # Initialize mixed precision scaler\n    #scaler = GradScaler('cuda')\n\n    # Convert model to bfloat16 for memory efficiency\n    model = model.to(torch.bfloat16)\n    logger.info(\"Model converted to bfloat16 for memory efficiency\")\n    \n    # Training history\n    history = {\n        'train_loss': [],\n        'val_loss': [],\n        'iou': [],\n        'dsc': [],\n        'precision': [],\n        'accuracy': [],\n        'learning_rate': [],\n        'epoch_times': []\n    }\n    \n    # Best metrics tracking\n    best_metrics = {\n        'best_dsc': 0.0,\n        'best_iou': 0.0,\n        'best_epoch': 0\n    }\n    \n    logger.info(f\"Starting training for {num_epochs} epochs\")\n    logger.info(f\"Device: {device}\")\n    logger.info(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    logger.info(f\"Training batches: {len(train_loader)}\")\n    logger.info(f\"Validation batches: {len(val_loader)}\")\n    \n    total_start_time = time.time()\n    \n    for epoch in range(num_epochs):\n        epoch_start_time = time.time()\n        \n        logger.info(f\"{'='*50}\")\n        logger.info(f\"Starting Epoch {epoch+1}/{num_epochs}\")\n        logger.info(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.2e}\")\n        \n        # Training phase\n        train_loss, batch_losses = train_one_epoch(\n            model, train_loader, optimizer, criterion, device, epoch, logger\n        )\n        \n        # Validation phase\n        val_loss, mean_iou, mean_dsc, mean_precision, mean_acc = validate_one_epoch(\n            model, val_loader, criterion, device, epoch, logger\n        )\n        \n        # Update learning rate scheduler\n        scheduler.step(mean_dsc)\n        \n        # Record metrics\n        history['train_loss'].append(train_loss)\n        history['val_loss'].append(val_loss)\n        history['iou'].append(mean_iou)\n        history['dsc'].append(mean_dsc)\n        history['precision'].append(mean_precision)\n        history['accuracy'].append(mean_acc)\n        history['learning_rate'].append(optimizer.param_groups[0]['lr'])\n        \n        epoch_time = time.time() - epoch_start_time\n        history['epoch_times'].append(epoch_time)\n        \n        # Log epoch summary\n        logger.info(f\"Epoch {epoch+1} Summary:\")\n        logger.info(f\"  Train Loss: {train_loss:.4f}\")\n        logger.info(f\"  Val Loss: {val_loss:.4f}\")\n        logger.info(f\"  IoU: {mean_iou:.4f}\")\n        logger.info(f\"  DSC: {mean_dsc:.4f}\")\n        logger.info(f\"  Precision: {mean_precision:.4f}\")\n        logger.info(f\"  Accuracy: {mean_acc:.4f}\")\n        logger.info(f\"  Epoch Time: {epoch_time:.2f}s\")\n        \n        # Save best model\n        if mean_dsc > best_metrics['best_dsc']:\n            best_metrics['best_dsc'] = mean_dsc\n            best_metrics['best_iou'] = mean_iou\n            best_metrics['best_epoch'] = epoch + 1\n            \n            # Save best model checkpoint\n            save_checkpoint(\n                model, optimizer, scaler, epoch,\n                {'dsc': mean_dsc, 'iou': mean_iou},\n                save_dir / \"best_model.pth\"\n            )\n            \n            logger.info(f\"🎉 New best model saved! DSC: {mean_dsc:.4f}, IoU: {mean_iou:.4f}\")\n        \n        # Save regular checkpoint every 5 epochs\n        if (epoch + 1) % 5 == 0:\n            save_checkpoint(\n                model, optimizer, scaler, epoch,\n                {'dsc': mean_dsc, 'iou': mean_iou},\n                save_dir / f\"checkpoint_epoch_{epoch+1}.pth\"\n            )\n            logger.info(f\"Checkpoint saved at epoch {epoch+1}\")\n        \n        # Save training history\n        with open(save_dir / \"training_history.json\", 'w') as f:\n            json.dump(history, f, indent=2)\n    \n    total_time = time.time() - total_start_time\n    \n    # Final summary\n    logger.info(f\"{'='*50}\")\n    logger.info(\"Training Completed!\")\n    logger.info(f\"Total Training Time: {total_time:.2f}s ({total_time/3600:.2f}h)\")\n    logger.info(f\"Average Epoch Time: {np.mean(history['epoch_times']):.2f}s\")\n    logger.info(f\"Best DSC: {best_metrics['best_dsc']:.4f} at epoch {best_metrics['best_epoch']}\")\n    logger.info(f\"Best IoU: {best_metrics['best_iou']:.4f}\")\n    \n    # Plot and save training curves\n    plot_training_curves(history, save_dir / \"training_curves.png\")\n    \n    return model, history, best_metrics\n\ndef run_training():\n    \"\"\"Main function to run training\"\"\"\n    print(f\"Using device: {DEVICE}\")\n    \n    # Load data\n    root_dir = Path('/kaggle/input/data-1/content')\n    all_images = sorted(list((root_dir / 'images').glob('*.bmp')))\n    all_masks = sorted(list((root_dir / 'masks').glob('*.png')))\n    \n    all_images = sorted(all_images, key=lambda x: x.stem)\n    all_masks = sorted(all_masks, key=lambda x: x.stem)\n    \n    print(f\"Total images: {len(all_images)}\")\n    print(f\"Total masks: {len(all_masks)}\")\n    \n    # Train/validation split (80/20)\n    split_idx = int(0.8 * len(all_images))\n    train_images = all_images[:split_idx]\n    train_masks = all_masks[:split_idx]\n    val_images = all_images[split_idx:]\n    val_masks = all_masks[split_idx:]\n    \n    print(f\"Training samples: {len(train_images)}\")\n    print(f\"Validation samples: {len(val_images)}\")\n    \n    # Create datasets\n    train_dataset = SegmentationDataset(train_images, train_masks, get_train_transform())\n    val_dataset = SegmentationDataset(val_images, val_masks, get_valid_transform())\n    \n    # Create dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n    \n    # Initialize model\n    model = ElectricForceModel().to(DEVICE).to(torch.bfloat16)\n    \n    # Setup optimizer and loss\n    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n    criterion = nn.CrossEntropyLoss()\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='max', patience=5, factor=0.5, verbose=True\n    )\n    \n    # Train model\n    model, history, best_metrics = train_model(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        optimizer=optimizer,\n        criterion=criterion,\n        scheduler=scheduler,\n        num_epochs=NUM_EPOCHS,\n        device=DEVICE,\n        save_dir=\"training_results\"\n    )\n    \n    # Final evaluation on validation set\n    print(\"\\nFinal Evaluation:\")\n    print(f\"Best DSC: {best_metrics['best_dsc']:.4f}\")\n    print(f\"Best IoU: {best_metrics['best_iou']:.4f}\")\n    print(f\"Best Epoch: {best_metrics['best_epoch']}\")\n\n# Run the training\nif __name__ == \"__main__\":\n    run_training()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T08:00:25.660621Z","iopub.execute_input":"2025-06-08T08:00:25.661364Z","iopub.status.idle":"2025-06-08T08:25:38.798994Z","shell.execute_reply.started":"2025-06-08T08:00:25.661336Z","shell.execute_reply":"2025-06-08T08:25:38.797977Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nTotal images: 300\nTotal masks: 300\nTraining samples: 240\nValidation samples: 60\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1 - Training:  19%|█▉        | 9/48 [25:13<1:49:16, 168.12s/it, loss=nan, avg_loss=nan, step_time=157.16s]      \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_99/3172837268.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;31m# Run the training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m     \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_99/3172837268.py\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m     model, history, best_metrics = train_model(\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_99/3172837268.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs, device, save_dir)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m# Training phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         train_loss, batch_losses = train_one_epoch(\n\u001b[0m\u001b[1;32m    263\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         )\n","\u001b[0;32m/tmp/ipykernel_99/3172837268.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, criterion, device, epoch, logger)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Forward pass with autocast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_99/1923050230.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, ipt, chunk_size)\u001b[0m\n\u001b[1;32m    636\u001b[0m                 \u001b[0;31m#theta_rad = self.compute_angle(x, y, chunk_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m                 \u001b[0;31m#Fx, Fy = self.compute_electric_force(ipt, theta_rad, D.pow(-2), chunk_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m                 \u001b[0mFx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate_to_one_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mipt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m                 \u001b[0mzeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_vector_translation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_99/1923050230.py\u001b[0m in \u001b[0;36maggregate_to_one_function\u001b[0;34m(self, ipt, chunk_size)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m                 \u001b[0mFx_ij_sub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF_radial_signed_ij\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta_ji_sub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnormalization_factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m                 \u001b[0mFy_ij_sub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF_radial_signed_ij\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta_ji_sub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnormalization_factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m                 \u001b[0;31m# G. Accumulate forces and potential for the current target_i_chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":13}]}